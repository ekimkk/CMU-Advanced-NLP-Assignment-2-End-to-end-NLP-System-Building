{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-hbFZLSmXvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b95b5f-55bd-4bf0-bebe-8808d83d91c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 8.8 MB/s eta 0:00:00\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "Collecting farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]\n",
            "  Downloading farm_haystack-1.25.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting boilerpy3 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting events (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting httpx (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.19.2)\n",
            "Collecting lazy-imports==0.3.1 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (9.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.2.0)\n",
            "Collecting posthog (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting prompthub-py==4.0.0 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading prompthub_py-4.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pydantic<2 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.2/150.2 kB 3.1 MB/s eta 0:00:00\n",
            "Collecting quantulum3 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading quantulum3-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting rank-bm25 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.31.0)\n",
            "Collecting requests-cache<1.0.0 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading requests_cache-0.9.8-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting scikit-learn>=1.3.0 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting sseclient-py (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (8.2.3)\n",
            "Collecting tiktoken>=0.5.1 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.66.2)\n",
            "Collecting transformers==4.37.2 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.4/129.4 kB 9.6 MB/s eta 0:00:00\n",
            "Collecting mlflow (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading mlflow-2.11.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting rapidfuzz<2.8.0,>=2.0.15 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading rapidfuzz-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.11.4)\n",
            "Collecting seqeval (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 3.2 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: huggingface-hub>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.20.3)\n",
            "Collecting sentence-transformers>=2.2.0 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pillow (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting langdetect (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 26.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.8.1)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from prompthub-py==4.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.4.2)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.11 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.10.0)\n",
            "Collecting jarowinkler<2.0.0,>=1.2.0 (from rapidfuzz<2.8.0,>=2.0.15->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading jarowinkler-1.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2024.2.2)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.4.4)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (23.2.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading cattrs-23.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting url-normalize>=1.4 (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.3.0)\n",
            "Collecting elastic-transport<8 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading elastic_transport-7.16.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting elasticsearch<8,>=7.17 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading elasticsearch-7.17.9-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.18.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.16.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.2.1)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.4)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2023.4)\n",
            "Collecting packaging>=20.0 (from transformers==4.37.2->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (7.0.2)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading docker-7.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.2.5)\n",
            "Collecting querystring-parser<2 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.0.28)\n",
            "Requirement already satisfied: pyarrow<16,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (14.0.2)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.5.2)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.7.1)\n",
            "Collecting graphene<4 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting gunicorn<22 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading gunicorn-21.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.8.2)\n",
            "Collecting monotonic>=1.5 (from posthog->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (7.0.0)\n",
            "Collecting num2words (from quantulum3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (5.9.5)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading Mako-1.3.2-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.2.0)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 3.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.0.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.1.2)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.1.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.12)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words->quantulum3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.11->transformers[sentencepiece,torch]==4.37.2; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.3.0)\n",
            "Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Downloading prompthub_py-4.0.0-py3-none-any.whl (6.9 kB)\n",
            "Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/8.4 MB 86.7 MB/s eta 0:00:00\n",
            "Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 14.9 MB/s eta 0:00:00\n",
            "Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 83.2 MB/s eta 0:00:00\n",
            "Downloading rapidfuzz-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 69.8 MB/s eta 0:00:00\n",
            "Downloading requests_cache-0.9.8-py3-none-any.whl (48 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.7/48.7 kB 3.7 MB/s eta 0:00:00\n",
            "Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 96.0 MB/s eta 0:00:00\n",
            "Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.5/156.5 kB 10.9 MB/s eta 0:00:00\n",
            "Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 57.8 MB/s eta 0:00:00\n",
            "Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
            "Downloading farm_haystack-1.25.0-py3-none-any.whl (768 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 768.7/768.7 kB 40.8 MB/s eta 0:00:00\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 5.5 MB/s eta 0:00:00\n",
            "Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.8/77.8 kB 6.1 MB/s eta 0:00:00\n",
            "Downloading mlflow-2.11.1-py3-none-any.whl (19.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.7/19.7 MB 69.0 MB/s eta 0:00:00\n",
            "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.3/41.3 kB 2.9 MB/s eta 0:00:00\n",
            "Downloading quantulum3-0.9.0-py3-none-any.whl (10.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/10.7 MB 8.4 MB/s eta 0:00:00\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.1/290.1 kB 19.0 MB/s eta 0:00:00\n",
            "Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.4/233.4 kB 15.8 MB/s eta 0:00:00\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 4.2 MB/s eta 0:00:00\n",
            "Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.6/147.6 kB 11.3 MB/s eta 0:00:00\n",
            "Downloading elastic_transport-7.16.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading elasticsearch-7.17.9-py2.py3-none-any.whl (385 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 386.0/386.0 kB 19.8 MB/s eta 0:00:00\n",
            "Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195.4/195.4 kB 11.5 MB/s eta 0:00:00\n",
            "Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 6.9 MB/s eta 0:00:00\n",
            "Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.2/80.2 kB 5.5 MB/s eta 0:00:00\n",
            "Downloading jarowinkler-1.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 8.1 MB/s eta 0:00:00\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 3.5 MB/s eta 0:00:00\n",
            "Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 3.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 93.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 62.6 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 35.1 MB/s eta 0:00:00\n",
            "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.3 MB/s eta 0:00:00\n",
            "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 6.8 MB/s eta 0:00:00\n",
            "Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 8.2 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 7.0 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 5.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 6.3 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 7.9 MB/s eta 0:00:00\n",
            "Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 11.7 MB/s eta 0:00:00\n",
            "Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.3/143.3 kB 11.3 MB/s eta 0:00:00\n",
            "Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 3.9 MB/s eta 0:00:00\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 4.3 MB/s eta 0:00:00\n",
            "Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202.9/202.9 kB 15.1 MB/s eta 0:00:00\n",
            "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 4.6 MB/s eta 0:00:00\n",
            "Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 6.0 MB/s eta 0:00:00\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 74.1 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: langdetect, seqeval, docopt\n",
            "  Building wheel for langdetect (setup.py): started\n",
            "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=a14932042b2aa121013b60823c135fa43f8783a3901a6706969a32ab86b93cf8\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=3d73126c933c325cac11b1f11feece58a653ee70324d18c661f7d9ff6f660a59\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=53b7602b2bcc7569b4159927c7ad138f3ceb4b263108ed901127939f96cb9a74\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built langdetect seqeval docopt\n",
            "Installing collected packages: sseclient-py, monotonic, events, docopt, aniso8601, urllib3, url-normalize, smmap, rank-bm25, querystring-parser, pydantic, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, num2words, Mako, lazy-imports, langdetect, jarowinkler, h11, graphql-core, cattrs, boilerpy3, backoff, scikit-learn, rapidfuzz, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, gunicorn, graphql-relay, gitdb, elasticsearch, elastic-transport, alembic, tiktoken, seqeval, requests-cache, quantulum3, prompthub-py, posthog, nvidia-cusolver-cu12, httpx, graphene, gitpython, docker, mlflow, transformers, accelerate, sentence-transformers, farm-haystack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.3\n",
            "    Uninstalling pydantic-2.6.3:\n",
            "      Successfully uninstalled pydantic-2.6.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "Successfully installed Mako-1.3.2 accelerate-0.28.0 alembic-1.13.1 aniso8601-9.0.1 backoff-2.2.1 boilerpy3-1.0.7 cattrs-23.2.3 docker-7.0.0 docopt-0.6.2 elastic-transport-7.16.0 elasticsearch-7.17.9 events-0.5 farm-haystack-1.25.0 gitdb-4.0.11 gitpython-3.1.42 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-21.2.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jarowinkler-1.2.3 langdetect-1.0.9 lazy-imports-0.3.1 mlflow-2.11.1 monotonic-1.6 num2words-0.5.13 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 packaging-23.2 pillow-9.0.0 posthog-3.5.0 prompthub-py-4.0.0 pydantic-1.10.14 quantulum3-0.9.0 querystring-parser-1.2.4 rank-bm25-0.2.2 rapidfuzz-2.7.0 requests-cache-0.9.8 scikit-learn-1.4.1.post1 sentence-transformers-2.5.1 seqeval-1.2.2 smmap-5.0.1 sseclient-py-1.8.0 tiktoken-0.6.0 transformers-4.37.2 url-normalize-1.4.3 urllib3-1.26.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        }
      ],
      "source": [
        "# Reference for the below code is: https://haystack.deepset.ai/tutorials/03_scalable_qa_system\n",
        "%%bash\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab,preprocessing,elasticsearch,inference,metrics]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX8-RkeQmXva"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ],
      "metadata": {
        "id": "J9Hzs7VdL5T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1yEFs8WmXvd",
        "outputId": "35b6941c-92a4-4146-980c-bad69da0d8e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'haystack'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f733849a553b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhaystack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_stores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElasticsearchDocumentStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get the host where Elasticsearch is running, default to localhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ELASTICSEARCH_HOST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'haystack'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "# Get the host where Elasticsearch is running, default to localhost\n",
        "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
        "\n",
        "document_store = ElasticsearchDocumentStore(host=host, username=\"\", password=\"\", index=\"document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnmiMsEEmXvd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "doc_dir = '/content/drive/My Drive/Colab Notebooks/NLP2/reference'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfjonkMvmXve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38cb76e-15d9-45cb-a279-78568234ed53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.nodes import TextConverter, PreProcessor\n",
        "\n",
        "# Initialize the TextConverter and PreProcessor with your desired configurations\n",
        "text_converter = TextConverter()\n",
        "preprocessor = PreProcessor(\n",
        "    clean_header_footer=True,\n",
        "    split_by=\"word\",\n",
        "    split_length=200,\n",
        "    split_overlap=0,\n",
        "    split_respect_sentence_boundary=False,\n",
        "    clean_empty_lines=False,\n",
        "    clean_whitespace=False,\n",
        ")\n",
        "\n",
        "# Create an indexing pipeline\n",
        "indexing_pipeline = Pipeline()\n",
        "\n",
        "# Add the TextConverter and PreProcessor to the pipeline\n",
        "indexing_pipeline.add_node(component=text_converter, name=\"TextConverter\", inputs=[\"File\"])\n",
        "indexing_pipeline.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])\n",
        "indexing_pipeline.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"PreProcessor\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9qlrk5UmXvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c3ab30-0d78-4b2a-8041-0f3885bc5c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting files: 100%|██████████| 528/528 [01:12<00:00,  7.27it/s]\n",
            "Preprocessing:  11%|█         | 59/528 [00:03<00:10, 45.39docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:Document 361aa62db9bc0a7212e426d94e10b64d is 11155 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
            "Preprocessing:  17%|█▋        | 91/528 [00:04<00:09, 46.25docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:Document 52b37eb9d79729a8ee0e8a0b920bd71c is 29176 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
            "WARNING:haystack.nodes.preprocessor.preprocessor:Document e2b909e0f52c32e2190ca9ff9fb7b590 is 19176 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
            "Preprocessing:  24%|██▍       | 128/528 [00:04<00:09, 42.57docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:Document 100449f5d963b0af9c19c7e4d173516c is 11935 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
            "Preprocessing:  33%|███▎      | 175/528 [00:05<00:07, 48.35docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:Document 2c2c41ad0d37aa14b8ab122fd11477bd is 12191 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
            "Preprocessing:  98%|█████████▊| 517/528 [00:14<00:00, 24.27docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:Document e4a569cdfab90e46f8537942c81efcb8 is 14596 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
            "WARNING:haystack.nodes.preprocessor.preprocessor:Document 806f8cef1969d81a2713bc52ed2321f2 is 12946 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
            "WARNING:haystack.nodes.preprocessor.preprocessor:Document 182ef7aa576c2400a485bafcdb9b3d6d is 12503 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time. This document will be now hard-split at 10000 chars recursively.\n",
            "Preprocessing: 100%|██████████| 528/528 [00:14<00:00, 35.71docs/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'documents': [<Document: {'content': \"25 Great Things About SCS | SCS25 - Carnegie Mellon University School of Computer Science\\nSkip to main content\\nLegal\\nEvents\\nRegister\\n25 Things\\nHistory\\nVideo\\n25 Great Things About SCS\\nWhat’s so great about computer science at Carnegie Mellon?We're glad you asked! Here are 25 great ideas from CMU computer scientists to think about as we celebrate the birthday of the School of Computer Science.1. Artificial intelligence, 1955-56\\xa0Can you write a working computer program without a computer? Herb Simon (H’90), at left, Allen Newell (IA’57), at right, and Cliff Shaw did. The team created the first artificial intelligence program, Logic Theorist, which could solve logic puzzles in the same way that a human might solve them. Newell demonstrated that it worked by writing the instructions on 3-by-5 index cards that were manipulated on the kitchen table by Newell, his wife, and a group of Carnegie Tech grad students.\\n2. Multi-core processors, 1971\\xa0Multi-core processors are common in today’s computers, but they were still science fiction in the early 1970s. But when CMU researchers found their existing machines too slow to keep pace with the advance of speech and graphics programs, they knew they had to do something. They solved the problem by ganging together 16 processors to build a pioneering\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f70819fd01fa4d3857c6c31c55941146'}>,\n",
              "  <Document: {'content': 'computer called C.mmp—then topped the feat by linking 50 processors into Cm*.\\n3. Tutoring machines, 1973Games and drills, such as flash cards, have long been used to help students learn tough subjects. But the cognitive tutoring programs developed at Carnegie Mellon, beginning in the 1970s, did more than simply drill students on math problems. Cognitive tutors were able to adapt, presenting harder or easier problems as students learned or stumbled. Today, cognitive tutors teach subjects such as algebra to hundreds of thousands of students every year.\\n4. Speech recognition, 1976If you have an iPhone, ask Siri to look up “Hearsay I,” the first computer system capable of continuous speech recognition. It was developed by future Turing Award winner and future SCS dean Raj Reddy along with his students. Their work on subsequent systems established many of the principles that still underlie speech recognition software.\\n5. Emoticons, 1982Sure, it was just a joke, but (for better or worse) it’s endured for more than three decades. CMU researcher Scott Fahlman created the emoticon to clear up misunderstandings on computer message boards. We’ve been looking at the world sideways ever since. :-)\\n6. Andrew project, 1982It was long the dream of computer scientists to put a workstation', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eadf99c909bcf6d10fd0f682d6aa9ad8'}>,\n",
              "  <Document: {'content': \"in every home and office, but no one had actually tried to accomplish it until researchers from Carnegie Mellon University and IBM launched the Andrew Project. Soon, every student, faculty member and employee had access to email, word processing, file-transfer services and graphics programs, and CMU was the most-wired campus in the world.\\n7. Autonomous robots, 1983Thanks to Red Whittaker (E'75,'79), robots moved off of the assembly lines and into places no human ever could go. His Robotic Reconnaissance Vehicle spent four years inspecting and cleaning up the contaminated reactor building at the crippled Three Mile Island nuclear plant.\\n8. User interfaces, 1983Why should humans adapt to fit computers? Shouldn’t computers adapt to fit humans? That was the attitude of CMU researchers, who applied design principles to computer science to develop better, easier-to-use interfaces. They called the new field “human-computer interaction.”\\n9. Machine translation, 1984Every “Star Trek” fan knows about the universal translator. Scientists in the Language Technologies Institute are moving those gadgets from science fiction to real life. Their pioneering systems include handheld, portable speech-to-speech translators, just like those depicted on the USS Enterprise.\\n10. Mach kernel, 1985In computer parlance, a “kernel” is the heart of an operating system, passing input and output\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'df9a93af6d094b3f204d459490795bf8'}>,\n",
              "  <Document: {'content': 'requests to and from the processor. At the core of all modern Apple devices --- including iPhones, iPads and MacBooks --- is the Mach kernel, developed at CMU under the leadership of then-professor Rick Rashid.\\n11. Computer chess, 1990Could a computer play chess at the level of the world’s best players? For many years, it was considered the “holy grail” of artificial intelligence. Hitech, developed by CMU researcher Hans Berliner (CS’74), was the first computer to achieve grandmaster status. CMU alumni played key roles in developing “Deep Blue,” the IBM machine that beat human chess champion Garry Kasparov in 1997.\\n12. Java, 1991 As a CMU grad student, James Gosling (CS’83) worked on the Andrew project, which stressed interoperability between computers, whether they were Macs, IBMs or Unix machines. Those lessons served Gosling well when he developed Java, the first programming language able to run on almost any platform.\\n13. Email attachments, 1992Steve Jobs liked the email system built into CMU’s Andrew so much that he tried to hire Nathaniel Borenstein (CS’81,’85) and the rest of his team to create a similar program for Apple. Borenstein didn’t take the offer, but he did like Jobs’ idea about attaching documents to email. Borenstein went', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 3}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f881077b2682d4f1e60eeb8b842ff8c4'}>,\n",
              "  <Document: {'content': 'on to develop the MIME standard that’s used by all email programs to send photos and other files over the Internet.\\n14. Web search engines, 1994The World Wide Web was still in its toddler stage when CMU researcher Michael “Fuzzy” Mauldin (CS’83,’89) developed one of the first successful search engines, Lycos. It was the most-visited site on the Web by 1999.\\n15. Model checking, 1994CMU professor Edmund Clarke had long stressed the importance of verifying computer hardware and software through a formal problem-solving technique called “model checking.” In 1994, his arguments gained new weight with the discovery that Intel’s amazing new Pentium chip made errors on certain math problems. Clarke would go on to receive the Turing Award for his role in the development of model checking.\\n16. CAPTCHAs, 2000 “Spam” and malicious attacks were a growing problem on the Internet when hackers developed automated “bots” that could sign up for email and other Web services without human intervention. Luis von Ahn (CS’03,’05), Nick Hopper (CS’04), John Langford (CS’02) and CMU professor Manuel Blum invented a “Completely Automated Public Turing Test to tell Computers and Humans Apart,” or CAPTCHA, to help foil the bots. A later variation, reCAPTCHA, is helping digitize old books', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 4}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c8e1cfc05633cfa7d516e41275e1bf96'}>,\n",
              "  <Document: {'content': 'and newspapers.\\n17. Robotic video cameras, 2001When Baltimore Ravens quarterback Trent Dilfer dropped back to pass, TV viewers of Super Bowl XXXV saw something they’d never seen before --- the motion froze and the view suddenly rotated to show Dilfer’s opposite side. CBS called it Eyevision. The synchronized system of robotic cameras and advanced image processing was the brainchild of CMU’s Takeo Kanade, one of many advances he pioneered in computer vision.\\n18. Self-driving vehicles, 2007Carnegie Mellon’s early attempts at self-driving vehicles progressed slowly, creeping around Pittsburgh’s Schenley Park in the late 1990s. But they were going full-throttle by the time CMU’s self-driving SUV, named BOSS, won the 2007 DARPA Urban Challenge road race.\\n19. Thought reading programs, 2007Your brain reacts in different ways, depending on what words you’re thinking about --- ways that are measurable with magnetic-resonance imaging, or MRI, machines. CMU researchers Tom Mitchell and Marcel Just are decoding those brain scans and are making progress at reading people’s thoughts.\\n20. Kidney donor matching, 2008 Organ transplants save lives every day, but more people could likely be saved if it was easier to match recipients with donors who are unrelated. An algorithm developed by CMU scientists is close to enabling a nationwide', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 5}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2f4175b3be0715462bfe3842006b1e78'}>,\n",
              "  <Document: {'content': \"network that would match living kidney donors with potential recipients whom they've never met in real life.\\n21. RNA sequencing via videogames, 2010 Thanks to crowdsourcing, science isn’t just for scientists any more. People without formal training in molecular biology are producing new insights into genetic encoding through a videogame called EteRNA, developed by researchers at CMU and Stanford, that allows players to fold and shift RNA molecules to solve on-screen puzzles.\\n22. Language learning software, 2010Learning a second language has always been challenging, but a CMU spinoff called Duolingo is proving that it doesn’t have to be expensive. Duolingo has developed language tutoring software that enables users to learn Spanish, English, Italian, German, Portuguese or French for free through its website and mobile apps. In the process, Duolingo users are helping to translate the Web.\\n23. Question-answering computers, 2011 Searching the Web for information is rarely as simple as asking a question in plain English. So-called “question-answering” machines moved from laboratories to TV screens when an IBM computer called “Watson” defeated two human champions on the game show “Jeopardy!” At the heart of Watson was computer architecture developed by CMU’s Eric Nyberg and his students.\\n24. Encrypting online information, 2012 Credit card numbers\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 6}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36ba94fb1ef93ae0151a270d4505d86e'}>,\n",
              "  <Document: {'content': 'and other data used online is safer thanks to an encryption scheme developed by CMU alumna Shafi Goldwasser (S’79). She shared the 2012 Turing Award for her role in developing practical encoding schemes that are difficult to break.\\n25. Smart, adaptable traffic signals, 2012 Smart traffic lights developed at CMU’s Robotics Institute are saving time and energy, and cutting down on the amount of air pollution created by idling cars. First rolled out in Pittsburgh’s East Liberty neighborhood, the new signals are being studied around the country.\\nA history of SCS | SCS25 - Carnegie Mellon University School of Computer Science\\nSkip to main content\\nLegal\\nEvents\\nRegister\\n25 Things\\nHistory\\nVideo\\nA history of SCS\\nFor an expanded history of the School of Computer Science and its predecessors at CMU, read \"Institutional Memories\" in the Summer 2014 issue of The Link magazine.In 2014, the School of Computer Science celebrated its 25th year as a stand-alone college within Carnegie Mellon University. It was arguably the first college devoted solely to computer science in the United States, and a model for others that followed. But CMU’s computer science era begins much earlier—in 1956, with the arrival of an IBM 650 computer on the campus of what was then known as Carnegie Institute', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 7}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '39dbd8e7c0057fdb0b150279afe332e6'}>,\n",
              "  <Document: {'content': 'of Technology. The IBM 650 had magnetic-drum memory and a processing speed of approximately 60 instructions per second. Herb Simon (H’90), associate dean of the Graduate School of Industrial Administration—now known as CMU’s Tepper School of Business—established Carnegie Tech’s first Computation Center with the help of its first director, Alan Perlis (S’42).First freshman-level computer science courseIn 1956 and 1957, Simon, Allen Newell (IA’57) and Cliff Shaw of RAND designed the Logic Theorist, a computer program that could develop proofs for theorems in much the same way a human would work. They also developed linked-list data structures, the foundation of computer programming. Perlis, Simon and Newell are credited with defining the term “computer science” as “the theory and design of computers,” as well as (in Newell’s words) “the study of the phenomena arising from them.” In 1958, Perlis began teaching the first freshman-level computer programming course in the United States at Carnegie Tech.Computer science Ph.D. program createdIn 1961, the Computation Center and its newest computer, a Bendix G-20, were moved to recently completed Scaife Hall. That same year, Carnegie Tech created an interdisciplinary Ph.D. program called Systems and Communications Sciences, combining elements of computer science, mathematics, psychology, business and electrical engineering.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 8}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a07784815e8a24b7b2a367acd243701'}>,\n",
              "  <Document: {'content': 'The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation. Perlis was the first department head. There were no undergraduates; only Ph.D. students were admitted, and the department’s focus was on research, much of it funded by the federal government through the Defense Department’s Advanced Research Projects Agency, or DARPA. In 1967, Carnegie Institute of Technology merged with the nearby Mellon Institute of Industrial Research to form Carnegie-Mellon University. The Department of Computer Science moved into the newly created Mellon Institute of Science, later renamed Mellon College of Science, or MCS. Future SCS dean Raj Reddy joined the CSD in 1969 after three years as an assistant professor at Stanford. He brought with him research in speech, language and computer vision. But in 1970 and 1971, the new Computer Science Department faced its first crisis, as half of its tenured faculty members—including department head Perlis—left for other universities. Joe Traub was recruited from Bell Labs to CMU to become the new department head.Multi-processor machines emergeCSD emerged from the brief crisis as a highly agile, interdisciplinary entity,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 9}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27eab5f0bfdcb01232e709f3fb0591d'}>,\n",
              "  <Document: {'content': 'with many new faculty members taking joint appointments with other CMU departments. Several large projects emerged in the Computer Science Department, including C.mmp, the first shared-memory multiprocessor computer, with 16 processing units, and Cm*, a 50-processor computer. These computers were the forerunners of today’s ubiquitous multi-core desktops and laptops.Turing Awards and a Nobel PrizeIn 1975, Simon and Newell were awarded the A.M. Turing Award for their work in artificial intelligence. (As of 2014, 12 CMU alumni or faculty have been awarded Turings, sometimes considered the Nobel Prize of computing.) Three years later, Simon received the Nobel Prize in Economics for his work on decision-making theory. As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 10}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2bc31ce6362e111b4f84159b0661bb40'}>,\n",
              "  <Document: {'content': 'InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 11}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f07b906022f8f736f1b9c38c9baf9da1'}>,\n",
              "  <Document: {'content': 'MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 12}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f831b352fb2eaa9490acdd12286c4c09'}>,\n",
              "  <Document: {'content': 'announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91. By 1995, there were 401 undergraduates in the School of Computer Science; in fall 2013, more than 600 undergraduates made up about 37 percent of student enrollment at SCS, along with more than 600 master’s degree students.New departments, new areas of studyAlong the way, the Center for Machine Translation became the Language Technologies Institute, and other new departments were formed, including the Human-Computer Interaction Institute (1993), the Institute for Software Research (1999), the Machine Learning', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 13}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e45d6d88dd48610a215a5a28481960a6'}>,\n",
              "  <Document: {'content': 'Department (2006) and the Ray and Stephanie Lane Center for Computational Biology (2009). SCS’s seven degree-granting departments draw faculty and students from a wide variety of disciplines, including engineering, mathematics, social sciences, linguistics and design.Committed to extending our founders’ visionThe School of Computer Science at Carnegie Mellon University enters its second quarter century as a world-leading educational and research institution, embracing all facets of computing. Its graduate programs are consisted ranked with the best in the world by a leading U.S. magazine, while its undergraduate programs are also rated the best in the U.S. by corporate recruiters. In 2013, SCS had 284 faculty members and a total student enrollment of nearly 1,700, including undergraduate, master’s and Ph.D. students, and conducted $124 million in research. Indeed, by itself, the Robotics Institute is the largest university robotics research group in the world, with more than 500 people and more than 100 ongoing research projects. A half-century ago, Perlis, Simon and Newell outlined a vision for computer science. The School of Computer Science at CMU remains committed to continuing and extending their vision in the context of big data and connected computing in the 21st century.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 14}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b39d0fa1ec1f7a30949133c21dfae6f5'}>,\n",
              "  <Document: {'content': 'History -     CMU - Carnegie Mellon University\\nCarnegie Mellon University\\n———\\nSearch\\nSearch\\nCMU\\n›\\xa0             About\\n›\\xa0             History\\nAndrew Carnegie\\nA self-educated \"working boy\" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world\\'s largest steel producing company by the end of the 19th century.\\nCarnegie Technical Schools\\nAt one point the richest man in the world, Carnegie believed that \"to die rich is to die disgraced.\" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed.\\nIn 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities.\\n\"My heart is in the work,\" he stated, which would become part of the school\\'s official motto.\\nThe Carnegie', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'da324d95a15ead1240c21680443c1347'}>,\n",
              "  <Document: {'content': 'Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College.\\nCarnegie Tech – Early Years\\nSoon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor\\'s degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or \"Carnegie Tech.\"\\nDuring the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were:\\nIt expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today.\\nIt pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech.\\nIt began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.\\nIt laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e72f7b63eb4811079e4d56e1925d97f'}>,\n",
              "  <Document: {'content': \"research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research.\\nIt initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society.\\nCarnegie died in 1919, but his vision for an educated public lived on after him.\\nCarnegie Tech - Post-war Years\\nWith the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study.\\nUniversity culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms.\\nThe times were changing, and Tech positioned itself at the forefront, opening three new schools:\\n1948:\\xa0The Graduate School of Industrial Administration, later renamed the\\xa0David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science.\\n1968:\\xa0School of Urban and Public Affairs, later renamed the\\xa0H. John Heinz III College, providing graduate training for work in the public sector.\\n1986:\\xa0School of Computer Science, pioneering computing and artificial\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '443f70b034422b3e86d3186cb45ced48'}>,\n",
              "  <Document: {'content': 'intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon.\\nCarnegie Mellon University\\nIn 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons.\\nIt allowed Carnegie Mellon to establish the last of its current pillars: the\\xa0Mellon College of Science\\xa0and the College of Humanities and Social Sciences, now known as\\xa0Marianna Brown Dietrich College of Humanities and Social Sciences.\\nIn 2017, Carnegie Mellon celebrated the 50th anniversary of the Carnegie Tech-Mellon Institute merger, revisiting the shared vision of the founders and recognizing the impact it has had, and will continue to have, in the world of higher education, research and discovery.\\nA Global Impact\\nIn its 115 years, Carnegie Mellon has soared to national and international leadership in higher education and research. A birthplace of innovation since its founding, it continues to be known for innovation, for solving real-world problems and for interdisciplinary collaboration.\\nIts\\xa0alumni can be found across the globe\\xa0— from Tony Award winners to Nobel Prize and Turing Award winners, from CEOs to entrepreneurs, from professors to artists.\\nIn the 2000s, in response to demand for expanded international', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 3}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '157c35d9ddc2217f7f5224c89c3a522c'}>,\n",
              "  <Document: {'content': 'educational opportunities, Carnegie Mellon began offering degree programs outside of Pittsburgh.\\nToday its global presence includes campuses in Qatar and Silicon Valley, Calif., more than a dozen degree-granting locations and more than 20 research partnerships such as Los Angeles; New York City; Washington, D.C.; Australia; China; Portugal and Rwanda.\\nThe Future\\nCMU is positioned like never before to meet the challenges of the 21st century. In the coming years, the university will see the largest expansion to the Pittsburgh campus since 1900.\\nAt the intersection of technology and humanity, CMU research, innovation and creativity will continue to guide our future as a world-class university.\\nAs outlined in the Strategic Plan 2025, the university will focus on advancing the individual student experience, the broader Carnegie Mellon community experience, and the social impact of Carnegie Mellon throughout the world.\\nCarnegie Mellon University challenges the curious and passionate to deliver work that matters.\\nCalendar CareersCOVID-19 Updates Directory / Contact FeedbackGlobal Locations\\nHealth & Safety News Site Map Title IX\\nAlumniBUSINESS & RESEARCH PARTNERS Faculty & Staff Students\\nCarnegie Mellon University     5000 Forbes Avenue\\nPittsburgh, PA 15213\\n412-268-2000\\nLegal Info\\nwww.cmu.edu\\n© 2023 Carnegie Mellon University\\nCMU on Facebook\\nCMU on Twitter\\nCMU on LinkedIn\\nCMU YouTube Channel\\nCMU on Instagram\\nCMU on Flickr\\nCMU Social Media Directory\\nAcademics\\nInterdisciplinary Programs\\nLibraries\\nLearning for a Lifetime\\nAdmission\\nUndergraduate\\nGraduate\\nAbout\\nLeadership\\nVision,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 4}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '916338803e9f49a6dc90fc576552a42a'}>,\n",
              "  <Document: {'content': 'Mission and Values\\nHistory\\nTraditions\\nDiversity, Equity, Inclusion and Belonging\\nPittsburgh\\nRankings\\nAwards\\nVisit\\nDavid & Susan Coulter Welcome Center\\nMaps & Getting Here\\nResearch\\nCenters & Institutes\\nStudent Experience\\nAthletics\\nGive\\nAlumni\\nBusiness & Research Partners\\nCOVID-19 Updates\\nFaculty & Staff\\nStudents', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 5}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '80abbe410e13d6a7e3a02ce5f364b408'}>,\n",
              "  <Document: {'content': 'Schedule -     Commencement - Carnegie Mellon University\\nCarnegie Mellon University\\n———\\nCommencement\\nCommencement\\n›\\xa0             Schedule\\nCommencement Schedule\\nWe are proud to celebrate our newest graduates during this year’s commencement exercises!\\nMain Commencement Ceremony\\nDiploma Ceremonies\\nMain Commencement Ceremony\\nBachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.\\nThe main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.\\nThere is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.\\nThe ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of the student procession. Access to guest seating will be restricted once the student procession begins.\\nDiploma Ceremonies\\nIn addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).\\nDiploma ceremonies will include the presentation of diplomas', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fb6e4947ad122bde0d9f2566aa8cb0b5'}>,\n",
              "  <Document: {'content': \"to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.\\nDiploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.\\nDiploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.\\nMore details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks.\\nThursday, May 9\\nPhi Beta Kappa Initiation CeremonyCeremony: 2–3 p.m.McConomy Auditorium, Cohon University CenterReception: 3–4 p.m.Connan Room, Cohon University Center\\nContact:Joseph Devinejd0x@andrew.cmu.edu\\nJoanne Ursenbachjoanneu@andrew.cmu.edu\\nPresident's Graduates Toast (bachelor's students)3:30–4:30 p.m.Location TBDRegistration required. Invitation, along with registration details, will be sent in late April.\\nFirst Gen Graduation RecognitionReception: 5-5:30 p.m.Alumni Concert Hall, College of Fine ArtsCeremony: 5:30-6:30p.m.Kresge Theater, College of Fine Arts\\nContact:M. Shernell Smithmssmith@andrew.cmu.edu\\nSam Colavecchioscolavec@andrew.cmu.edu\\xa0412-268-7733\\nFriday, May 10\\nDiploma CeremoniesVarious times\\nSenior Leadership Recognition Ceremony4–5:30 p.m.Wiegand Gym, Cohon University CenterUndergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.\\nSaturday, May 11\\nDiploma CeremoniesVarious times\\nCenter for Student Diversity and Inclusion CeremonyNoon–2:30 p.m.Simmons Auditorium, Tepper Building\\nContact:M. Shernell Smithmssmith@andrew.cmu.edu\\nSam Colavecchioscolavec@andrew.cmu.edu\\nNaval ROTC CommissioningCeremony: 1:30-2:30 p.m.Auditorium, Soldiers & Sailors Memorial Hall & Museum\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bd4852bc23e2fbf71f738fd57dd6b10b'}>,\n",
              "  <Document: {'content': '*4141 Fifth Avenue, Pittsburgh, PA 15213\\nContact:Mike Dankomdanko@andrew.cmu.edu\\nThe President’s Reception in honor of CMU’s Doctoral Candidates4–6 p.m.Tepper Building Atrium\\nSunday, May 12\\nGesling Stadium opens to guests8 a.m.\\nRobing and procession for graduates9–10 a.m.Various locations across campus\\nStudent procession begins9:15 a.m.All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.\\nCommencement Ceremony10–11:30 a.m.Gesling Stadium, CMU’s campus\\nDiploma CeremoniesVarious times\\nCarnegie Mellon University     5000 Forbes Avenue\\nPittsburgh, PA 15213\\n412-268-5052\\nContact Us\\nLegal Info\\nwww.cmu.edu\\nSchedule\\nMain Ceremony\\nDiploma Ceremonies\\nGraduates\\nDoctoral Candidates\\nFaculty\\nCap and Gown Information\\nFamilies & Guests\\nFAQ\\nContact Us', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c1bf277e145adfdd2e7c8b794e398f8'}>,\n",
              "  <Document: {'content': \"Buggy Races Keep Rolling at Carnegie Mellon -     News - Carnegie Mellon University\\nCarnegie Mellon University\\n———\\nSearch\\nSearch\\nSearch this site only\\nNews\\nNews\\n›\\xa0             Stories\\n›\\xa0             Archives\\n›\\xa0             2019\\n›\\xa0             April\\n›\\xa0             Buggy Races Keep Rolling at Carnegie Mellon\\nApril 10, 2019\\nBuggy Races Keep Rolling at Carnegie Mellon\\nIn its 99th year, the tradition is a Spring Carnival treat\\nBy Heidi Opdyke opdyke(through)andrew.cmu.edu\\nMedia Inquiries\\nJulie Mattera\\nMarketing and Communications\\njmattera(through)cmu.edu\\n412-268-2902\\nSweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago.\\nToday, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill.\\nBut while five pushers and a driver navigate the course's\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '102d19b8fca4b89fdf69f6a8797e066a'}>,\n",
              "  <Document: {'content': 'hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing.\\nThe Machine\\nThe basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used.\\nEach has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool.\\nFairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy.\\n\"We have a reputation of being the quietest on the course,\" said Diya Nuxoll, who wrapped up her bachelor\\'s degree in mechanical engineering in December and is working on an advanced studies master\\'s degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics.\\nAn mural celebrating Buggy can be found in\\xa0CMU\\'s\\xa0Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions.\\nSweepstakes Slang\\nBuggy: Vehicle being raced and also a nickname for the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '99902357234697baa789dca32522aecc'}>,\n",
              "  <Document: {'content': 'competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy\\'s aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy.\\nMembers of Fringe, celebrating their 50th year of Buggy racing,\\xa0allowed a rare visit into its shop for a behind-the-scenes tour.\\nThis year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the \"Froom.\"\\n\"We\\'re used to saying everything', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '30eee79031c50f13aaace1c3a080fb39'}>,\n",
              "  <Document: {'content': 'with \\'fr\\' in front of it but when we say something in front of other people, it gets them confused,\" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor\\'s degree in mechanical engineering and biomedical engineering.\\nFringe vehicles often are named with the letter \"B,\" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice.\\nThis year\\'s buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It\\'s one of the few times spectators can see buggies up close.\\nDave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe.\\nBehind the Wheel\\nDrivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 3}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ffca49cf54a33452c0698479c4f05676'}>,\n",
              "  <Document: {'content': 'to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women.\\nTishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year\\'s Sweepstakes.\\n\"Our team\\'s philosophy is to teach everyone how to do everything,\" Girdhar said. \"But I came to CMU wanting to drive. I wanted to drive so badly.\"\\nAmy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016.\\nSafety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it\\'s safe to proceed.\\nThe streets of the race course haven\\'t changed, but the condition of the course', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 4}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad1f7ba8cdef11824af89124ed84dc0c'}>,\n",
              "  <Document: {'content': 'varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.\\nWith the vehicles just inches from the ground, \"even poorly filled potholes makes it dangerous to drive,\" she added.\\nDepending on Friday and Saturday\\'s conditions, most heats may run just two lanes instead of three. But still, despite some of the challenges, Chen said she wouldn\\'t miss it.\\n\"Being a driver is really fun,\" Chen said. \"I love going fast and going around the course.\"\\n— #CMUcarnival —\\nPowered by Curator.io\\n— Related Content —\\nMedia Advisory: Carnegie Mellon Celebrates Spring Carnival\\nBuggy Races Keep Rolling at Carnegie Mellon\\nEnjoy a World of Fun at Spring Carnival\\nStudent Architects Design Carnival Archway\\nTake a Ride on The Old Mill at CMU\\nMoBot Turns 25\\nWitchner, Wood Roll Into Buggy Royalty\\nThe Piper: Campus & Community News\\nOfficial Events Calendar\\nCarnegie Mellon University     5000 Forbes Avenue\\nPittsburgh, PA 15213\\n412-268-2900\\nLegal Info\\nwww.cmu.edu\\n© 2021 Carnegie Mellon University\\nCMU on Facebook\\nCMU on Twitter\\nCMU on LinkedIn\\nCMU YouTube Channel\\nCMU RSS Feed\\nCMU on Instagram\\nCMU Social Media Directory\\nStories\\nCollege of Engineering\\nCollege of Fine Arts\\nDietrich College of Humanities & Social Sciences\\nHeinz', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 5}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8c444c7d7d56c86147fb85ec01a924d5'}>,\n",
              "  <Document: {'content': 'College of Information Systems and Public Policy\\nMellon College of Science\\nSchool of Computer Science\\nTepper School of Business\\nArchives\\n2021\\nMarch\\nFebruary\\nJanuary\\n2020\\nDecember\\nNovember\\nOctober\\nSeptember\\nAugust\\nJuly\\nJune\\nMay\\nApril\\nMarch\\nFebruary\\nJanuary\\n2019\\nDecember\\nNovember\\nOctober\\nSeptember\\nAugust\\nJuly\\nJune\\nMay\\nApril\\nMarch\\nFebruary\\nJanuary\\n2018\\nDecember\\nNovember\\nOctober\\nSeptember\\nAugust\\nJuly\\nJune\\nMay\\nApril\\nMarch\\nFebruary\\nJanuary\\n2017\\nJanuary\\nFebruary\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\n2016\\nJanuary\\nFebruary\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\n2015\\nJanuary\\nFebruary\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\n2014\\nJanuary\\nFebruary\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\n2013\\nJanuary\\nFebruary\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\n2012\\nJanuary\\nFebruary\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\n2011\\nJanuary\\nFebruary\\nMarch\\nApril\\nMay\\nJune\\nJuly\\nAugust\\nSeptember\\nOctober\\nNovember\\nDecember\\nMedia Highlights\\nMedia Resources\\nExperts (Alphabetical)\\nExperts (by Topic)\\nContact Us\\nThe Piper: Campus & Community News', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 6}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c96e30bb0ebb24e3a99f3319e494c635'}>,\n",
              "  <Document: {'content': '==Faculty==\\n\\nYonatan Bisk\\nAssistant Professor\\nEmail: ybisk@cs.cmu.edu\\nOffice: Gates & Hillman Centers\\nResearch Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning\\n\\nRalf Brown\\nPrincipal Systems Scientist\\nEmail: ralf@andrew.cmu.edu\\nOffice: 5711 Gates & Hillman Centers\\nPhone: 412-268-8298\\nResearch Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics\\n\\nJamie Callan\\nProfessor and PhD Program Director\\nEmail: callan@cs.cmu.edu\\nOffice: 5419 Gates & Hillman Centers\\nPhone: 412-268-4525\\nResearch Areas: Information Retrieval, Text Mining and Analytics\\n\\nJustine Cassell\\nProfessor (On Leave)\\nEmail: jcassell@andrew.cmu.edu\\nOffice: 5107 Gates & Hillman Centers\\nPhone: 412-204-6268\\n\\nMona Diab\\nLTI Director and Tenured Professor\\nEmail: mdiab@andrew.cmu.edu\\nPhone: 412-268-3669\\n\\nFernando Diaz\\nAssociate Professor\\nEmail: diazf@cmu.edu\\nPhone: 412-268-4229\\nResearch Areas: Information Retrieval: Recommender Systems, Retrieval and Ranking Models, Natural Language Processing: Fairness and Ethics in Language Technology, Creativity, Evaluation\\n\\nScott Fahlman\\nResearch Professor Emeritus\\nEmail: sef@cs.cmu.edu\\nOffice: 6417 Gates & Hillman Centers\\nPhone: 412-268-2575\\nResearch Areas: AI, Knowledge Representation and Reasoning, Natural Language Understanding\\n\\nRobert Frederking\\nPrincipal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director\\nEmail: ref@cs.cmu.edu\\nOffice: 6515 Gates & Hillman Centers\\nPhone: 412-268-6656\\n\\nDaniel Fried\\nAssistant Professor\\nEmail: dfried@andrew.cmu.edu\\nResearch Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI\\n\\nAnatole Gershman\\nDistinguished Service Professor\\nEmail: anatole.gershman@cs.cmu.edu\\nOffice: 6415 Gates & Hillman Centers\\nPhone: 412-268-8259\\nResearch Areas: Information Extraction, Summarization and Question Answering\\n\\nAlexander Hauptmann\\nResearch Professor\\nEmail: alex@cs.cmu.edu\\nOffice: 5519 Gates & Hillman Centers\\nPhone: 412-268-1448\\nResearch Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1c586722f8aeb2fd8704c11a3ebb0006'}>,\n",
              "  <Document: {'content': 'Mining and Analytics, Machine Learning, Multimodal Computing and Interaction\\n\\nDaphne Ippolito\\nAssistant Professor\\nEmail: daphnei@cmu.edu\\nPhone: 412-268-7250\\nResearch Areas: Natural Language Generation, Privacy and Security, Language Technology Application Areas/Issues, Creativity\\n\\nLori Levin\\nResearch Professor\\nEmail: lsl@cs.cmu.edu\\nOffice: 5717 Gates & Hillman Centers\\nPhone: 412-268-6193\\nResearch Areas: Machine Translation, Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources\\n\\nLei Li\\nAssistant Professor\\nEmail: leili@andrew.cmu.edu\\nPhone: 412-268-6355\\nResearch Areas: Machine Learning, Machine Translation, Large Language Models, AI Drug Discovery\\n\\nTeruko Mitamura\\nResearch Professor\\nEmail: teruko@cs.cmu.edu\\nOffice: 6711 Gates & Hillman Centers\\nPhone: 412-268-6596\\nResearch Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics\\n\\nLouis-Philippe Morency\\nLeonardo Associate Professor of Computer Science\\nEmail: morency@cs.cmu.edu\\nOffice: 5411 Gates & Hillman Centers\\nPhone: 412-268-5508\\nResearch Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing\\n\\nDavid Mortensen\\nAssistant Research Professor\\nEmail: dmortens@cs.cmu.edu\\nOffice: 5707 Gates & Hillman Centers\\nPhone: 412-268-2894\\nResearch Areas: Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources\\n\\nGraham Neubig\\nAssociate Professor\\nEmail: gneubig@cs.cmu.edu\\nOffice: 5409 Gates & Hillman Centers\\nResearch Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning\\n\\nEric Nyberg\\nProfessor\\nEmail: ehn@cs.cmu.edu\\nOffice: 6715 Gates & Hillman Centers\\nPhone: 412-268-7281\\nResearch Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education\\n\\nKemal Oflazer\\nTeaching Professor of Computer Science\\nEmail: ko@qatar.cmu.edu\\nOffice: 1009 Carnegie Mellon - Qatar Campus\\nPhone:\\n\\n\\n==Faculty==\\n\\nBhiksha Ramakrishnan\\nProfessor\\nEmail: bhiksha@cs.cmu.edu\\nOffice: 6705 Gates', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '44e2b447a6aa8f192c68c5dba8821565'}>,\n",
              "  <Document: {'content': '& Hillman Centers\\nPhone: 412-268-9826\\nResearch Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy\\n\\nCarolyn Rosé\\nProfessor\\nEmail: cprose@cs.cmu.edu\\nOffice: 5415 Gates & Hillman Centers\\nPhone: 412-268-7130\\nResearch Areas: Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics, Computer Supported Collaborative Learning/MOOCs\\n\\nAlexander Rudnicky\\nResearch Professor Emeritus\\nEmail: alex.rudnicky@cs.cmu.edu\\nOffice: 6511 Gates & Hillman Centers\\nPhone: 412-268-2622\\nResearch Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing\\n\\nMaarten Sap\\nAssistant Professor\\nEmail: msap2@andrew.cmu.edu\\nResearch Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue\\n\\nMichael Shamos\\nDistinguished Career Professor\\nEmail: shamos@cs.cmu.edu\\nOffice: 6707 Gates & Hillman Centers\\nPhone: 412-268-8193\\n\\nRita Singh\\nAssociate Research Professor\\nEmail: rsingh@cs.cmu.edu\\nOffice: 6703 Gates & Hillman Centers\\nPhone: 412-268-9859\\n\\nEmma Strubell\\nAssistant Professor\\nEmail: estrubel@andrew.cmu.edu\\nOffice: Gates & Hillman Centers\\n\\nAlexander Waibel\\nProfessor\\nEmail: waibel@cs.cmu.edu\\nOffice: 205 407 South Craig Street\\nPhone: 412-268-7676\\nResearch Areas: Spoken Language Translation, Machine Translation, Speech Processing, Neural Networks, Machine Learning, Multimodal Interaction, Dialog Processing\\n\\nShinji Watanabe\\nAssociate Professor\\nEmail: swatanab@andrew.cmu.edu\\nPhone: 412-268-3687\\nResearch Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing\\n\\nSean Welleck\\nAssistant Professor (Starting January 2024)\\nEmail: swelleck@andrew.cmu.edu\\n\\nEric P. Xing\\nProfessor (On Leave)\\nEmail: epxing@andrew.cmu.edu\\nOffice: 8101 Gates & Hillman Centers\\nPhone: 412-268-2559\\n\\nChenyan Xiong\\nAssociate Professor\\nEmail: cx@andrew.cmu.edu\\nPhone: 412-268-7641\\n\\nYiming Yang\\nProfessor\\nEmail: yiming@cs.cmu.edu\\nOffice:', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27fca675659240eb44594c79e91a4ddb'}>,\n",
              "  <Document: {'content': '6717 Gates & Hillman Centers\\nPhone: 412-268-1364\\n\\n\\n\\n\\n==Affiliated Faculty==\\n\\nJeffrey Bigham\\nAssociate Professor\\nEmail: jbigham@andrew.cmu.edu\\nOffice: 3525 Newell-Simon Hall\\nPhone: 412-945-0708\\n\\nMatt Gormley\\nAssistant Teaching Professor\\nEmail: mgormley@cs.cmu.edu\\nOffice: 8227 Gates & Hillman Centers\\nPhone: 412-268-7205\\n\\nIan Lane\\nAssistant Research Professor CMU Silicon Valley\\nEmail: lane@cmu.edu\\nPhone: 408-505-3178\\n\\nBrian MacWhinney\\nProfessor of Psychology at Carnegie Mellon University\\nEmail: macw@cmu.edu\\nOffice: 254M Baker Hall\\n\\nTom Mitchell\\nE. Fredkin University Professor in the Machine Learning Department\\nEmail: mitchell@andrew.cmu.edu\\nOffice: 8211 Gates & Hillman Centers\\nPhone: 412-268-2611\\n\\nJack Mostow\\nResearch Professor Emeritus\\nEmail: mostow@cs.cmu.edu\\nOffice: 3113 Newell-Simon Hall\\nPhone: 412-268-1330\\n\\nRaj Reddy\\nMoza Bint Nasser University Professor\\nEmail: rr0s@andrew.cmu.edu\\nOffice: 5327 Wean Hall\\nPhone: 412-268-2597\\n\\nRoni Rosenfeld\\nProfessor and Head: Machine Learning Department\\nEmail: roni.rosenfeld@cs.cmu.edu\\nOffice: 8002 Gates & Hillman Centers\\nPhone: 412-268-7678\\nResearch Areas: Computational Epidemiology, Dialog Systems for the Developing World\\n\\nNorman Sadeh\\nProfessor of Computer Science in the Institute for Software Research and Co-director of the MSIT in Privacy Engineering Program\\nEmail: ns1i@andrew.cmu.edu\\n\\nRichard Stern\\nProfessor\\nEmail: rms@cs.cmu.edu\\nOffice: B24 Baker-Porter Hall\\nPhone: 412-268-2535\\n\\nRodolfo M Vega\\nEmail: rmvega@andrew.cmu.edu', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 3}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '98e8a731176941e27dfdc47e3ed8abca'}>,\n",
              "  <Document: {'content': '==Adjunct Faculty==\\n\\nMalihe Alikhani\\nAssistant Professor at the University of Pittsburgh\\nEmail: malihe@pitt.edu\\n\\nTaylor Berg-Kirkpatrick\\nAssistant Professor, University of California, San Diego\\nEmail: tberg@cs.cmu.edu\\nOffice: 6403 Gates & Hillman Centers\\n\\nWilliam Cohen\\nDirector of Research Engineering at Google AI and SCS Consulting Professor\\nEmail: wcohen@cs.cmu.edu\\nOffice: 8217 Gates & Hillman Centers\\nPhone: 412-268-7664\\n\\nChristopher Dyer\\nSenior Staff Scientist for DeepMind\\nEmail: cdyer@cs.cmu.edu\\nPhone:\\nResearch Areas: Machine Learning, Machine Translation, Natural Language Processing and Computational Linguistics\\n\\nMadhavi Ganapathiraju\\nAssociate Professor Department of Biomedical Informatics at University of Pittsburgh\\nEmail: madhavi@pitt.edu\\nOffice: Gates & Hillman Centers\\nPhone:\\n\\nMatthias Grabmair\\nAssistant Professor, Technical University of Munich, Germany\\nEmail: mgrabmai@andrew.cmu.edu\\nPhone:\\n\\nLu Jiang\\nStaff Research Scientist at Google\\nEmail: lujiang@google.com\\n\\nAlon Lavie\\nVice President of Language Technologies at Unbabel and Consulting Professor at the Language Technologies Institute\\nEmail: alavie@cs.cmu.edu\\nPhone:\\nResearch Areas: Machine Translation, Natural Language Processing and Computational Linguistics\\n\\nMichael Mauldin\\nInventor of Lycos (early Internet search engine) in 1994\\n\\nFlorian Metze\\nAssociate Research Professor\\nEmail: fmetze@cs.cmu.edu\\nOffice: 202 407 South Craig Street\\nPhone: 412-268-8984\\nResearch Areas: Machine Learning, Speech Processing\\n\\nThomas Schaaf\\nPrincipal Research Scientist at 3M | M*Modal\\n\\nRavi Starzl\\nAdjunct Professor\\nEmail: rstarzl@andrew.cmu.edu\\nOffice: 6701 Gates & Hillman Centers\\nPhone: 412-268-8425\\n\\nYulia Tsvetkov\\nAssistant Professor at the University of Washington\\nEmail: yuliats@cs.washington.edu\\nOffice: Gates & Hillman Centers\\nPhone:\\n\\nMonika Woszczyna\\nHead of Speech Technology Group at Multimodal Technologies Inc.\\nEmail: monikaw@andrew.cmu.edu\\nPhone:\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1b8dddbeaf0d4f6967496e8fad9fe0cf'}>,\n",
              "  <Document: {'content': 'Ph.D. Programs\\nPh.D. in Language and Information Technology\\nOverview\\nRequirements\\nCurriculum\\nAdmission\\nHandbook\\nAdditional Info\\nThe Ph.D. in LTI focuses on developing the next generation of scientific and entrepreneurial leaders. The first two years of the Ph.D. program are similar to our MLT program. After the second year, you will spend most of your time working closely with your faculty advisor on research that advances the state-of-the-art in computer science.\\nPh.D. students are expected to publish papers about original research in the most competitive scientific journals and international conference proceedings, and to present their research at conferences and workshops. Most of our Ph.D. graduates become professors and research scientists, while a few have started their own companies.\\nIn general, students pursuing a Ph.D. in Language and Information Technologies must\\nPass at least 96 units of graduate-level courses.\\nSatisfy proficiencies in writing, presentation, programming and teaching; and\\nPropose, write and defend a Ph.D. dissertation (thesis).\\nStudents must also attend the LTI Colloquium each semester and satisfy our Research Speaking Requirement.\\nFor a detailed breakdown of the above requirements, download and read the\\nPhD Handbook\\n.\\nIn order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements:\\nAt least 72 units of LTI courses: Must', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a0cea35d5738c00f916f9b9ca7e1d189'}>,\n",
              "  <Document: {'content': \"include one class in each LTI focus area.\\nAt least 24 units of SCS courses.\\nAt least two lab courses in two different research areas.\\nHere's a sample of what your five-year schedule might look like.\\nFall\\nSpring\\nSummer\\nYear 1\\nGrammars and Lexicons\\nAlgorithms for NLP\\nDirected Study\\nSearch Engines or Machine Learning for Text Mining\\nMachine Translation\\nDirected Study\\nRequired Research\\nYear 2\\nSoftware Engineering for LT (I)\\nSpeech Understanding\\nSelf-Paced Lab\\nDirected Study\\nSoftware Engineering for LT (II)\\nSelf-Paced Lab\\nDirected Study\\nRequired Research\\nYear 3\\nDirected Research\\nDirected Research\\nDirected Research\\nYear 4\\nDirected Research\\nDirected Research\\nDirected Research\\nYear 5\\nDirected Research\\nDirected Research\\nDirected Research\\nCarnegie Mellon's School of Computer Science has a centralized\\nonline application process\\n. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered.\\nThe application period for Fall 2024 will open on September 6, 2023.\\nFinal Application Deadline\\nDecember 13, 2023 at 3 p.m. EST.\\nCost\\n$100 per program and $80 if the applicant applies before November 29, 2023 at 3 p.m. EST (early deadline).\\nFee Waivers\\nFee waivers may be available in cases of financial hardship. For more information, please refer to the\\nSchool of Computer Science Fee Waiver\\npage.\\nRequirements\\nThe School of Computer Science requires the following for all Ph.D. applications.\\nGRE scores:\\xa0GREs are now optional, but if you want to submit GRE scores:\\nThese must be less than\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '884a2137d71d68d1b5169d6c388b14e3'}>,\n",
              "  <Document: {'content': 'five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\nEnglish Proficiency Requirement:\\xa0If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required.\\xa0The English proficiency requirement cannot be waived for any reason.\\xa0Find more information under \"Test Scores\" on our\\nFAQ\\npage.\\nSuccessful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\\nOfficial transcripts from each university you have attended, regardless of whether you received your degree there.\\nCurrent resume.\\nStatement of Purpose.\\nThree letters of recommendation.\\nFor more details on these requirements, please see the\\nSCS Doctoral Admissions page\\n.\\nIn addition to the SCS guidelines, the LTI requires:\\nA short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts.\\xa0This is not a required part of the application process, but it\\'s strongly suggested.\\nAny outside funding you are receiving must be accompanied by an official award letter.\\nNo incomplete applications will be eligible for consideration.\\nFor specific application/admissions questions, please contact\\nStacey Young\\n.\\nFor a complete', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74637ebd954a599b0d74127145cb9be2'}>,\n",
              "  <Document: {'content': 'breakdown of the Ph.D. program and its policies, including information about internships, please view the\\nPhD Handbook\\n.\\nFor more information about the Ph.D. program, contact\\nStacey Young\\n.\\nDual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership)\\nOverview\\nRequirements\\nCurriculum\\nAdmission\\nAdditional Info\\nThe LTI offers a dual-degree Ph.D. in Language and Information Technologies in cooperation with:\\nUniversidade de Aveiro (\\nPh.D. in Computer Engineering\\n),\\xa0Universidade do Minho (\\nPh.D. in Informatics\\n)\\xa0 and the Universidade do Porto\\n(FCUP, Ph.D. in Computer Science and FEUP, Ph.D. in Computer Science) as part of MAPi;\\nUniversidade de Lisboa, Faculdade de Ciências – FCUL (\\nPh.D. in Informatics\\n)\\nUniversidade de Lisboa, Instituto Superior Técnico – IST \\xa0(\\nPh.D. in Computer Science and Engineering\\n,\\nPh.D. in Electrical and Computer Engineering\\n,\\nPh.D. in Information Security\\n)\\nUniversidade Nova de Lisboa, Faculdade de Ciências e Tecnologia – FCTUNL (\\nPh.D. in Computer Science\\n)\\nUniversidade de Coimbra, Faculdade de Ciências e Tecnologia – FCTUC (\\nPh.D. in Information Science and Technology\\n)\\nStudents jointly enrolled in the LTI Ph.D program spend a year in Portugal, then two years\\xa0at Carnegie Mellon taking classes in linguistics, computer science, statistical learning and task orientation.\\nAfter completing the majority of their academic requirements, students return to Portugal for the next two years to conduct extensive research, ultimately leading to a dissertation topic that will be publicly defended. One adviser from each institution', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 3}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b78cf23ce6df8d235f4bf7da1300b56'}>,\n",
              "  <Document: {'content': \"co-supervises their student’s progress and helps to define their final thesis topic.\\nStudents participating in the dual-degree program will spend their first year in Portugal, followed by two years in Pittsburgh to complete their coursework. They will complete a maximum of eight courses with a proper balance of focus areas (linguistics, computer science, statistical/learning and task orientation). After that, they will return to Portugal for their last two years, pursuing research and completing their dissertation. For more, see the\\nCarnegie Mellon | Portugal page\\n.\\nWhile in the dual Ph.D. program, your schedule may look like this.\\nFall\\nSpring\\nSummer\\nYear 1\\n(In Portugal)\\nClasses and Directed Study\\nClasses and Directed Study\\nRequired Research\\nYear 2\\n(In Pittsburgh)\\nGrammar and Lexicon\\nStructured Prediction\\nDirected Study\\nLanguage and Statistics\\n2 Self-Paced Labs\\nDirected Study\\nRequired Research\\nYear 3\\n(In Pittsburgh)\\nDirected Research\\nDirected Research\\nDirected Research\\nYear 4\\n(In Portugal)\\nDirected Research\\nDirected Research\\nDirected Research\\nYear 5\\n(In Portugal)\\nDirected Research\\nDirected Research\\nDirected Research\\nStudents applying to the dual degree program must apply through Carnegie Mellon's online application. In addition to the requirements listed below, prospective students must also contact\\nStacey Young\\nwhen applying.\\nCarnegie Mellon's School of Computer Science has a centralized\\nonline application process\\n. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered.\\nThe application period for Fall 2024 will\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 4}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b2b8f91ec1428b09c631ea6b3ca1b941'}>,\n",
              "  <Document: {'content': 'open on September 6, 2023.\\nFinal Application Deadline\\nDecember 13, 2023 3:00 p.m. EST.\\nCost\\n$100 per program, $80 for applications submitted by November 29, 2023 at 3:00 p.m. EST\\n(early deadline)\\n.\\nFee Waivers\\nFee waivers may be available in cases of financial hardship, or for participants in select programs. For more information, please refer to the\\nSchool of Computer Science Fee Waiver page\\n.\\nRequirements\\nThe School of Computer Science requires the following for all Ph.D. applications.\\nGRE scores: These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\nEnglish Proficiency Requirement:\\xa0If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required.\\xa0The English proficiency requirement cannot be waived for any reason.\\xa0Find more information under \"Test Scores\" on our\\nFAQ\\npage.\\nSuccessful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\\nOfficial transcripts from each university you have attended, regardless of whether you received your degree there.\\nCurrent resume.\\nStatement of Purpose.\\nThree letters of recommendation.\\nFor more details on these requirements, please see the\\nSCS Doctoral', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 5}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b92689a0125af644c86481f10f06b12b'}>,\n",
              "  <Document: {'content': \"Admissions page\\n.\\nIn addition to the SCS guidelines, the LTI requires:\\nA short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts.\\xa0This is not a required part of the application process, but it's strongly suggested.\\nAny outside funding you are receiving must be accompanied by an official award letter.\\nNo incomplete applications will be eligible for consideration.\\nFor specific application/admissions questions, please contact\\nStacey Young\\n.\\nFor more information, see the\\nCMU | Portugal website\\n\\nContact Us\\nLanguage Technologies Institute\\n5000 Forbes Avenue\\nPittsburgh, PA\\n15213-3891\\n412-268-6591\\nltiwebmaster@cs.cmu.edu\\nConnect\\nLogin\\n|\\nLogout\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 6}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8ed228ca66b362492e0521981203566'}>,\n",
              "  <Document: {'content': \"☰ Menu\\nAcademics\\nResearch\\nPartnership\\nNews\\nEvents\\nDEI\\nLTI Intranet\\nContact Us\\nCareers\\nApply\\nAcademics\\nResearch\\nPartnership\\nPeople\\nSearch form\\nSearch\\nLearn at LTI\\nThe LTI's degree programs draw from a common set of courses and core skills, but emphasize different types of expertise that prepare you for a wide range of career options. All of our programs provide the hands-on experience and rigorous curriculum that are the hallmark of computer science at Carnegie Mellon.\\n.\\nMaster's Programs\\nMaster of Language Technologies\\nOverview\\nRequirements\\nCurriculum\\nAdmissions\\nHandbook\\nAdditional Info\\nThe\\nMLT program\\nprepares students for a research career in academia or industry. In this program, you’ll be immersed in research for two full years. During the academic year, your time will be evenly split between taking courses and doing research with your faculty advisor. Your summer will be devoted entirely to research. Many MLT grads continue on to Ph.D. programs at CMU and other top institutions, while others pursue careers at companies emphasizing research and rapid innovation.\\nThe MLT program lasts two years (24 months), and students must complete two summers of research. Students should usually expect to graduate in August of their second year.\\nMLT students take 120 or more course units (about 10 courses), at least 72 of which are LTI courses, and 24 of which are School of Computer Science (SCS) courses. Most of these are 12-unit courses, although\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cf2a8231074e4ef48b5d6ef3f8d19e6a'}>,\n",
              "  <Document: {'content': \"lab courses are typically 6 units. Our courses generally assume knowledge of programming and data structures. The remaining units may also be taken from the LTI, or with approval from the faculty advisor, any other senior- or graduate-level course offered at CMU or Pitt.\\nDirected research is another integral part of the MLT program; MLT students carry out directed research during their studies, with guidance from their faculty advisors.\\nStudents may also choose to complete an optional MLT thesis. Guidelines can be found in the\\nMLT Handbook\\n.\\nHere's an example of how your two years in the MLT program may break down.\\nFall\\nSpring\\nSummer\\nYear 1\\nGrammars and Lexicons\\nAlgorithms for NLP\\nDirected Study\\nSearch Engines or Machine Learning for Text Mining\\nMachine Translation\\nSelf-Paced Lab\\nDirected Study\\nRequired Research\\nYear 2\\nSoftware Engineering for LT (I)\\nSpeech Understanding\\nSelf-Paced Lab\\nDirected Study\\nSoftware Engineering for LT (II)\\nDirected Study\\nElective\\nRequired Research\\nCarnegie Mellon's School of Computer Science has a centralized\\nonline application process\\n. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered.\\nThe application period for Fall 2024 will open on September 6, 2023.\\n*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f6b5e1e72d47b2c799644bfb6ed625d7'}>,\n",
              "  <Document: {'content': 'to the admissions office. If you are accepted to a program, you will be given instruction to then mail your materials.\\nFinal Application Deadline\\nDecember 13, 2023 at 3 p.m. EST.\\nCost\\n$100 per program, $80 for applications submitted before November 29, 2023 at 3PM EST\\n(early deadline)\\n.\\nFee Waivers\\nFee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the\\nSchool of Computer Science Fee Waiver page\\n.\\nRequirements\\nThe School of Computer Science requires the following for all graduate program applications:\\nGRE scores:\\xa0GREs are now optional, but if you want to submit GRE scores:\\nThese must be less than five years old. A GRE subject test in science, engineering, computer science, math, etc. is not required, but you may complete one and submit the scores if you wish. Our Institution Code is 2074; Department Code is 0402.\\nIf you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required.\\xa0The English proficiency requirement cannot be waived for any reason.\\xa0Find more information under \"Test Scores\" on our\\nFAQ\\npage.\\nSuccessful applicants will have a minimum TOEFL score', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '86721084788823827d2f985661cafea7'}>,\n",
              "  <Document: {'content': \"of 100. Our Institution Code is 4256; the Department Code is 78.\\nOfficial transcripts from each university you have attended, regardless of whether you received your degree there.\\nCurrent resume.\\nStatement of Purpose.\\nThree letters of recommendation.\\nFor more details on these requirements, please see the\\nSCS Master's Admissions page\\n.\\nIn addition to the SCS guidelines, the LTI requires:\\nA short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts.\\xa0This is not a required part of the application process, but it's strongly suggested.\\nAny outside funding you are receiving must be accompanied by an official award letter.\\nNo incomplete applications will be eligible for consideration.\\nFor specific application/admissions questions, please contact\\nKate Schaich\\n.\\nTuition Rates\\nTuition is set by the School of Computer Science and can vary by year. Current tuition rates can be found on the\\nGraduate Tuition\\nsection of the\\nStudent Financial Services website\\n.\\nFinancial Aid Resources\\nResearch Assistant-ships are occasionally offered by research advisors to current MLT students. These are not guaranteed and vary from semester-to-semester. They fluctuate and are dependent on the funding source, research advisor and MLT student.\\nStudent Financial Services\\nhas additional information on financial aid and billing / payments. They have a detailed outline of how to apply for financial aid on\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 3}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c34230a40357f08af6c89b9fb18ecebf'}>,\n",
              "  <Document: {'content': \"the\\nGraduate Financial Aid Process\\npage.\\nEnrollment & Finances\\nhas additional resource links to assist with financial aid and tuition payments.\\nGraduate Education\\n–\\nFinancial Assistance\\nprovides resources for current students regarding emergency loans and conference travel grants.\\nFor a complete breakdown of the MLT program and its policies, including information about internships, please view the\\nMLT Handbook\\n.\\nFor more information about the MLT program, contact\\nKate Schaich\\n.\\nMaster of Science in Intelligent Information Systems (MIIS)\\nOverview\\nRequirements\\nCurriculum\\nAdmission\\nHandbook\\nAdditional Info\\nThe\\nMaster's in Intelligent Information Systems\\ndegree focuses on recognizing and extracting meaning from text, spoken language and video. As an MIIS student, you’ll receive the department’s deepest exposure to content analysis and machine learning. In addition to completing the program’s coursework, you’ll work on directed study projects with your faculty advisor for two semesters; participate in a summer internship; and collaborate with your peers on a semester-long, group-oriented capstone project. This combination of classroom instruction, professional experience, and using new skills in significant projects with world-class colleagues will help prepare you for a successful career in industry or government. Our alumni have gone on to exciting careers at places like Apple, IBM and Google, and most have job offers within six weeks of graduation.\\nThe Intelligent Information Systems degree offers students the flexibility to create their own course of\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 4}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4932b6864ba6bffec85d58ef67690722'}>,\n",
              "  <Document: {'content': 'study in consultation with their advisor.\\nMIIS students gain three types of practical experience: software development supervised by their advisor (24 units equivalent to two courses); a summer internship (which can be waived for students that have sufficient prior professional experience); and a capstone project executed in a group of peers (42 units equivalent to three 12-unit courses and one 6-unit course). This combination is proven to help IIS students to broaden their skills quickly. The MIIS degree is offered in two options:\\nOption 1.\\nStandard MIIS degree (MIIS-16) - A 16-month track that is completed in three academic semesters (fall, spring, fall) and a summer internship.\\nOption 2.\\nMIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in four academic semesters (fall, spring, fall, spring) and a summer internship.\\nMIIS: Advanced Study track offers indepth degree in one of the following areas of concentration:\\nHuman Language for Language Technologies\\nLanguage Technology Application\\nMachine Learning for Language Technologies\\nPart-time education option is available in some cases.\\nMIIS-16 students must take at least 84 units (typically 7 courses) of qualifying and elective courses that satisfy human language, machine learning, and language technology applications breadth requirements. MIIS-21 students have to take at least two more courses from the selected concentration', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 5}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '925a84a6bd1bd023180eecf44a20f96f'}>,\n",
              "  <Document: {'content': 'area to satisfy their degree requirements, making it total of 108 units (typically 9 courses) of qualifying and elective courses, that also satisfy breadth requirements.\\nFor a full list of requirements, read the\\nMIIS Handbook\\n.\\nClick one of the two options below:\\nMIIS-16\\nMIIS-21\\nHere are some example schedules for completing the MIIS-16 program.\\nExample Course of Study #1\\nThis schedule would satisfy course requirements for a student interested in text mining, text analytics and question-answering systems.\\nFall 1\\nSpring\\nSummer\\nFall 2\\nMachine Learning\\nSearch Engines\\nDesign and Engineering of Intelligent Systems\\\\ Directed Study\\nLanguage and Statistics\\nNatural Language Processing\\nQuestion Answering\\nDirected Study\\nMIIS Capstone Planning Seminar\\nInternship\\nMachine Learning for Text Mining\\nMIIS Capstone Project\\nExample Course of Study #2\\nThis schedule would satisfy course requirements for a student interested in voice-based computer applications.\\nFall 1\\nSpring\\nSummer\\nFall 2\\nMachine Learning\\nAlgorithms for NLP\\nSpeech Recognition and Understanding\\nDirected Study\\nApplied Machine Learning\\nCompetitive Engineering\\nDesign and Implementation of Speech Recognition Systems\\nDirected Study\\nMIIS Capstone Planning Seminar\\nInternship\\nConversational Interfaces\\nMIIS Capstone Project\\nExample Course of Study #3\\nThis example would satisfy course requirements for a student interested in text mining, text analytics and question-answering systems who has petitioned to have the summer internship waived.\\nFall 1\\nSpring\\nSummer\\nSearch Engines\\nAnalysis of Social Media\\nDesign and Engineering of Intelligent Systems\\nDirected Study\\nMachine Learning\\nNatural Language Processing\\nQuestion Answering\\nDirected Study\\nMIIS Capstone Planning Seminar\\nAcademic Research Practices and Scientific Communities\\nMIIS Capstone Project\\nHere are some example schedules for completing the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 6}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c14401d8df001b84c1555b7b75a18da'}>,\n",
              "  <Document: {'content': \"MIIS-21 program.\\nExample Course of Study #1\\nThis schedule would satisfy course requirements for a student interested in deepening their expertise in Machine Learning area of concentration.\\nFall 1\\nSpring 1\\nSummer\\nFall 2\\nSpring 2\\nSearch Engines\\nAlgorithms for NLP\\nIntro to ML (MLD)\\nMIIS Directed Study\\nQuestion Answering\\nIntro to Deep Learning\\nMIIS Capstone Planning Seminar\\nMIIS Directed Study\\nInternship\\nMIIS Capstone Project\\nApplied ML\\nML for Text Mining\\nML for Signal Processing\\nElective\\nExample Course of Study #2\\nThis schedule would satisfy course requirements for a student interested in deepening their expertise in Language Technology Applications area of concentration.\\nFall 1\\nSpring 1\\nSummer\\nFall 2\\nSpring 2\\nSearch Engines\\nAlgorithms for NLP\\nIntro to ML (MLD)\\nMIIS Directed Study\\nQuestion Answering\\nIntro to Deep Learning\\nMIIS Capstone Planning Seminar\\nMIIS Directed Study\\nInternship\\nMIIS Capstone Project\\nMachine Translation\\nComp Semantics for NLP\\nNeural Networks for NLP\\nElective\\nExample Course of Study #3\\nThis example would satisfy course requirements for a student interested in deepening their expertise in Human Language area of concentration\\nFall 1\\nSpring 1\\nSummer\\nFall 2\\nSpring 2\\nNatural Language Processing\\nAlgorithms for NLP\\nIntro to ML (MLD)\\nMIIS Directed Study\\nQuestion Answering\\nIntro to Deep Learning\\nMIIS Capstone Planning Seminar\\nMIIS Directed Study\\nInternship\\nMIIS Capstone Project\\nLanguage and Statistics\\nComp Semantics for NLP\\nML for Signal Processing\\nElective\\nCarnegie Mellon's School of Computer Science has a centralized\\nonline application process\\n. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 7}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58e35e613191d251b2f182a0bdecac8c'}>,\n",
              "  <Document: {'content': 'be considered.\\nThe application period for Fall 2024 will open on September 6, 2023.\\n*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything to the admissions office. If you are accepted to a program, you will be given instruction to then mail your materials.\\nFinal Application Deadline\\nDecember 13, 2023 at 3:00 p.m. EST.\\nCost\\n$100 per program, $80 for applications submitted before November 29, 2023 at 3:00 p.m. EST\\n(early deadline)\\n.\\nFee Waivers\\nFee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the\\nSchool of Computer Science Fee Waiver page\\n.\\nRequirements\\nThe School of Computer Science requires the following for all Master\\'s applications.\\nGRE scores: MIIS applicants must submit their GRE scores. The scores must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\nProof of English Language Proficiency:\\nIf you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), we are required to formally evaluate your English proficiency.\\xa0We require applicants who will', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 8}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8e7c7b39888363788fa0c5b898962a14'}>,\n",
              "  <Document: {'content': 'be studying on an F-1 or J-1 visa, and for whom English is not a native language, to demonstrate English proficiency via one of these standardized tests: TOEFL (preferred), IELTS, or Duolingo.\\xa0\\xa0We discourage the use of the \"TOEFL ITP Plus for China,\" since speaking is not scored.\\nWe do not issue waivers for non-native speakers of English.\\xa0 In particular, we do not issue waivers based on previous study at a U.S. high school, college, or university.\\xa0 We also do not issue waivers based on previous study at an English-language high school, college, or university outside of the United States.\\xa0 No amount of educational experience in English, regardless of which country it occurred in, will result in a test waiver.\\nSubmit valid, recent scores: If as described above you are required to submit proof of English proficiency, your TOEFL, IELTS or Duolingo test scores will be considered valid as follows:\\xa0If you have not received a bachelor’s degree in the U.S., you will need to submit an\\xa0English proficiency score\\xa0no older than two years. (scores from exams taken before Sept. 1, 2021, will not be accepted.)\\nIf you are currently working on or have received a bachelor\\'s and/or a master\\'s degree in the U.S., you may', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 9}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e4b5e030569219de356a4a347aa29835'}>,\n",
              "  <Document: {'content': \"submit an expired test score up to five years old. (scores from exams taken before Sept. 1, 2018, will not be accepted.)\\nAdditional details about English proficiency requirements are provided on the\\nFAQ\\npage.\\nOfficial transcripts from each university you have attended, regardless of whether you received your degree there.\\nCurrent resume.\\nStatement of Purpose.\\nThree letters of recommendation.\\nFor more details on these requirements, please see the\\nSCS Master's Admissions page\\n.\\nIn addition to the SCS guidelines, the MIIS requires:\\nA short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts.\\nNo incomplete applications will be eligible for consideration.\\nFor specific application/admissions questions, please contact\\nBrianna Eriksen\\n.\\nFor a complete breakdown of the MIIS program and its policies, including information about internships, please view the\\nMIIS Handbook.\\nFor more information on the MIIS program, please contact\\nBrianna Eriksen\\n.\\nMaster of Computational Data Science (MCDS)\\nOverview\\nRequirements\\nCurriculum\\nAdmission\\nHandbook\\nAdditional Info\\nThe\\nMCDS degree\\nfocuses on engineering and deploying large-scale information systems. Our comprehensive curriculum equips you with the skills and knowledge to develop the layers of technology involved in the next generation of massive information system deployments and analyze the data these systems generate. When you graduate, you’ll have a unified vision of these systems from your core courses; internship experience; and semester-long, group-oriented\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 10}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78d7361a1978629994d6417b867e4132'}>,\n",
              "  <Document: {'content': 'capstone project. MCDS graduates are sought-after software engineers, data scientists and project managers at leading information technology, software services and social media companies.\\nThe MCDS program offers three majors: Systems, Analytics, and Human-Centered Data Science. All three require the same total number of course credits, split among required core courses, electives, data science seminar and capstone courses specifically defined for each major. The degree can also be earned in two different ways, depending on the length of time you spend working on it. Regardless of the timing option, all MCDS students must complete a minimum of 144 units to graduate.\\nHere are the options:\\nStandard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study. Each semester comprises a minimum of 48 units. This timing is typical for most students. Students graduate in December.\\nExtended Timing — a 20-month degree consisting of study for fall and spring semesters, a summer internship, and a second year of fall and spring study. Each semester comprises a minimum of 36 units. Students graduate in May.\\nFor a complete overview of the MCDS requirements, visit the\\nMCDS website\\nor read the\\nMCDS Handbook\\n.\\nTo earn an MCDS degree, student must pass courses', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 11}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e07e695f76115f8afa229ed6b8d7c994'}>,\n",
              "  <Document: {'content': 'in the core curriculum, the MCDS seminar, a concentration area and electives. Students must also complete a capstone project in which they work on a research project at CMU or on an industry-sponsored project.\\nIn total, students must complete 144 eligible units of study, including eight 12-unit courses, two 12-unit seminar courses and one 24-unit capstone course. Students must choose at minimum five core courses. The remainder of the 12-unit courses with course numbers 600 or greater can be electives chosen from the SCS course catalog. Any additional non-prerequisite units taken beyond the 144 units are also considered electives.\\nMCDS students must also pass the undergraduate course 15-513 Introduction to Computer Systems (6 units), typically in the summer before their program commences. The student must pass with a grade of B- or better. Failure to pass the course means that the student takes 15-213 during either the fall or spring semester. Note that in both cases the units do not count toward the 144 eligible units of study.\\nClick here\\nto see the MCDS Course Map.\\nSome example courses of study are included below.\\nExample 1: Analytics Major, 16 Months\\nFall\\nSpring\\nSummer\\nYear 1\\nData Science Seminar\\nMachine Learning\\nMachine Learning for Text Mining\\nAdvanced Machine Learning\\nDesign and Engineering of Intelligent Information Systems\\nBig', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 12}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5cdf56981a5f8ab9d8e4d25b1bfc79f4'}>,\n",
              "  <Document: {'content': \"Data Analytics\\nData Science Seminar\\nCapstone Planning Seminar\\nMachine Learning with Big Data Sets\\nCloud Computing\\nInformation Systems Project\\nSearch Engines\\nMultimedia Databases and Data Mining\\nLarge Scale Multimedia Analysis\\nSummer Internship\\nYear 2\\nData Science Analytics Capstone\\nExample 2: Systems Major, 16 Months\\nFall\\nSpring\\nSummer\\nYear 1\\nComputational Data Science Seminar\\nAdvanced Storage Systems\\nCloud Computing\\nDistributed Systems\\nMachine Learning\\nComputational Data Science Seminar\\nParallel Computer Architecture and Programming\\nAdvanced Databases\\nSearch Engines\\nSummer Internship\\nYear 2\\nComputational Data Science Systems Capstone\\nOperating Systems or Web Applications\\nExample 3: Human-Centered Data Science Major, 16 Months\\nExample Schedule\\nFall\\nSpring\\nEmpirical Analysis of Interactive Systems\\nML\\nEconometrics\\nSocial Web\\nNetwork Science\\nBusiness Analytics\\nInteractive Data Science\\nPsych Found for Design Impact\\nEconometrics\\nDHCS\\nSocial Web Analytics & Design\\nML\\nARM\\nSocial Web\\nNetwork Science\\nCrowd Programming\\nData Pipeline\\nML for Text Analytics\\nDHCS\\nUbiquitous Computing\\nDHCS\\nML\\nARM\\nInteractive Data Science\\nRapid Prototyping\\nGadgets\\nUsable Priv & Security\\nAdvanced ML\\nEducational Software Design\\nDHCS\\nML\\nARM\\nLearning Analytics and EDS\\nLearning with Peers\\nPsych Found for Design Impact\\nML with Big Data\\nML with Text Analysis\\nCarnegie Mellon's School of Computer Science has a centralized\\nonline application process\\n. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered.\\nThe application period for Fall 2024 will open on September 6, 2023.\\n*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything to the admissions office. If you are accepted to a program, you\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 13}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ad282b12a0cc64e9d372eb6cfec4e1a'}>,\n",
              "  <Document: {'content': 'will be given instruction to then mail your materials.\\nFinal Application Deadline\\nDecember 13, 2023 at 3:00 p.m. EST.\\nCost\\n$100 per program, $80 for applications submitted before November 29,2023 at 3:00 p.m. EST\\n(early deadline)\\n.\\nFee Waivers\\nFee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the\\nSchool of Computer Science Fee Waiver page\\n.\\nRequirements\\nThe School of Computer Science requires the following for all Ph.D. applications.\\nGRE scores: These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\nProof of English Language Proficiency:\\nIf you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), we are required to formally evaluate your English proficiency. We require applicants who will be studying on an F-1 or J-1 visa, and for whom English is not a native language, to demonstrate English proficiency via one of these standardized tests: TOEFL (preferred), IELTS, or Duolingo. We discourage the use of the \"TOEFL ITP Plus for China,\" since speaking is not scored.\\nWe do not issue waivers for non-native speakers', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 14}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '594c1dbb787d72bb72a6f75c7f761732'}>,\n",
              "  <Document: {'content': 'of English. In particular, we do not issue waivers based on previous study at a U.S. high school, college, or university. We also do not issue waivers based on previous study at an English-language high school, college, or university outside of the United States. No amount of educational experience in English, regardless of which country it occurred in, will result in a test waiver.\\nApplicants applying to MCDS are required to submit scores from an English proficiency exam taken within the last two years. Scores taken before Sept. 1, 2021, will not be accepted regardless of whether you have previously studied in the U.S. For more information about their English proficiency score policies,\\xa0visit the\\nMCDS\\nadmission website.\\nSuccessful applicants will have a minimum TOEFL score of 100 (Reading, Listening, Speaking, Writing scores all 25 or above), IELTS score of 7.5 (Reading 7 or above, Listening 7 or above, Speaking 7.5 or above, Writing 6.5 or above), or DuoLingo score of 120 (Literacy 115 or above, Comprehension 125 or above, Production 100 or above, Conversation 105 or above). Our Institution Code is 4256; the Department Code is 78. Additional details about English proficiency requirements are provided on the\\nFAQ\\npage.\\nOfficial transcripts from each university you have attended,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 15}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e53c65058d3979ba5197259a98fee7cb'}>,\n",
              "  <Document: {'content': \"regardless of whether you received your degree there.\\nA short (1-3 minutes) video of yourself. Tell us about you and why you are interested in the MCDS program. This is not a required part of the application process, but it is\\xa0STRONGLY\\xa0suggested.\\nCurrent resume.\\nStatement of Purpose.\\nThree letters of recommendation.\\nFor more details on these requirements, please see the\\nSCS Master's Admissions page\\n.\\nIn addition to the SCS guidelines, the LTI requires:\\nAny outside funding you are receiving must be accompanied by an official award letter.\\nNo incomplete applications will be eligible for consideration.\\nFor specific application/admissions questions, please contact\\nJennifer Lucas\\nor\\nCaitlin Korpus\\n.\\nFor a complete breakdown of the MCDS program and its policies, including information about internships, please view the\\nMCDS Handbook\\n.\\nFor more on the MCDS program, contact\\nJennifer Lucas\\nor\\nCaitlin Korpus\\n.\\nMaster of Science in Artificial Intelligence and Innovation (MSAII)\\nOverview\\nRequirements\\nCurriculum\\nAdmission\\nHandbook\\nAdditional Info\\nThe\\nMaster of Science in Artificial Intelligence and Innovation\\n(MSAII) program is a successor to the M.S. in Biotechnology, Innovation and Computing (MSBIC). It combines a rigorous AI and machine learning curriculum with real-world team experience in identifying an AI market niche and developing a responsive product in cooperation with external stakeholders. The core program, which lasts four semesters and leads to a capstone project, focuses on both intrapreneurship and entrepreneurship, equipping graduates to either begin\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 16}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ced77ab3b407688a97788004610bcaa'}>,\n",
              "  <Document: {'content': \"a startup or develop a new organization within an existing company. Students will also gain critical practical skills, such as making persuasive technical presentations, assembling development teams, and evaluating the potential of new market ideas.\\nIncoming students generally hold undergraduate degrees in computer science, software engineering, bioinformatics or bioengineering.\\xa0To earn the MSAII degree, you must pass courses in the Core Curriculum, the Knowledge Requirements and Electives. You must also complete a capstone project in which you work on a development project as part of the Core Curriculum. In total, you will complete 192 eligible units of study, including 84 units of Core Curriculum (including the 36-unit Capstone), 72 units of Knowledge Requirements and at least 36 units of approved Electives.\\nFor full requirements and program details, read the\\nMSAII Handbook\\n.\\nThe MSAII degree generally takes four semesters.\\xa0Here's an example of how your coursework might break down:\\nSemester One\\nSemester Two\\nSemester Three\\nSemester Four\\nArtificial Intelligence and Future Markets\\nLaw of Computer Technology\\nCoding Bootcamp\\nMachine Learning\\nAI Engineering\\nMachine Learning with Large Datasets\\nCloud Computing*\\nSoftware Engineering for Startups*\\nAI Innovation\\nNatural Language Processing\\nApplied Machine Learning\\nWeb Application Development*\\nDeep Learning\\nCapstone Project\\n(36 units)\\n*Elective course\\nFor more information on the MSAII program curriculum and requirements, see the\\nprogram website\\n.\\nCarnegie Mellon's School of Computer Science has a centralized\\nonline application process\\n. Applications and all\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 17}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '70a1a2b72849324fe350381519c50c17'}>,\n",
              "  <Document: {'content': 'supporting documentation for fall admission to any of the LTI\\'s graduate programs must be received by the application deadline. Incomplete applications will not be considered.\\nThe application period for Fall 2024 will open on September 6, 2023.\\nFinal Application Deadline\\nDecember 13, 2023 at 3:00 p.m. EST.\\nCost\\n$100 for one program, $80 if application is submitted\\nbefore\\nNovember 29, 2023 at 3PM EST\\n(early deadline).\\nFee Waivers\\nFee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the\\nSchool of Computer Science Fee Waiver page\\n.\\nRequirements\\nThe School of Computer Science requires the following for all applications:\\nA GPA of 3.0 or higher. (Students should report raw university GPA scores and NOT converted scores. Please DO NOT convert your international score to a US GPA or weighted GPA or other system).\\nGRE scores: GRE is required. Our Institution Code is 2074; Department Code is 0402.\\nEnglish Language Proficiency: If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required.\\xa0The English proficiency requirement cannot be waived for any reason.\\xa0Find more information under \"Test Scores\" on', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 18}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '30cb5944399260d185f1c5691950ea5a'}>,\n",
              "  <Document: {'content': 'our\\nFAQ\\npage.Unofficial transcripts from each university you have attended, regardless of whether you received a degree.\\nCurrent resume.\\nStatement of Purpose. A Statement of Purpose is not a resume. It should discuss your reasons for choosing the MSAII program and indicate your intended career path.\\nThree letters of recommendation.\\nA short (1-3 minutes) video of yourself. Tell us about you and why you are interested in the MSAII program. This is not a required part of the application process, but it is\\xa0STRONGLY\\xa0suggested.\\nFor specific application/admissions questions, please contact\\nAmber Vivis\\n.\\nFor a complete breakdown of the MSAII program and its policies, including information about internships, please view the\\nMSAII Handbook\\n.\\nFor more about the MSAII program, visit the\\nMSAII website\\n, or contact\\nAmber Vivis\\n.\\nUndergraduate Programs\\nLT Concentration\\nOverview\\nRequirements\\nCurriculum\\nAdmission\\nAdditional Info\\nHuman language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.\\nStudents interested in the language technologies minor must complete our', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 19}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3257c8aeb299b4fe8130a67a59e36a2'}>,\n",
              "  <Document: {'content': 'prerequisite courses with an average grade of B (3.0) or better before applying to the program. (Students who do not meet this average must submit a letter of explanation along with their application.) Prerequisites include:\\nPrinciples of Imperative Computation (15-122)\\nPrinciples of Functional Programming (15-150)\\nWe also strongly encourage candidates to take\\nDifferential and Integral Calculus (21-120) and Integration and Approximation (21-122)\\nMatrices and Linear Transformations (21-241) or Matrix Theory (21-242)\\nProbability and Computing (15-259) or Probability (21-325) or Probability Theory for Computer Scientists (36-218) or Introduction to Probability Theory (36-225)\\nThe Language Technologies Concentration requires that SCS students complete one core course and their choice of three elective courses of at least 9 units each. The electives can be chosen from a specific set of stand-alone courses. In addition to the four courses, students are required to do an undergraduate research project for at least 9 units to complete their concentration.\\nCourse Requirements for Undergraduate Minor\\nCore Course\\nElectives (Choose Three)\\nProject\\nHuman Language for Artificial Intelligence (11-411)\\nNatural Language Processing (11-411)\\nMachine Learning for Text and Graph-based Mining (11-441)\\nSearch Engines (11-442)\\nSpeech Processing (11-492)\\nMachine Learning in Practice (11-344)\\nAdvanced Natural Language Processing (11-711)\\nMachine Translation and Sequence-to-Sequence Models (11-731)\\nMultilingual Natural Language Processing (11-737)\\nNeural Networks for NLP (11-747)\\nSpeech Recognition and Understanding (11-751)\\nLanguage and Statistics (11-761)\\nMultimodal Affective', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 20}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a85897c127cf78d262538ff4a80b8d37'}>,\n",
              "  <Document: {'content': \"Computing (11-776)\\nThe Nature of Language (80-180)\\nStudents must complete a semester-long directed research project in the context of being registered for an independent study or thesis.\\xa0 This should provide hands-on experience and an in-depth study of a topic in same area as a chosen elective.\\nStudents interested in earning a minor in language technologies must apply for admission no later than September 30 of their senior year. An admission decision will usually be made within one month. Students may petition the LTI undergraduate program director to be admitted to the minor earlier or later in their undergraduate careers. To apply, contact the program's director,\\nCarolyn Rosé\\n.\\nFor more information on the undergraduate minor, contact\\nCarolyn Rosé\\n.\\nContact Us\\nLanguage Technologies Institute\\n5000 Forbes Avenue\\nPittsburgh, PA\\n15213-3891\\n412-268-6591\\nltiwebmaster@cs.cmu.edu\\nConnect\\nLogin\\n|\\nLogout\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 21}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a8de7456469c01e6b3ef825a87ec6ad1'}>,\n",
              "  <Document: {'content': '{\\n    \"Yonatan Bisk\": [\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"e41482f4ee984f17382f6cdd900df094d928be06\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 0}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2a2afb1fdfa3f7f1ccdcb16b1cb203c1'}>,\n",
              "  <Document: {'content': '              \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"WebArena: A Realistic Web Environment for Building Autonomous Agents\",\\n                    \"abstract\": \"With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 1}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4197767b468a560c2c34b40790b4cca6'}>,\n",
              "  <Document: {'content': 'we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 2}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e39229fe2715ecc6d0ac1be8be612cc7'}>,\n",
              "  <Document: {'content': '     \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2307.13854\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.\"\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 3}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '57876ad6dc6b61c9aa860f2f67d6cd62'}>,\n",
              "  <Document: {'content': '  }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 4,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"ef7d31137ef06c5be8c2824ecc5af6ce3358cc8f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 4}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad733b477ba1fe565af9e4f7de3e1641'}>,\n",
              "  <Document: {'content': '     \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Open X-Embodiment: Robotic Learning Datasets and RT-X Models\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 5}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4edc5a003b287cec18c20ce95db5503c'}>,\n",
              "  <Document: {'content': ' \"abstract\": \"\\\\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models,\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A consolidation of pretrained models in domains from NLP to Computer Vision, where large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications.\"\\n                    }\\n                },\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 6}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '530c6dc5f6825d1b173a1051d7c9271d'}>,\n",
              "  <Document: {'content': '     {\\n                    \"paperId\": \"62d205aa2c2ebab9d5b6a537eaafea86027f2e2b\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Open X-Embodiment: Robotic Learning Datasets and RT-X Models\",\\n                    \"abstract\": \"Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 7}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5e7c5262e4fd3978583ed331c430359'}>,\n",
              "  <Document: {'content': 'this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website $\\\\\\\\href{https://robotics-transformer-x.github.io}{\\\\\\\\text{robotics-transformer-x.github.io}}$.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper provides datasets in standardized data formats and models to make it possible to explore the possibility of generalist X-robot policy in the context of robotic manipulation, alongside experimental', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 8}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8e038bd8c8369ccaf7ff661645e13b40'}>,\n",
              "  <Document: {'content': 'results that provide an example of effective X-robot policies.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"94efe34c62855b5115c0e0c38a487f62bd7a8016\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Open X-Embodiment: Robotic Learning Datasets and RT-X Models\",\\n                    \"abstract\": \"\\\\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer\",\\n                    \"openAccessPdf\": null,\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 9}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd2f16294794663d4435f3dec599f0fac'}>,\n",
              "  <Document: {'content': '             \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": null\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 10}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f0a9f7d12fa470f64423200889c3e915'}>,\n",
              "  <Document: {'content': '   \"paperId\": \"5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 11}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '868a394d84f9e2dfc5914d1b3633cc4e'}>,\n",
              "  <Document: {'content': ' \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents\",\\n                    \"abstract\": \"Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM\\'s ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 12}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '220f20ec362ee3ee211bbab2c65e6d20'}>,\n",
              "  <Document: {'content': 'receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.02412\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 13}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c7d9a3447eb1cd78b29b852911326c21'}>,\n",
              "  <Document: {'content': '   \"text\": \"A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 4,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"3b0c02955e88f5862e61b560c7f70ba8cf235b1d\",\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 14}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd669cf5311b464bc2c352c1d084a3ac0'}>,\n",
              "  <Document: {'content': '          \"publicationVenue\": {\\n                        \"id\": \"fbfbf10a-faa4-4d2a-85be-3ac660454ce3\",\\n                        \"name\": \"Conference on Robot Learning\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"CoRL\",\\n                            \"Conf Robot Learn\"\\n                        ]\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 15}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '45ad1ff164bba0abdef70bad0c255885'}>,\n",
              "  <Document: {'content': '                },\\n                    \"title\": \"HomeRobot: Open-Vocabulary Mobile Manipulation\",\\n                    \"abstract\": \"HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 16}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cc31ab893c1898d4d5b758438cdcfcad'}>,\n",
              "  <Document: {'content': 'home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.11565\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 17}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e51b1f49ce549b2984dd3a4a3dfa13dd'}>,\n",
              "  <Document: {'content': '\"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The HomeRobot OVMM benchmark is introduced, where an agent navigates household environments to grasp novel objects and place them on target receptacles, and baselines achieve a 20% success rate in the real world; the experiments identify ways future research work improve performance.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"be8bbe45f74c9153d72c65fc94a1a75051240fb9\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 18}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5a9d0273e022e7b4294e091b52189d83'}>,\n",
              "  <Document: {'content': '          \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"UniTeam: Open Vocabulary Mobile Manipulation Challenge\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 19}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f04ba858fe317d361570948c3137356'}>,\n",
              "  <Document: {'content': '    \"abstract\": \"This report introduces our UniTeam agent - an improved baseline for the\\\\\"HomeRobot: Open Vocabulary Mobile Manipulation\\\\\"challenge. The challenge poses problems of navigation in unfamiliar environments, manipulation of novel objects, and recognition of open-vocabulary object classes. This challenge aims to facilitate cross-cutting research in embodied AI using recent advances in machine learning, computer vision, natural language, and robotics. In this work, we conducted an exhaustive evaluation of the provided baseline agent; identified deficiencies in perception, navigation, and manipulation skills; and improved the baseline agent\\'s performance. Notably, enhancements were made in perception - minimizing misclassifications; navigation - preventing infinite loop commitments; picking - addressing failures due to changing object visibility; and placing - ensuring accurate positioning for successful object placement.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 20}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '323850033a7e56034bf146c79080055f'}>,\n",
              "  <Document: {'content': '             \"text\": \"This report introduces the UniTeam agent - an improved baseline for the \\\\\"HomeRobot: Open Vocabulary Mobile Manipulation\\\\\" challenge, and identified deficiencies in perception, navigation, and manipulation skills; and improved the baseline agent\\'s performance.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"9976672fd95dd1b7579117a01957f5a0c46e9d01\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fbfbf10a-faa4-4d2a-85be-3ac660454ce3\",\\n                        \"name\": \"Conference on Robot Learning\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 21}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '25b34568b07e1f46f37cc179a0a2aa72'}>,\n",
              "  <Document: {'content': '                 \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"CoRL\",\\n                            \"Conf Robot Learn\"\\n                        ]\\n                    },\\n                    \"title\": \"Open-World Object Manipulation using Pre-trained Vision-Language Models\",\\n                    \"abstract\": \"For robots to follow instructions from people,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 22}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'aaea0d7b108822cc2054d347c12ab7e6'}>,\n",
              "  <Document: {'content': 'they must be able to connect the rich semantic information in human vocabulary, e.g.\\\\\"can you get me the pink stuffed whale?\\\\\"to their sensory observations and actions. This brings up a notably difficult challenge for robots: while robot learning approaches allow robots to learn many different behaviors from first-hand experience, it is impractical for robots to have first-hand experiences that span all of this semantic information. We would like a robot\\'s policy to be able to perceive and pick up the pink stuffed whale, even if it has never seen any data interacting with a stuffed whale before. Fortunately, static data on the internet has vast semantic information, and this information is captured in pre-trained vision-language models. In this paper, we study whether we can interface robot policies with these pre-trained models, with the aim of allowing robots to complete instructions involving object categories that the robot has never seen first-hand. We develop a simple approach, which we call Manipulation of Open-World Objects (MOO), which leverages a pre-trained vision-language model to extract object-identifying information from the language command and image, and conditions the robot policy on the current image, the instruction, and the extracted object information. In a variety of experiments', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 23}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eea44266440984efce47d5512de839b5'}>,\n",
              "  <Document: {'content': 'on a real mobile manipulator, we find that MOO generalizes zero-shot to a wide range of novel object categories and environments. In addition, we show how MOO generalizes to other, non-language-based input modalities to specify the object of interest such as finger pointing, and how it can be further extended to enable open-world navigation and manipulation. The project\\'s website and evaluation videos can be found at https://robot-moo.github.io/\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2303.00905\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 24}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c41e45356467ab197f1fcbd57a162661'}>,\n",
              "  <Document: {'content': '  \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Manipulation of Open-World Objects (MOO) is developed, which leverages a pre-trained vision-language model to extract object-identifying information from the language command and image, and conditions the robot policy on the current image, the instruction, and the extracted object information.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 25}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e8f5011938ac3b5ac459a5d16f0a537'}>,\n",
              "  <Document: {'content': '     \"paperId\": \"f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 26}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6f4b3d282507c0a2dc5d7fd493d0c9da'}>,\n",
              "  <Document: {'content': '   \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents\",\\n                    \"abstract\": \"Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems\\' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 27}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3d32d669b08e50f3c0a6aa6608dc022d'}>,\n",
              "  <Document: {'content': 'generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA\\'s promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.\"\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 28}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd78961ad18f930a772290608d281c0ec'}>,\n",
              "  <Document: {'content': '          }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"376f494126d1ea4f571ea0263c43ac2b6331800a\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 29}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '45acacad925b799a69c9f14ab802e9d5'}>,\n",
              "  <Document: {'content': '\"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs\",\\n                    \"abstract\": \"In', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 30}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1fd6b0bef66701ab001ac5fb9695d5b7'}>,\n",
              "  <Document: {'content': 'this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM\\'s vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.17842\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 31}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7df0059f35d31554704add5fd15aff51'}>,\n",
              "  <Document: {'content': '       \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 32}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4edaf17e2c86e898690daf6eefa93c4e'}>,\n",
              "  <Document: {'content': '     \"total\": 2722,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"6ea9bc55ee84db9d7370bcff98a46677f6f693e1\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"75f22e51-7d22-427e-aff6-cc35afbc508c\",\\n                        \"name\": \"IIAI International Conference on Advanced Applied Informatics\",\\n                        \"type\": \"conference\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 33}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c3ae24622369876b518fed3c50131c90'}>,\n",
              "  <Document: {'content': '        \"alternate_names\": [\\n                            \"IIAI-AAI\",\\n                            \"IIAI Int Conf Adv Appl Informatics\"\\n                        ]\\n                    },\\n                    \"title\": \"Movie Caption Generation with Vision Transformer and Transformer-based Language Model\",\\n                    \"abstract\": \"The paper presents a video caption generation system using Vision Transformer (ViT) and a Transformer-based language model. A video is regarded as images with time information but it is difficult to analyze a video with', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 34}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e8d2a32eabefd17babd13bebefe90143'}>,\n",
              "  <Document: {'content': 'ordinary image recognition techniques because of the difference between spacial information and time information. So, the system analyzes each image with ViT and integrates all image features into a video feature. ViT is a state-of-the-art object recognition system and consists of stacked Transformers. ViT is not trained with a training dataset but is employed with a pre-trained model. In a caption generation module, a caption is generated with Transformer decoders based on the video feature. The caption generation module is trained with a training dataset. In experiments, we use a large-scale video captioning dataset and train the proposed system. As experiment results, the proposed system achieves 0.27 F-measure in Rouge-L and we confirm that the trained system can generate appropriate captions according to input videos from the viewpoint of human judgment. The results show the proposed system is superior to the system with VGG16.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 35}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7d30a2e53cb6a218b045985ddb0f6371'}>,\n",
              "  <Document: {'content': '         \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A video caption generation system using Vision Transformer and a Transformer-based language model and it is confirmed that the trained system can generate appropriate captions according to input videos from the viewpoint of human judgment.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"38324aba4aed75c0d08408991c5f94be51ae670f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"c89c0957-b21e-40c3-9e6c-0ab46adf1e53\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 36}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '13f87ad195236384bcc5eceea21ad807'}>,\n",
              "  <Document: {'content': '            \"name\": \"International Conference on Image Analysis and Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ICIAP\",\\n                            \"Int Conf Image Anal Process\"\\n                        ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=1380\"\\n                    },\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 37}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8dc19ec4b081d75c228b9c491d090c29'}>,\n",
              "  <Document: {'content': '                  \"title\": \"SynthCap: Augmenting Transformers with Synthetic Data for Image Captioning\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://iris.unimore.it/bitstream/11380/1309206/6/2023-iciap-captioning.pdf\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 38}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5dfe9becd861c3e6aed70e4976e16e9b'}>,\n",
              "  <Document: {'content': '         \"text\": \"This work proposes a simple yet effective synthetic data augmentation framework that is capable of significantly improving the quality of captions generated by a standard Transformer-based model, leading to competitive results on the COCO dataset.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"0a36008613d67fb3aac8345f847fc4787a0d69f3\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 39}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '82142b7138e71387ad5e66dc48a4a547'}>,\n",
              "  <Document: {'content': '            \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"PixLore: A Dataset-driven Approach to Rich Image Captioning\",\\n                    \"abstract\": \"In the domain of vision-language integration, generating detailed image captions poses a significant challenge due to', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 40}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd1210fa0fffa15adf2834e7374954e0b'}>,\n",
              "  <Document: {'content': 'the lack of a curated and rich dataset. This study introduces PixLore, a novel method that leverages Querying Transformers through the fine-tuning of the BLIP-2 model using the LoRa method on a standard commercial GPU. Our approach, which involves training on a carefully assembled dataset from state-of-the-art Computer Vision models combined and augmented by ChatGPT, addresses the question of whether intricate image understanding can be achieved with an ensemble of smaller-scale models. Comparative evaluations against major models such as GPT-4 and Google Bard demonstrate that PixLore-2.7B, despite having considerably fewer parameters, is rated higher than the existing State-of-the-Art models in over half of the assessments. This research not only presents a groundbreaking approach but also highlights the importance of well-curated datasets in enhancing the performance of smaller models.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 41}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '70a3e7425c9dd061601bdfd0e855c971'}>,\n",
              "  <Document: {'content': '                  \"text\": \"PixLore is introduced, a novel method that leverages Querying Transformers through the fine-tuning of the BLIP-2 model using the LoRa method on a standard commercial GPU to address the question of whether intricate image understanding can be achieved with an ensemble of smaller-scale models.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 4,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 42}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '66fe364b87a35439977441c89da7f656'}>,\n",
              "  <Document: {'content': '              \"paperId\": \"4c711000915f8f45c54fdcb70c9db7880e3373ea\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"768b87bb-8a18-4d9c-a161-4d483c776bcf\",\\n                        \"name\": \"Computer Vision and Pattern Recognition\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"CVPR\",\\n                            \"Comput Vis Pattern Recognit\"\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 43}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6f8ef924afa5732e7d3312b198fcf491'}>,\n",
              "  <Document: {'content': '                       ],\\n                        \"issn\": \"1063-6919\",\\n                        \"url\": \"https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147\",\\n                        \"alternate_urls\": [\\n                            \"https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition\"\\n                        ]\\n                    },\\n                    \"title\": \"EXCALIBUR: Encouraging and Evaluating Embodied Exploration\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 44}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '24a33ef3b830d5751ebf0232ca5f5e61'}>,\n",
              "  <Document: {'content': '                \"abstract\": \"Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: \\\\u201cis the small heavy red bowl made from glass?\\\\u201d or \\\\u201cis there a silver spoon heavier than the egg?\\\\u201d. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 45}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c0acaf0f33507f19e0bd5cf04a583efa'}>,\n",
              "  <Document: {'content': 'world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures and represents the next frontier for embodied AI research.\"\\n                    }\\n                },\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 46}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d28b8c12ce85edd72bacf67badae0c8'}>,\n",
              "  <Document: {'content': '{\\n                    \"paperId\": \"6d30c65ca4677cd28cc99974d5f2a6d2f6175e6a\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"2baad992-2268-4c38-9120-e453622f2eeb\",\\n                        \"name\": \"Journal of Medical Internet Research\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"J Med Internet Res\"\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 47}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '79a9655efee76530f068e5a5178334ad'}>,\n",
              "  <Document: {'content': ' ],\\n                        \"issn\": \"1438-8871\",\\n                        \"url\": \"http://www.symposion.com/jmir/index.htm\",\\n                        \"alternate_urls\": [\\n                            \"http://www.jmir.org/\",\\n                            \"https://www.jmir.org/\"\\n                        ]\\n                    },\\n                    \"title\": \"Embodied Conversational Agents', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 48}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b5f8f658db0e7582d2683513e4d9f717'}>,\n",
              "  <Document: {'content': 'Providing Motivational Interviewing to Improve Health-Related Behaviors: Scoping Review\",\\n                    \"abstract\": \"Background Embodied conversational agents (ECAs) are advanced human-like interfaces that engage users in natural face-to-face conversations and interactions. These traits position ECAs as innovative tools for delivering interventions for promoting health-related behavior adoption. This includes motivational interviewing (MI), a therapeutic approach that combines brief interventions with motivational techniques to encourage the adoption of healthier behaviors. Objective This study aims to identify the health issues addressed by ECAs delivering MI interventions, explore the key characteristics of these ECAs (eg, appearance, dialogue mechanism, emotional model), analyze the implementation of MI principles and techniques within ECAs, and examine the evaluation methods and primary outcomes of studies that use ECAs providing MI interventions. Methods We conducted a scoping review following the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) methodology. Our systematic search covered the PubMed, Scopus, IEEE Xplore, ACM Digital, and PsycINFO databases for papers published between January 2008 and December 2022. We included papers describing ECAs developed for delivering MI interventions targeting health-related behaviors and excluded articles that', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 49}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8bf9b98b8492a5a71d840ff580311fdd'}>,\n",
              "  <Document: {'content': 'did not describe ECAs with human appearances and without the necessary evaluation or MI explanation. In a multistage process, 3 independent reviewers performed screening and data extraction, and the collected data were synthesized using a narrative approach. Results The initial search identified 404 articles, of which 3.5% (n=14) were included in the review. ECAs primarily focused on reducing alcohol use (n=5, 36%), took on female representations (n=9, 64%), and gave limited consideration to user ethnicity (n=9, 64%). Most of them used rules-driven dialogue mechanisms (n=13, 93%), include emotional behavior to convey empathy (n=8, 57%) but without an automatic recognition of user emotions (n=12, 86%). Regarding MI implementation, of 14 studies, 3 (21%) covered all MI principles, 4 (29%) included all processes, and none covered all techniques. Most studies (8/14, 57%) conducted acceptability, usability, and user experience assessments, whereas a smaller proportion (4/14, 29%) used randomized controlled trials to evaluate behavior changes. Overall, the studies reported positive results regarding acceptability, usability, and user experience and showed promising outcomes in changes in attitudes, beliefs, motivation, and behavior. Conclusions This study revealed significant advancements in the use of ECAs for delivering MI interventions aimed at promoting healthier behaviors over the past 15 years.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 50}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f677a2cf4e67103562d1b69ebcf95b37'}>,\n",
              "  <Document: {'content': 'However, this review emphasizes the need for a more in-depth exploration of ECA characteristics. In addition, there is a need for the enhanced integration of MI principles, processes, and techniques into ECAs. Although acceptability and usability have received considerable attention, there is a compelling argument for placing a stronger emphasis on assessing changes in attitudes, beliefs, motivation, and behavior. Consequently, inclusion of more randomized controlled trials is essential for comprehensive intervention evaluations.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.jmir.org/2023/1/e52097/PDF\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 51}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '33ed90ae6f8759a44738f3b540943a8'}>,\n",
              "  <Document: {'content': '       \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Significant advancements in the use of ECAs for delivering MI interventions aimed at promoting healthier behaviors over the past 15 years are revealed, but there is a need for the enhanced integration of MI principles, processes, and techniques into ECAs.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"01de5efb4619533643cf948c492c71f7cf4be3ec\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"89097a03-8be6-4e2d-ae2c-a6df64c77a06\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 52}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cccca000df5fb86d811eea224ec4a44e'}>,\n",
              "  <Document: {'content': '               \"name\": \"Frontiers in Psychology\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Front Psychol\"\\n                        ],\\n                        \"issn\": \"1664-1078\",\\n                        \"url\": \"http://www.frontiersin.org/Cultural_Psychology\",\\n                        \"alternate_urls\": [\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 53}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d017263595dae53f1d8742588a7bd45'}>,\n",
              "  <Document: {'content': '                        \"http://frontiersin.org/psychology/\",\\n                            \"https://www.frontiersin.org/journals/psychology\",\\n                            \"http://journal.frontiersin.org/journal/psychology\",\\n                            \"http://www.frontiersin.org/psychology\"\\n                        ]\\n                    },\\n                    \"title\": \"Sustainability pedagogy: Understanding, exploring and internalizing nature\\\\u2019s complexity and coherence\",\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 54}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4d7bf9faa9b431ddbf7c8f7096ee617a'}>,\n",
              "  <Document: {'content': '  \"abstract\": \"Online learning during the COVID-19 pandemic has affected student academic performance as well as mental, physical, and social wellbeing. During a lockdown at the University of Toronto in Canada (September 2020\\\\u2013April 2021), my students expressed an underlying sense of monotony yet uncertainty. I recalled a contrasting paradox from the teachings of Indigenous Cree on mental wellness in land-based experiences: a sense of stimulation and security that we can liken to variations of Appleton\\\\u2019s prospect-refuge theory. I modified my Environmental Science and Pathways to Sustainability course to support stimulation and security through embodied, interactive pedagogy at student-selected individual field sites. My main goals were to (i) support student mental wellness and (ii) provide an alternative to experiential field trips for understanding and connecting with nature as an adaptive complex system. I prompted students with field activities contextualized by a course narrative that purposefully directed attention to nature through intrinsically motivated curiosity, exploration, and discovery; conditions more similar to evolutionary environments of adaptedness than \\\\u201cgetting away\\\\u201d in passive retreats. Student weekly field observations and reflections culminated in a post-intervention Reflection Assignment (n\\\\u2009=\\\\u200915) which became the bases of thematic and narrative analysis. Other assignments were added to my evaluation of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 55}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78cbf0fa0cdde02ddf2ec3ca83ced17c'}>,\n",
              "  <Document: {'content': 'complexity comprehension. The intervention successfully instilled security and stimulation via purpose-directed attention to different aspects of nature in the same setting followed by periods of knowledge integration. This empowered students with sustainability mindsets indicated by greater self-reported: sense of coherence, change agency, cognitive and affective restoration, nature connectedness, nature relatedness, social connectedness, and pro-environmental values. Assignments demonstrated an understanding of the environment as an adaptive complex system that was not present at the beginning of the course. Some students\\\\u2019 self-construct adopted nature and its complexity, empowering them with greater trait resilience. This work speaks to opportunities for merging psychological restoration and analytical curricula by integrating cognitive and sensory meaningfulness in sustainability narratives. It asks scholars to reflect on how we operationalize foundational theories of Environmental Psychology based on ancestral survival conditions and encourages empirical research to consider how sociocultural contexts can direct attention to nature through purposeful inquiry.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.frontiersin.org/articles/10.3389/fpsyg.2022.922275/pdf\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 56}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '175641858f277e9d210da103c2428af6'}>,\n",
              "  <Document: {'content': '                 \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": null\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 3,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"e7b3b692b0816821aafc0d354749bc3802cbf6ac\",\\n                    \"publicationVenue\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 57}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '119f38a027fe834057776e5660dd9e0b'}>,\n",
              "  <Document: {'content': '{\\n                        \"id\": \"939c6e1d-0d17-4d6e-8a82-66d960df0e40\",\\n                        \"name\": \"International Conference on Learning Representations\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Int Conf Learn Represent\",\\n                            \"ICLR\"\\n                        ],\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 58}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e17d48382416bbe24bbbfa5d59724a10'}>,\n",
              "  <Document: {'content': '           \"url\": \"https://iclr.cc/\"\\n                    },\\n                    \"title\": \"Computational Language Acquisition with Theory of Mind\",\\n                    \"abstract\": \"Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 59}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dae3805e873fbdc8e06d29dd5cd505c8'}>,\n",
              "  <Document: {'content': 'will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2303.01502\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 60}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5f83072db905127b22d28ad7415de32'}>,\n",
              "  <Document: {'content': '              \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors\\' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"3b75900a3dba7518e4314c8963c06d8110ddf779\",\\n                    \"publicationVenue\": null,\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 61}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd7e18f0359435e43d85ce35cea0e0c59'}>,\n",
              "  <Document: {'content': ' \"title\": \"The Nature and Ontogenesis of Meaning\",\\n                    \"abstract\": \"Contents: Preface. W.F. Overton, Contexts of Meaning: The Computational and the Embodied Mind. K.J. Gergen, The Communal Creation of Meaning. G. Lakoff, What Is a Conceptual System? M. Turner, Design for a Theory of Meaning. E.K. Scholnick, K. Cookson, A Developmental Analysis of Cognitive Semantics: What Is the Role of Metaphor in the Construction of Knowledge and Reasoning? R. Jackendoff, Word Meanings and What It Takes to Learn Them: Reflections on the Piaget-Chomsky Debate. J. Macnamara, The Foundations of Logic and the Foundations of Cognition. T. Brown, Affective Dimensions of Meaning. J. Langer, From Acting to Understanding: The Comparative Development of Meaning. L. Bloom, Meaning and Expression. K. Hirsh-Pasek, R.M. Golinkoff, L. Reeves, Constructivist Explanations for Language Acquisition May Be Insufficient: The Case for Language-Specific Principles. C. Feldman, J. Bruner, D. Kalmar, B. Renderer, Plot, Plight, and Dramatism: Interpretation at Three Ages. R.F. Kitchener, Semantic Naturalism: The Problem of Meaning and Naturalistic Psychology.\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 62}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '81d048d7df96a67cd0a5b061d6b95634'}>,\n",
              "  <Document: {'content': '\"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"8dfb3d2f99e820968e33a8807492db3b42b61e9e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"ae41bf69-3ec1-4492-9531-dbef8290008c\",\\n                        \"name\": \"Heritage Language Journal\",\\n                        \"alternate_names\": [\\n                          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 63}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f0dfe195400b1fcecd0f1ea0af4bdf84'}>,\n",
              "  <Document: {'content': ' \"Heritage Lang J\"\\n                        ],\\n                        \"issn\": \"1550-7076\",\\n                        \"url\": \"https://www.international.ucla.edu/lrc/hlj/index.asp\",\\n                        \"alternate_urls\": [\\n                            \"http://www.heritagelanguages.org/\"\\n                        ]\\n                    },\\n                    \"title\": \"Theory of Mind and Evidentiality', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 64}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5406e491aad9d7be8cbb1486e19cb38'}>,\n",
              "  <Document: {'content': 'Acquisition in Heritage Bilingual Aymara-Spanish Children in the North of Chile\",\\n                    \"abstract\": \"\\\\nThe present work examines the connection between language and conceptual development, investigating whether false-belief reasoning (FBR) and source-monitoring ability (SMA), abilities within the theory of mind (ToM) framework, trigger the comprehension of semantic and pragmatic knowledge (evidential scalar implicatures) encoded in the evidentiality structure (Papafragou et al., 2007). Furthermore, the present work examines whether age modulates the comprehension of ToM abilities, and whether age influences comprehension of grammar knowledge encoded in the target structure. Twenty-one bilingual Aymara-Spanish heritage children performed one FBR task and one SMA task. In addition, the same group and 15 bilingual Aymara-Spanish adults performed one semantic and one pragmatic Aymara evidentiality task. The results showed that age was relevant in developing ToM abilities and comprehending the target structure\\\\u2019s evidential semantic features. Furthermore, the results demonstrated that semantic features had a positive relationship with the acquisition of ToM abilities, as opposed to pragmatic knowledge. These outcomes suggest that the computation of Aymara evidential scalar implicatures is lexically driven as opposed to contextually driven.\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 65}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74ee3d556c0a3d3f5eb966869a79ea06'}>,\n",
              "  <Document: {'content': '                \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 4,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 66}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e144e237f51d0f914c147ee243c99070'}>,\n",
              "  <Document: {'content': '            \"paperId\": \"ef7d31137ef06c5be8c2824ecc5af6ce3358cc8f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 67}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6b6deb277affbff731bb991a0d778dd'}>,\n",
              "  <Document: {'content': '          \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Open X-Embodiment: Robotic Learning Datasets and RT-X Models\",\\n                    \"abstract\": \"\\\\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models,\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 68}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd5cfa310afd6c1c11f3b570e7e7dcfd4'}>,\n",
              "  <Document: {'content': '  \"text\": \"A consolidation of pretrained models in domains from NLP to Computer Vision, where large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"62d205aa2c2ebab9d5b6a537eaafea86027f2e2b\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Open X-Embodiment: Robotic Learning Datasets and RT-X Models\",\\n                    \"abstract\": \"Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 69}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '39083820964fcfec55ae89235ab2b44c'}>,\n",
              "  <Document: {'content': 'to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website $\\\\\\\\href{https://robotics-transformer-x.github.io}{\\\\\\\\text{robotics-transformer-x.github.io}}$.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 70}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd9ddd842ecf4ce6f542c19f4e7d27d00'}>,\n",
              "  <Document: {'content': '                      \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper provides datasets in standardized data formats and models to make it possible to explore the possibility of generalist X-robot policy in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"94efe34c62855b5115c0e0c38a487f62bd7a8016\",\\n                    \"publicationVenue\": null,\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 71}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'becc10a7264ff0c803722e4982da9cdd'}>,\n",
              "  <Document: {'content': '\"title\": \"Open X-Embodiment: Robotic Learning Datasets and RT-X Models\",\\n                    \"abstract\": \"\\\\u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": null\\n                    }\\n                }\\n            ]\\n        },\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 72}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a984041ba5fadee65600975ec803083f'}>,\n",
              "  <Document: {'content': '      {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"6140211405f9917ded519da50f00eee989eabd7f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 73}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '595d5b0e9379ddc0a8581b11d2891532'}>,\n",
              "  <Document: {'content': '      \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis\",\\n                    \"abstract\": \"Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 74}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ba8a2f01d600e757e51bce7ca77e782b'}>,\n",
              "  <Document: {'content': 'and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository of resources, including papers reviewed in this survey as well as related projects and repositories for developing foundation models for robotics.\",\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 75}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c1ed4e0ff0f776d7510e666897c78669'}>,\n",
              "  <Document: {'content': '            \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"An overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable is provided and a taxonomy is established to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 76}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e106d95147c10da84febdea7bd927b5e'}>,\n",
              "  <Document: {'content': '   \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"b777aa86b5a1d49ce8eababc5c2ee56d3562801e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 77}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3448640137439ff7dad6514636fa2ef'}>,\n",
              "  <Document: {'content': '                       \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 78}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bf3685d098914c2d2174a6529363dfdd'}>,\n",
              "  <Document: {'content': '          \"title\": \"The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment\",\\n                    \"abstract\": \"Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\\\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.\",\\n                    \"openAccessPdf\": {\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 79}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4162efd88f5abe438ae5b89f81dac2e2'}>,\n",
              "  <Document: {'content': '   \"url\": \"http://arxiv.org/pdf/2302.06117\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.\"\\n                    }\\n                }\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 80}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3a0c5c273af7c552c912a127afe09671'}>,\n",
              "  <Document: {'content': '       ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"8035a247980cb18abf2bb7b9d96e7d4c63622ef2\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 81}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb2ec6c225a21a2b5080447edb68f0e7'}>,\n",
              "  <Document: {'content': '                       \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Reasoning about the Unseen for Efficient Outdoor Object Navigation\",\\n                    \"abstract\": \"Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 82}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e326fa947d83a51ea3f67aef381a0c37'}>,\n",
              "  <Document: {'content': 'semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2309.10103\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 83}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6662f7faa4165854c4824679ad27fd1f'}>,\n",
              "  <Document: {'content': '       \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A new task OUTDOOR is introduced, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain are introduced.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 84}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3503732849ae5f9c3b717f40b14def53'}>,\n",
              "  <Document: {'content': '           \"total\": 6,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"9500f1f9cee4a84cc1f9f07cd1038435545d9c44\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fbfbf10a-faa4-4d2a-85be-3ac660454ce3\",\\n                        \"name\": \"Conference on Robot Learning\",\\n                        \"type\": \"conference\",\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 85}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '230bf45635cb1986fdc90a066f75efd8'}>,\n",
              "  <Document: {'content': '           \"alternate_names\": [\\n                            \"CoRL\",\\n                            \"Conf Robot Learn\"\\n                        ]\\n                    },\\n                    \"title\": \"SLAP: Spatial-Language Attention Policies\",\\n                    \"abstract\": \"Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 86}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b516fb3dfec70ffff6a632a3ce28b8b0'}>,\n",
              "  <Document: {'content': 'robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 87}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1a5bfba85d996740793c7cec6f1ccc00'}>,\n",
              "  <Document: {'content': '                      \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The proposed Spatial-Language Attention Policies (SLAP) uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy, which shows an 80% success rate in the real world across eight tasks with a single model, and a 4x improvement over baseline in mobile manipulation setting.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"55a65286626dc763bdb4067454a455cfb8300739\",\\n                    \"publicationVenue\": {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 88}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1a1828e02a0a756ac9af856ca64532ee'}>,\n",
              "  <Document: {'content': '              \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 89}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '53088999a63991935b971bf8e61aa261'}>,\n",
              "  <Document: {'content': '       \"title\": \"Spatial-Language Attention Policies for Efficient Robot Learning\",\\n                    \"abstract\": \"\\\\u2014We investigate how to build and train spatial rep- resentations for robot decision making with Transformers. In particular, for robots to operate in a range of environments, we must be able to quickly train or \\\\ufb01ne-tune robot sensorimotor policies that are robust to clutter, data ef\\\\ufb01cient, and generalize well to different circumstances. As a solution, we propose Spatial Language Attention Policies (SLAP). SLAP uses three- dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object con\\\\ufb01gurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and con\\\\ufb01gurations). See videos on our website: https://robotslap.github.io\",\\n                    \"openAccessPdf\": {\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 90}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c169a99457805c78f35e9dec5135b36d'}>,\n",
              "  <Document: {'content': '                    \"url\": \"https://arxiv.org/pdf/2304.11235\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Spatial Language Attention Policies (SLAP) uses three- dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy.\"\\n                    }\\n                },\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 91}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bc9f3da5c3b5b7525060d596aef0a011'}>,\n",
              "  <Document: {'content': '          {\\n                    \"paperId\": \"7fe4b9eff9cfced12fa083e5eeb70d549b492337\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"38ef5a81-0fad-4de7-abc2-0fb847d3ece7\",\\n                        \"name\": \"Applied and Computational Engineering\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Appl Comput Eng\"\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 92}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2c7999b32bed4293e64b596d99e66c59'}>,\n",
              "  <Document: {'content': '         ],\\n                        \"issn\": \"2755-2721\"\\n                    },\\n                    \"title\": \"The estimation of spatial distribution patterns of different socio-economic status (SES) groups by housing advertisement data and machine learning techniques: a case study in brooklyn, new york\",\\n                    \"abstract\": \"Poverty eradication has long been a central issue for sustainable development goals (SDGs), which draws attention to the issue of urban inequalities that can hinder regional economic development and increase unemployment and crime rates. It is critical to understand the local socio-economic distribution pattern for better urban policies and planning strategies. Traditional SES measurements are mainly based on census data and surveys, which are slowly updated and often fail to apply in the latest analysis. The SES inference', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 93}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd26f2f0d18a5cea596ca4e64b515b487'}>,\n",
              "  <Document: {'content': 'methods using other data (e.g., satellite maps, nighttime lighting data) lack a theoretical basis and are of coarse resolution. The study takes advantage of the latest data (i.e., online housing advertisement data) and point of interests (POIs) to infer fine-grained block-group-level SES in Brooklyn through machine learning techniques. In addition, natural language processing (NLP) methods are used to derive twelve housing-related SES predictors. The results show that the speculative models and predictors are feasible, and the Global decision tree (GBDT) algorithm is the most efficient of the seven algorithms. The SES distribution map demonstrates a clear socio-economic stratification in Brooklyn. The rich are mainly concentrated in the western and northern areas with a high density of facilities. Based on the analysis of the local SES, three policy recommendations are proposed. First, for the inequitable distribution of facilities, additional investment should be made in the central and eastern regions. Second, a high level of greenery should be given priority in urban planning. Third, in terms of housing, disadvantaged groups should be given attention.\",\\n                    \"openAccessPdf\": {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 94}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e4d6cbe04b9a43687e7863da63dd31a'}>,\n",
              "  <Document: {'content': '                \"url\": \"https://ace.ewapublishing.org/media/b0ee933ad3c248c8bb7dca10734159a6.marked_F1pIj3e.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This study takes advantage of the latest data and point of interests to infer fine-grained block-group-level SES in Brooklyn through machine learning techniques and shows that the speculative models and predictors are feasible, and the Global decision tree (GBDT) algorithm is the most efficient of the seven algorithms.\"\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 95}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f03bdee0f548347050c0105ad262518f'}>,\n",
              "  <Document: {'content': '}\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        }\\n    ],\\n    \"Ralf Brown\": [\\n        {\\n            \"total\": 11703,\\n            \"offset\": 0,\\n            \"next\": 20,\\n            \"data\": [\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 96}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9e746926ac38d2bcb54d2e944dd1270'}>,\n",
              "  <Document: {'content': '      {\\n                    \"paperId\": \"22437bb3fc122338a91444c746594dabde278e8c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"12cf261e-f151-4062-8f3d-93679fa07009\",\\n                        \"name\": \"Advanced Energy Materials\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Adv Energy Mater\"\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 97}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e02949c5c693fb51444d193a1667035'}>,\n",
              "  <Document: {'content': '    ],\\n                        \"issn\": \"1614-6832\",\\n                        \"url\": \"http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1614-6840\",\\n                        \"alternate_urls\": [\\n                            \"https://onlinelibrary.wiley.com/page/journal/16146840/homepage/productinformation.html\"\\n                        ]\\n                    },\\n                    \"title\": \"Multi\\\\u2010Dimensional Characterization of Battery Materials\",\\n                    \"abstract\": \"Demand for low', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 98}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f2537757367d24c2600279f5c8b92d74'}>,\n",
              "  <Document: {'content': 'carbon energy storage has highlighted the importance of imaging techniques for the characterization of electrode microstructures to determine key parameters associated with battery manufacture, operation, degradation, and failure both for next generation lithium and other novel battery systems. Here, recent progress and literature highlights from magnetic resonance, neutron, X\\\\u2010ray, focused ion beam, scanning and transmission electron microscopy are summarized. Two major trends are identified: First, the use of multi\\\\u2010modal microscopy in a correlative fashion, providing contrast modes spanning length\\\\u2010 and time\\\\u2010scales, and second, the application of machine learning to guide data collection and analysis, recognizing the role of these tools in evaluating large data streams from increasingly sophisticated imaging experiments.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aenm.202300103\",\\n                        \"status\": \"HYBRID\"\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 99}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '84a875efd2de454dbf1eabec1702d1b5'}>,\n",
              "  <Document: {'content': '},\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"c71d93ad0408d5d58ede911efaa7202d6e36fe07\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"a7a5b67a-e18c-40e8-ae2b-22e31da230d1\",\\n                        \"name\": \"Surface & Coatings Technology\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 100}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3aa97a11f353f64d7a4461df464a075a'}>,\n",
              "  <Document: {'content': '                          \"Surf  Coat Technol\"\\n                        ],\\n                        \"issn\": \"0257-8972\",\\n                        \"url\": \"http://www.sciencedirect.com/science/journal/02578972\"\\n                    },\\n                    \"title\": \"Thermal evolution, phase composition and fracture toughness of electroless Ni-P, Ni-W-P and Ni-Mo-W-P films for solderable surfaces on copper\",\\n                    \"abstract\": null,\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 101}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6501c0d3181c0d00632a8c1d57a58125'}>,\n",
              "  <Document: {'content': '   \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"0de4784c156189416b1bf913e6312ae05424ed8b\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"6eeec983-c589-4af3-ba18-64fb589ba627\",\\n                        \"name\": \"Journal of General Virology\",\\n                        \"type\": \"journal\",\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 102}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '38e584f552e1e76e2d1866850d9cb50'}>,\n",
              "  <Document: {'content': ' \"alternate_names\": [\\n                            \"J Gen Virol\"\\n                        ],\\n                        \"issn\": \"0022-1317\",\\n                        \"url\": \"https://www.microbiologyresearch.org/content/journal/jgv\",\\n                        \"alternate_urls\": [\\n                            \"http://jgv.sgmjournals.org/\"\\n                        ]\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 103}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c29a6d25d7019914f49822a6fea3a4b0'}>,\n",
              "  <Document: {'content': '   },\\n                    \"title\": \"Annual (2023) taxonomic update of RNA-directed RNA polymerase-encoding negative-sense RNA viruses (realm Riboviria: kingdom Orthornavirae: phylum Negarnaviricota)\",\\n                    \"abstract\": \"In April 2023, following the annual International Committee on Taxonomy of Viruses (ICTV) ratification vote on newly proposed taxa, the phylum Negarnaviricota was amended and emended. The phylum was expanded by one new family, 14 new genera, and 140 new species. Two genera and 538 species were renamed. One species was moved, and four were abolished. This article presents the updated taxonomy of Negarnaviricota as now accepted by the ICTV.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 104}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e4b5d43669aeaf837a99735596ec01f6'}>,\n",
              "  <Document: {'content': '                     \"text\": \"The updated taxonomy of Negarnaviricota is presented, as now accepted by the International Committee on Taxonomy of Viruses.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"15cc546825d2edf97668258a366057c4db8e4b88\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"cb30f0c9-2980-4b7d-bbcb-68fc5472b97c\",\\n                        \"name\": \"Science Advances\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 105}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e6fef957975bb841ace1de9153c73f1'}>,\n",
              "  <Document: {'content': '        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Sci Adv\"\\n                        ],\\n                        \"issn\": \"2375-2548\",\\n                        \"url\": \"http://www.scienceadvances.org/\",\\n                        \"alternate_urls\": [\\n                            \"https://advances.sciencemag.org/\"\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 106}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62681f4d8aef4921e673fae7fc431acf'}>,\n",
              "  <Document: {'content': '              ]\\n                    },\\n                    \"title\": \"Density declines, richness increases, and composition shifts in stream macroinvertebrates\",\\n                    \"abstract\": \"Documenting trends of stream macroinvertebrate biodiversity is challenging because biomonitoring often has limited spatial, temporal, and taxonomic scopes. We analyzed biodiversity and composition of assemblages of >500 genera, spanning 27 years, and 6131 stream sites across forested, grassland, urban, and agricultural land uses throughout the United States. In this dataset, macroinvertebrate density declined by 11% and richness increased by 12.2%, and insect density and richness declined by 23.3 and 6.8%, respectively, over 27 years. In addition, differences in richness and composition between urban and agricultural versus forested and grassland streams have increased over time. Urban and agricultural streams lost the few disturbance-sensitive taxa they once had and gained disturbance-tolerant taxa. These results suggest that current', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 107}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '73ac0c457a58f066e700db8461930339'}>,\n",
              "  <Document: {'content': 'efforts to protect and restore streams are not sufficient to mitigate anthropogenic effects.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.science.org/doi/pdf/10.1126/sciadv.adf4896?download=true\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"2ba1c826ae7538180ca4af47f844e445f618b9e8\",\\n                    \"publicationVenue\": {\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 108}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '340ae81dba2ccef9cfb977b4127597a4'}>,\n",
              "  <Document: {'content': '                      \"id\": \"799a0280-441b-4633-bba4-975ba837fb4f\",\\n                        \"name\": \"Atmospheric Chemistry and Physics\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Atmospheric Chem Phys\"\\n                        ],\\n                        \"issn\": \"1680-7316\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 109}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f9039cadc10f632becd9cb8bd223b224'}>,\n",
              "  <Document: {'content': '    \"url\": \"https://www.atmospheric-chemistry-and-physics.net/\",\\n                        \"alternate_urls\": [\\n                            \"https://www.cosis.net/members/journals/df/volumes.php?j_id=2\",\\n                            \"https://www.atmospheric-chemistry-and-physics.net/home.html\",\\n                            \"http://www.atmospheric-chemistry-and-physics.net/\"\\n                        ]\\n                    },\\n                    \"title\": \"Comparison of isoprene chemical mechanisms under atmospheric night-time conditions in chamber experiments: evidence of hydroperoxy aldehydes and epoxy products from NO3', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 110}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd57c89c957fc2954fcb22e66e55deea3'}>,\n",
              "  <Document: {'content': 'oxidation\",\\n                    \"abstract\": \"Abstract. The gas-phase reaction of isoprene with the nitrate radical (NO3) was investigated in experiments in the outdoor SAPHIR chamber under atmospherically relevant conditions specifically with respect to the chemical lifetime and fate of nitrato-organic peroxy radicals (RO2). Observations of organic products were compared to concentrations expected from different chemical mechanisms: (1) the Master Chemical Mechanism, which simplifies the NO3 isoprene chemistry by only considering one RO2 isomer; (2) the chemical mechanism derived from experiments in the Caltech chamber, which considers different RO2 isomers; and (3) the FZJ-NO3 isoprene mechanism derived from quantum chemical calculations, which in addition to the Caltech mechanism includes equilibrium reactions of RO2 isomers, unimolecular reactions of nitrate RO2 radicals and epoxidation reactions of nitrate alkoxy radicals. Measurements using mass spectrometer instruments give evidence that the new reactions pathways predicted by quantum chemical calculations play a role in the NO3 oxidation of isoprene. Hydroperoxy aldehyde (HPALD) species, which are specific to unimolecular reactions of nitrate RO2, were detected even in the presence of an OH scavenger, excluding the possibility that concurrent oxidation by hydroxyl radicals', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 111}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bb535e1669218f271bebd7b323bc87e5'}>,\n",
              "  <Document: {'content': '(OH) is responsible for their formation. In addition, ion signals at masses that can be attributed to epoxy compounds, which are specific to the epoxidation reaction of nitrate alkoxy radicals, were detected. Measurements of methyl vinyl ketone (MVK) and methacrolein (MACR) concentrations confirm that the decomposition of nitrate alkoxy radicals implemented in the Caltech mechanism cannot compete with the ring-closure reactions predicted by quantum chemical calculations. The validity of the FZJ-NO3 isoprene mechanism is further supported by a good agreement between measured and simulated hydroxyl radical (OH) reactivity. Nevertheless, the FZJ-NO3 isoprene mechanism needs further investigations with respect to the absolute importance of unimolecular reactions of nitrate RO2 and epoxidation reactions of nitrate alkoxy radicals. Absolute concentrations of specific organic nitrates such as nitrate hydroperoxides would be required to experimentally determine product yields and branching ratios of reactions but could not be measured in the chamber experiments due to the lack of calibration standards for these compounds. The temporal evolution of mass traces attributed to product species such as nitrate hydroperoxides, nitrate carbonyl and nitrate alcohols as well as hydroperoxy aldehydes observed by the mass spectrometer instruments demonstrates that further oxidation by the nitrate radical and ozone at atmospheric concentrations', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 112}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3626551b36b3f9b0d090a75078cb129e'}>,\n",
              "  <Document: {'content': 'is small on the timescale of one night (12\\\\u2009h) for typical oxidant concentrations. However, oxidation by hydroxyl radicals present at night and potentially also produced from the decomposition of nitrate alkoxy radicals can contribute to their nocturnal chemical loss.\\\\n\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"7dc42f0328ddf2718863333539f2c8e2b059b676\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Euclid preparation: XXVIII. Modelling of the weak lensing angular power spectrum\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 113}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2a11a431edc99eacc60041e9cec423a6'}>,\n",
              "  <Document: {'content': '     \"abstract\": \"This work considers which higher-order effects in modelling the cosmic shear angular power spectra must be taken into account for Euclid. We identify which terms are of concern, and quantify their individual and cumulative impact on cosmological parameter inference from Euclid. We compute the values of these higher-order effects using analytic expressions, and calculate the impact on cosmological parameter estimation using the Fisher matrix formalism. We review 24 effects and find the following potentially need to be accounted for: the reduced shear approximation, magnification bias, source-lens clustering, source obscuration, local Universe effects, and the flat Universe assumption. Upon computing these explicitly, and calculating their cosmological parameter biases, using a maximum multipole of $\\\\\\\\ell=5000$, we find that the magnification bias, source-lens clustering, source obscuration, and local Universe terms individually produce significant ($\\\\\\\\,>0.25\\\\\\\\sigma$) cosmological biases in one or more parameters, and accordingly must be accounted for. In total, over all effects, we find biases in $\\\\\\\\Omega_{\\\\\\\\rm m}$, $\\\\\\\\Omega_{\\\\\\\\rm b}$, $h$, and $\\\\\\\\sigma_{8}$ of $0.73\\\\\\\\sigma$, $0.28\\\\\\\\sigma$, $0.25\\\\\\\\sigma$, and $-0.79\\\\\\\\sigma$, respectively, for flat $\\\\\\\\Lambda$CDM. For the $w_0w_a$CDM case, we find biases in $\\\\\\\\Omega_{\\\\\\\\rm m}$, $\\\\\\\\Omega_{\\\\\\\\rm b}$, $h$, $n_{\\\\\\\\rm s}$, $\\\\\\\\sigma_{8}$, and $w_a$ of $1.49\\\\\\\\sigma$, $0.35\\\\\\\\sigma$, $-1.36\\\\\\\\sigma$, $1.31\\\\\\\\sigma$, $-0.84\\\\\\\\sigma$,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 114}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0c7ca4d2027b8ce6b1017cc7c559cd7'}>,\n",
              "  <Document: {'content': 'and $-0.35\\\\\\\\sigma$, respectively; which are increased relative to the $\\\\\\\\Lambda$CDM due to additional degeneracies as a function of redshift and scale.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"e221925d581d472d220c436e6f8bf2d4290ea81f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"80784324-e8da-41d8-99fb-e2018faa0537\",\\n                        \"name\": \"Journal of Clinical Endocrinology and Metabolism\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 115}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b8ab8ee6566d7787f826ff1f7f294d01'}>,\n",
              "  <Document: {'content': '               \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"The Journal of Clinical Endocrinology and Metabolism\",\\n                            \"J Clin Endocrinol Metab\"\\n                        ],\\n                        \"issn\": \"0021-972X\",\\n                        \"url\": \"http://jcem.endojournals.org/\"\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 116}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ed30d11f11667908621c2d3d3709fcd5'}>,\n",
              "  <Document: {'content': '},\\n                    \"title\": \"Improvements in Sperm Motility Following Low- or High-Intensity Dietary Interventions in Men With Obesity\",\\n                    \"abstract\": \"Abstract Introduction Obesity increases risks of male infertility, but bariatric surgery does not improve semen quality. Recent uncontrolled studies suggest that a low-energy diet (LED) improves semen quality. Further evaluation within a randomized, controlled setting is warranted. Methods Men with obesity (18-60 years) with normal sperm concentration (normal count) (n = 24) or oligozoospermia (n = 43) were randomized 1:1 to either 800 kcal/day LED for 16 weeks or control, brief dietary intervention (BDI) with 16 weeks\\\\u2019 observation. Semen parameters were compared at baseline and 16 weeks. Results Mean age of men with normal count was 39.4 \\\\u00b1 6.4 in BDI and 40.2 \\\\u00b1 9.6 years in the LED group. Mean age of men with oligozoospermia was 39.5 \\\\u00b1 7.5 in BDI and 37.7 \\\\u00b1 6.6 years in the LED group. LED caused more weight loss than BDI in men with normal count (14.4 vs', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 117}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5fada6e096cf56cd0dde58367be3db1f'}>,\n",
              "  <Document: {'content': '6.3 kg; P < .001) and men with oligozoospermia (17.6 vs 1.8 kg; P < .001). Compared with baseline, in men with normal count total motility (TM) increased 48 \\\\u00b1 17% to 60 \\\\u00b1 10% (P < .05) after LED, and 52 \\\\u00b1 8% to 61 \\\\u00b1 6% (P < .0001) after BDI; progressive motility (PM) increased 41 \\\\u00b1 16% to 53 \\\\u00b1 10% (P < .05) after LED, and 45 \\\\u00b1 8% to 54 \\\\u00b1 65% (P < .001) after BDI. In men with oligozoospermia compared with baseline, TM increased 35% [26] to 52% [16] (P < .05) after LED, and 43% [28] to 50% [23] (P = .0587) after BDI; PM increased 29% [23] to 46% [18] (P < .05) after LED, and 33% [25] to 44% [25] (P < .05) after BDI. No differences in postintervention TM or PM were observed between LED and BDI groups in men with normal count or oligozoospermia. Conclusion LED or BDI may be sufficient to improve sperm motility in men with obesity. The effects of paternal dietary intervention on fertility outcomes requires investigation.\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 118}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1bbc7f694677904e8f31bf74e75efade'}>,\n",
              "  <Document: {'content': ' \"openAccessPdf\": {\\n                        \"url\": \"https://academic.oup.com/jcem/advance-article-pdf/doi/10.1210/clinem/dgad523/51332083/dgad523.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"LED or BDI may be sufficient to improve sperm motility in men with obesity and the effects of paternal dietary intervention on fertility outcomes requires investigation.\"\\n                    }\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 119}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1521aae4762a43f135a0a5444ac56270'}>,\n",
              "  <Document: {'content': '    },\\n                {\\n                    \"paperId\": \"bed122fd85ec76d54276ba5d880c9b1a229e8a22\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"4c858f5b-d9cb-4a2a-bc42-7ef869b103d8\",\\n                        \"name\": \"Antibiotics\",\\n                        \"type\": \"journal\",\\n                        \"issn\": \"2079-6382\",\\n                        \"url\": \"http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-271133\",\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 120}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1649e92e651934c0a918115b37e726ce'}>,\n",
              "  <Document: {'content': '           \"alternate_urls\": [\\n                            \"https://www.mdpi.com/journal/antibiotics\",\\n                            \"http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-271133\"\\n                        ]\\n                    },\\n                    \"title\": \"Rapid Minimum Inhibitory Concentration (MIC) Analysis Using Lyophilized Reagent Beads in a Novel Multiphase, Single-Vessel Assay\",\\n                    \"abstract\": \"Antimicrobial resistance (AMR) is a global threat fueled by incorrect (and overuse) of antibiotic drugs, giving rise to the evolution of multi- and extreme drug-resistant bacterial strains. The longer time to', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 121}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '821bcb66271d8b5ae41ff66d51de2bfd'}>,\n",
              "  <Document: {'content': 'antibiotic administration (TTA) associated with the gold standard bacterial culture method has been responsible for the empirical usage of antibiotics and is a key factor in the rise of AMR. While polymerase chain reaction (PCR) and other nucleic acid amplification methods are rapidly replacing traditional culture methods, their scope has been restricted mainly to detect genotypic determinants of resistance and provide little to no information on phenotypic susceptibility to antibiotics. The work presented here aims to provide phenotypic antimicrobial susceptibility testing (AST) information by pairing short growth periods (~3\\\\u20134 h) with downstream PCR assays to ultimately predict minimum inhibitory concentration (MIC) values of antibiotic treatment. To further simplify the dual workflows of the AST and PCR assays, these reactions are carried out in a single-vessel format (PCR tube) using novel lyophilized reagent beads (LRBs), which store dried PCR reagents along with primers and enzymes, and antibiotic drugs separately. The two reactions are separated in space and time using a melting paraffin wax seal, thus eliminating the need to transfer reagents across different consumables and minimizing user interactions. Finally, these two-step single-vessel reactions are multiplexed by using a microfluidic manifold that allows simultaneous testing of an unknown bacterial sample against different', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 122}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b9bdf306668b7a343b25072a5be2497'}>,\n",
              "  <Document: {'content': 'antibiotics at varying concentrations. The LRBs used in the microfluidic system showed no interference with the bacterial growth and PCR assays and provided an innovative platform for rapid point-of-care diagnostics (POC-Dx).\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.mdpi.com/2079-6382/12/11/1641/pdf?version=1700528145\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work aims to provide phenotypic antimicrobial susceptibility', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 123}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a608880f084033809d89d8897a93a80a'}>,\n",
              "  <Document: {'content': 'testing (AST) information by pairing short growth periods (~3\\\\u20134 h) with downstream PCR assays to ultimately predict minimum inhibitory concentration (MIC) values of antibiotic treatment.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"d798154521ebe0ab5b6d4449f3b8cbd188a2dc21\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"3ab927ca-fc94-4c5a-86e8-17868def352a\",\\n                        \"name\": \"HemaSphere\",\\n                        \"issn\": \"2572-9241\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 124}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'abe15e7913b8c2287bbe969eee954f8c'}>,\n",
              "  <Document: {'content': '                 \"url\": \"http://www.hemaspherejournal.com/\",\\n                        \"alternate_urls\": [\\n                            \"https://journals.lww.com/hemasphere/pages/currenttoc.aspx\"\\n                        ]\\n                    },\\n                    \"title\": \"Radiation and Dose-densification of R-CHOP in Aggressive B-cell Lymphoma With Intermediate Prognosis: The UNFOLDER Study\",\\n                    \"abstract\": \"UNFOLDER (Unfavorable Young Low-Risk Densification of R-Chemo Regimens) is an international phase-3 trial in patients 18\\\\u201360 years with aggressive B-cell lymphoma and intermediate prognosis defined by age-adjusted International Prognostic', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 125}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '50f2bef7639f0ecf782266891e7a40ba'}>,\n",
              "  <Document: {'content': 'Index (aaIPI) of 0 and bulky disease (\\\\u22657.5\\\\u2009cm) or aaIPI of 1. In a 2\\\\u2009\\\\u00d7\\\\u20092 factorial design patients were randomized to 6\\\\u00d7 R-CHOP-14 or 6\\\\u00d7 R-CHOP-21 (rituximab, cyclophosphamide, doxorubicin, vincristine, and prediso[lo]ne) and to consolidation radiotherapy to extralymphatic and bulky disease or observation. Response was assessed according to the standardized response criteria published in 1999, not including F-18 fluordesoxyglucose positron emission tomography/computed tomography (FDG-PET). Primary endpoint was event-free survival (EFS). A total of 695 of 700 patients were eligible for the intention-to-treat analysis. Totally 467 patients qualified for radiotherapy of whom 305 patients were randomized to receive radiotherapy (R-CHOP-21: 155; R-CHOP-14: 150) and 162 to observation (R-CHOP-21: 81, R-CHOP-14: 81). Two hundred twenty-eight patients not qualifying for radiotherapy were randomized for R-CHOP-14 versus R-CHOP-21. After a median observation of 66 months 3-year EFS was superior in the radiotherapy-arm versus observation-arm (84% versus 68%; P = 0.0012), due to a lower rate of partial responses (PR) (2% versus 11%). PR often triggered additional treatment, mostly radiotherapy. No significant difference was observed in progression-free survival (PFS) (89% versus 81%; P = 0.22) and overall survival (OS) (93% versus 93%; P = 0.51). Comparing R-CHOP-14 and R-CHOP-21 EFS, PFS and OS were not', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 126}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9af3cb1803a95b8a2f0ded113220c62'}>,\n",
              "  <Document: {'content': 'different. Patients randomized to radiotherapy had a superior EFS, largely due to a lower PR rate requiring less additional treatment (NCT00278408, EUDRACT 2005-005218-19).\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Patients randomized to radiotherapy had a superior EFS, largely due to a lower PR rate requiring less additional treatment (NCT00278408, EUDRACT 2005-005218-19), while patients eligible for radiotherapy were eligible for the intention-to-treat analysis.\"\\n                    }\\n                },\\n                {\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 127}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '80f5f4f1adc2d98c4ab1f44a28fc137a'}>,\n",
              "  <Document: {'content': '                  \"paperId\": \"a4ad2f13b12461fcbd10e9190726a8e44f2d147a\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Genetic diversity management of sculpin (Cottus spp.) and brown trout (Salmo trutta) in the Palatinate Forest-North Vosges Biosphere Reserve\",\\n                    \"abstract\": \"Protected areas can make an important contribution to the conservation of genetic diversity in the current biodiversity crisis. We have examined two representative freshwater fish taxa, Cottus spp. and Salmo trutta, in 15 midmountain headwaters of the Franco-German Palatinate Forest-North Vosges Biosphere Reserve in Central Europe to facilitate freshwater genetic diversity protection. Population genetic analyses of microsatellites and mtDNA showed lower genetic diversity, but distinctly differentiated genetic structure in Cottus spp., and higher diversity, but less differentiated structure in Salmo trutta. Phylogenetic analyses of mtDNA designated most sculpin to Cottus gobio, but also identified the first known population of Cottus', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 128}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '76e3b256a98245f1aacf0de48cc8e62e'}>,\n",
              "  <Document: {'content': 'rhenanus in the region. In addition to species-specific recommendations, we derived stream-specific guidance in an attempt to make optimal use of the combined genetic information on both taxa for habitat-oriented management prioritization and improved conservation of freshwater genetic diversity.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Examining two representative freshwater fish taxa in 15 midmountain headwaters of the Franco-German Palatinate Forest-North Vosges Biosphere Reserve in Central Europe to facilitate freshwater genetic diversity protection derived stream-specific guidance in an attempt to make optimal use of the combined genetic information on both taxa for habitat-oriented management prioritization and improved conservation of freshwater genetic Diversity.\"\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 129}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '862ec3289067355aa9f3335807483132'}>,\n",
              "  <Document: {'content': '     }\\n                },\\n                {\\n                    \"paperId\": \"353f4c8a55becdbd12619cedc5df19693d6be3d5\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"b0d34fc8-4511-4e50-9ae9-a811116075c1\",\\n                        \"name\": \"Monthly notices of the Royal Astronomical Society\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 130}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '82595b060651e8ae6530062756e33502'}>,\n",
              "  <Document: {'content': '             \"Mon not R Astron Soc\",\\n                            \"Monthly Notices of the Royal Astronomical Society\",\\n                            \"Mon Not R Astron Soc\"\\n                        ],\\n                        \"issn\": \"0035-8711\",\\n                        \"url\": \"http://mnras.oxfordjournals.org/\"\\n                    },\\n                    \"title\": \"From dark', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 131}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74374018fb5840ceb75981ac7881114c'}>,\n",
              "  <Document: {'content': 'matter halos to pre-stellar cores: High resolution follow-up of cosmological Lyman-Werner simulations\",\\n                    \"abstract\": \"\\\\n Molecular hydrogen allows cooling in primordial gas, facilitating its collapse into Population III stars within primordial halos. Lyman-Werner (LW) radiation from these stars can escape the halo and delay further star formation by destroying H2 in other halos. As cosmological simulations show that increasing the background LW field strength increases the average halo mass required for star formation, we perform follow-up simulations of selected halos to investigate the knock-on effects this has on the Population III IMF. We follow 5 halos for each of the J21 = 0, 0.01 and 0.1 LW field strengths, resolving the pre-stellar core density of 10\\\\u22126 g cm\\\\u22123 (1018 cm\\\\u22123) before inserting sink particles and following the fragmentation behaviour for hundreds of years further. We find that the mass accreted onto sinks by the end of the simulations is proportional to the mass within the \\\\u223c10\\\\u22122 pc molecular core, which is not correlated to the initial mass of the halo. As such, the IMFs for masses above the brown dwarf limit show', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 132}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f162e036f66decaafe3532aa955bdbb4'}>,\n",
              "  <Document: {'content': 'little dependence on the LW strength, although they do show variance in the number of low-mass clumps formed. As the range of background LW field strengths tested here covers the most likely values from literature, we conclude that the IMF for so-called Pop III.2 stars is not significantly different from the initial population of Pop III.1 stars. The primordial IMF therefore likely remains unchanged until the formation of the next generation of Population II stars.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://orca.cardiff.ac.uk/id/eprint/156564/1/stad188.pdf\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": null\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 133}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '55fa05f0c5a7164d7b1a93985ced136'}>,\n",
              "  <Document: {'content': '  },\\n                {\\n                    \"paperId\": \"9e1e369d04c82f6e4636606a630f311b62e88169\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Wildfire plume ageing in the Photochemical Large Aerosol Chamber (PHOTO-LAC).\",\\n                    \"abstract\": \"Plumes from wildfires are transported over large distances from remote to populated areas and threaten sensitive ecosystems. Dense wildfire plumes are processed by atmospheric oxidants and complex multiphase chemistry, differing from processes at typical ambient concentrations. For studying dense biomass burning plume chemistry in the laboratory, we establish a Photochemical Large Aerosol Chamber (PHOTO-LAC) being the world\\'s largest aerosol chamber with a volume of 1800 m3 and provide its figures of merit. While the photolysis rate of NO2 (jNO2) is comparable to that of other chambers, the PHOTO-LAC and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 134}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f2e832be720c87d66b915aea17eb3a52'}>,\n",
              "  <Document: {'content': 'its associated low surface-to-volume ratio lead to exceptionally low losses of particles to the walls. Photochemical ageing of toluene under high-NOx conditions induces substantial formation of secondary organic aerosols (SOAs) and brown carbon (BrC). Several individual nitrophenolic compounds could be detected by high resolution mass spectrometry, demonstrating similar photochemistry to other environmental chambers. Biomass burning aerosols are generated from pine wood and debris under flaming and smouldering combustion conditions and subsequently aged under photochemical and dark ageing conditions, thus resembling day- and night-time atmospheric chemistry. In the unprecedented long ageing with alternating photochemical and dark ageing conditions, the temporal evolution of particulate matter and its chemical composition is shown by ultra-high resolution mass spectrometry. Due to the spacious cavity, the PHOTO-LAC may be used for applications requiring large amounts of particulate matter, such as comprehensive chemical aerosol characterisation or cell exposures under submersed conditions.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 135}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a8cafb950f7e0db11b60170f4f8ef8db'}>,\n",
              "  <Document: {'content': ' },\\n                {\\n                    \"paperId\": \"663baa13db816bac8611f750dfa00d5ec69c0749\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"d4b2b018-ceea-4f34-8450-437a778fd34e\",\\n                        \"name\": \"Lebensmittel Zeitung\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Lebensm Ztg\"\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 136}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd71c151a2bd552975f5aa7c131132d73'}>,\n",
              "  <Document: {'content': '             ],\\n                        \"issn\": \"0947-7527\"\\n                    },\\n                    \"title\": \"Jack Daniels gewinnt Marktanteile\",\\n                    \"abstract\": \"Markenprodukte, Convenience-Artikel und h\\\\u00f6here Qualit\\\\u00e4ten: An diesen Kriterien will Brown-Forman seine Strategie auch in Deutschland ausrichten.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 137}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '601ccc74f320b5c94a2fc6568807634f'}>,\n",
              "  <Document: {'content': '            \"paperId\": \"02a53c74ce51d0404a961581619e81fdefca80e8\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"16c9f9d4-bee1-435d-8c85-22a3deba109d\",\\n                        \"name\": \"Physical Review Letters\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Phys Rev Lett\"\\n                        ],\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 138}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '284dbfa59cb49da4d0dde7889a0020c1'}>,\n",
              "  <Document: {'content': '              \"issn\": \"0031-9007\",\\n                        \"url\": \"https://journals.aps.org/prl/\",\\n                        \"alternate_urls\": [\\n                            \"http://journals.aps.org/prl/\",\\n                            \"http://prl.aps.org/\"\\n                        ]\\n                    },\\n                    \"title\": \"Imaging via Correlation of X-Ray Fluorescence Photons.\",\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 139}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd99b6aaed1a2c70abe6999e9f7fa6441'}>,\n",
              "  <Document: {'content': '            \"abstract\": \"We demonstrate that x-ray fluorescence emission, which cannot maintain a stationary interference pattern, can be used to obtain images of structures by recording photon-photon correlations in the manner of the stellar intensity interferometry of Hanbury Brown and Twiss. This is achieved utilizing femtosecond-duration pulses of a hard x-ray free-electron laser to generate the emission in exposures comparable to the coherence time of the fluorescence. Iterative phasing of the photon correlation map generated a model-free real-space image of the structure of the emitters. Since fluorescence can dominate coherent scattering, this may enable imaging uncrystallised macromolecules.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://bib-pubdb1.desy.de/record/485730/files/Main%20document.pdf\",\\n                        \"status\": \"GREEN\"\\n                    },\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 140}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e7aaf9fb0282d95e176c5e3b89eb45b6'}>,\n",
              "  <Document: {'content': '                 \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"32e584b64a5838587c900704ba9955484741ada1\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"baea33cc-0a71-4b2a-9f01-5071f7b1abc9\",\\n                        \"name\": \"Frontiers in Endocrinology\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 141}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '614b974636d7f004f3402b1ca2b87ca5'}>,\n",
              "  <Document: {'content': '                      \"Front Endocrinol\"\\n                        ],\\n                        \"issn\": \"1664-2392\",\\n                        \"url\": \"http://www.frontiersin.org/Endocrinology\",\\n                        \"alternate_urls\": [\\n                            \"https://www.frontiersin.org/journals/endocrinology\"\\n                        ]\\n                    },\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 142}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9dc22863f4d4cca9e8a3eb307b07c87'}>,\n",
              "  <Document: {'content': '              \"title\": \"An insight into brown/beige adipose tissue whitening, a metabolic complication of obesity with the multifactorial origin\",\\n                    \"abstract\": \"Brown adipose tissue (BAT), a thermoregulatory organ known to promote energy expenditure, has been extensively studied as a potential avenue to combat obesity. Although BAT is the opposite of white adipose tissue (WAT) which is responsible for energy storage, BAT shares thermogenic capacity with beige adipose tissue that emerges from WAT depots. This is unsurprising as both BAT and beige adipose tissue display a huge difference from WAT in terms of their secretory profile and physiological role. In obesity, the content of BAT and beige adipose tissue declines as these tissues acquire the WAT characteristics via the process called \\\\u201cwhitening\\\\u201d. This process has been rarely explored for its implication in obesity, whether it contributes to or exacerbates obesity. Emerging research has demonstrated that BAT/beige adipose tissue whitening is a sophisticated metabolic complication of obesity that is linked to multiple factors. The current review provides clarification on the influence of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 143}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22bafeb8992d0f0a947e7c129c40479b'}>,\n",
              "  <Document: {'content': 'various factors such as diet, age, genetics, thermoneutrality, and chemical exposure on BAT/beige adipose tissue whitening. Moreover, the defects and mechanisms that underpin the whitening are described. Notably, the BAT/beige adipose tissue whitening can be marked by the accumulation of large unilocular lipid droplets, mitochondrial degeneration, and collapsed thermogenic capacity, by the virtue of mitochondrial dysfunction, devascularization, autophagy, and inflammation.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.frontiersin.org/articles/10.3389/fendo.2023.1114767/pdf\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 144}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c1d0d088ffadfe568df9b4932a16040e'}>,\n",
              "  <Document: {'content': '                    \"text\": null\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"b171c9927b63da1c0d7b190a650a726164a0d5e0\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"8506a01a-40b8-4e6f-bbb8-ce2492139c15\",\\n                        \"name\": \"International Journal of Molecular Sciences\",\\n                        \"type\": \"journal\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 145}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f9aba25e7a7021a65e69cc4fd6fe2415'}>,\n",
              "  <Document: {'content': '                  \"alternate_names\": [\\n                            \"Int J Mol Sci\"\\n                        ],\\n                        \"issn\": \"1422-0067\",\\n                        \"url\": \"http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-157693\",\\n                        \"alternate_urls\": [\\n                            \"https://www.mdpi.com/journal/ijms\",\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 146}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff15d3da0007060422bcf49bd44a84d'}>,\n",
              "  <Document: {'content': '     \"http://www.mdpi.com/journal/ijms/\",\\n                            \"http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-157693\"\\n                        ]\\n                    },\\n                    \"title\": \"Brown Adipose Tissue: A New Potential Target for Glucagon-like Peptide 1 Receptor Agonists in the Treatment of Obesity\",\\n                    \"abstract\": \"Adipose tissue can be divided into white adipose tissue (WAT), brown adipose tissue (BAT), and beige adipose tissue, according to the differences in morphology. WAT acts as a buffer for increased energy intake and decreased energy expenditure during the development of obesity, resulting in visceral and ectopic WAT accumulation. These WAT depots are strongly associated with chronic systemic inflammation, insulin resistance, and cardiometabolic risk', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 147}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58fafd8698354f95a3f3b20a63e521ce'}>,\n",
              "  <Document: {'content': 'related to obesity. They represent a primary weight loss target in anti-obesity management. Second-generation anti-obesity medications glucagon-like peptide-1 receptor agonists (GLP-1RAs) cause weight loss and improve body composition by reducing visceral and ectopic fat depots of WAT, resulting in improved cardiometabolic health. Recently, the understanding of the physiological significance of BAT beyond its primary function in generating heat through non-shivering thermogenesis has been expanded. This has raised scientific and pharmaceutical interest in the manipulation of BAT to further enhance weight reduction and body weight maintenance. This narrative review focuses on the potential impact of GLP-1 receptor agonism on BAT, particularly in human clinical studies. It provides an overview of the role of BAT in weight management and highlights the need for further research to elucidate the mechanisms by which GLP-1RAs affect energy metabolism and weight loss. Despite encouraging preclinical data, limited clinical evidence supports the notion that GLP-1RAs contribute to BAT activation.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.mdpi.com/1422-0067/24/10/8592/pdf?version=1683790200\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 148}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '941625c11e3a67ff334d4f674285d65f'}>,\n",
              "  <Document: {'content': '                     \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"An overview of the role of BAT in weight management is provided and the need for further research to elucidate the mechanisms by which GLP-1RAs affect energy metabolism and weight loss is highlighted.\"\\n                    }\\n                },\\n                {\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 149}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '289912a8eaef131f24b120b2388fc98a'}>,\n",
              "  <Document: {'content': '                 \"paperId\": \"03437ecb11cd98eac415a1ded4067ae2c266a39c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"9040859b-7b9b-476d-a667-1a1f9b7159b0\",\\n                        \"name\": \"Archives of Mining Sciences\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Arch Min Sci\"\\n                        ],\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 150}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3bb72af14bc923bd31a42613e40e3ea'}>,\n",
              "  <Document: {'content': '                    \"issn\": \"0860-7001\",\\n                        \"url\": \"http://www.degruyter.com/view/j/amsc\"\\n                    },\\n                    \"title\": \"Convergence monitoring as a basis for numerical analysis of changes of rock-mass quality and Hoek-Brown failure criterion parameters due to longwall excavation\",\\n                    \"abstract\": \"In the longwall exploitation system, the main gates are subject of the most intensive movements of the rock mass, where the proximity of the excavation front is a key factor. The paper presents the results of a research on the constants mb and s of Hoek-Brown failure criterion for the rocks surrounding the gallery: shale, sandy shale, coal and medium-grained sandstone, in relation to the distance to longwall face. The research', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 151}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c25a12a38591255c65935e3e74e24d9'}>,\n",
              "  <Document: {'content': 'comprised numerical modeling based on convergence monitoring records. The convergence measurements were carried out on three stations in a selected maingate in a coal mine from Upper Silesia Coal Basin near Jastrz \\\\u0119 bie-Zdr\\\\u00f3j, concurrently with changing distance to the longwall face. The measu- red were the width, the height and the heave of the floor of the gate. The measurements showed that the convergence at the longwall-maingate crossing was 1.5-3 times greater than in the locations much further from the longwall face. It was demonstrated that this effect was due to continuously changing properties of the rock-mass surrounding the gallery that can be expressed as decreasing empirical parameters mb i s of Hoek-Brown\\\\u2019s criterion. These parameters are decreasing exponentially together with the distance to the longwall face The consistency between the theoretical and factual curve varies between 70% to 98%. The change of each of the parameters can be described by general equation P = a \\\\u00b7exp(\\\\u2013 b \\\\u00b7 d ), where a , b are constants, and d is the distance to the excavation face. The authors highlight that during the me- asurements period the horizontal stress was 1.45 to 1.61 times greater than the concurrent vertical stress.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 152}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '163653da2f8c2c39863025bdfd002e0b'}>,\n",
              "  <Document: {'content': 'The so high horizontal stress causes heave of unsupported gallery floor which is commonly observed in the mines in Silesia.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://journals.pan.pl/Content/110187/PDF/Archiwum-64-1-07-Malkowski.pdf\",\\n                        \"status\": \"BRONZE\"\\n                    },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"17b882375162f490daa18e75d2fbcde8be6540c7\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 153}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b496d96c5694665c7da8bfe33c6fc64f'}>,\n",
              "  <Document: {'content': '    \"publicationVenue\": {\\n                        \"id\": \"a965ebdf-2f98-45e6-a053-3c5735e3ecc9\",\\n                        \"name\": \"Journal of Climate Finance\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"J Clim Finance\"\\n                        ],\\n                        \"issn\": \"2949-7280\"\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 154}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '769256bc3bd5af96e94e7850ce99af38'}>,\n",
              "  <Document: {'content': '       },\\n                    \"title\": \"Where is the Carbon Premium? Global Performance of Green and Brown Stocks\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.econstor.eu/bitstream/10419/271890/1/cesifo1_wp10246.pdf\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": null\\n                },\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 155}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '746408f7e30397d06cfe6229e1b7431d'}>,\n",
              "  <Document: {'content': '    {\\n                    \"paperId\": \"14e5b36f2325b7d731ff24a9b3ea6d388b6b9cf1\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"45c77f4e-d4c0-4df3-99dd-119b0d2adb78\",\\n                        \"name\": \"Cell Research\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Cell Res\"\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 156}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '26c99c15d1a0cdb48a736d6e8e2e4437'}>,\n",
              "  <Document: {'content': '],\\n                        \"issn\": \"1001-0602\",\\n                        \"url\": \"http://www.cell-research.com/\",\\n                        \"alternate_urls\": [\\n                            \"http://www.cell-research.com/index.asp\",\\n                            \"http://www.nature.com/cr/archive/index.html\"\\n                        ]\\n                    },\\n                    \"title\": \"RALF signaling pathway activates', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 157}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '899462e1f81a0bc6bdb149c3699c5ade'}>,\n",
              "  <Document: {'content': 'MLO calcium channels to maintain pollen tube integrity\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.nature.com/articles/s41422-022-00754-3.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is concluded that RALF peptides derived from pollen tube', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 158}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c4620547c0062cee09949daf201570cd'}>,\n",
              "  <Document: {'content': 'bind to their receptors to establish pollen tube Ca^2+ gradient through activation of the MLO channels, providing a mechanistic link between the RalF signaling pathway and Ca^1+ signaling in controlling pollen tube integrity and growth.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"e0b94200737424d7626b2bf9ddece70905dbac3c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"8506a01a-40b8-4e6f-bbb8-ce2492139c15\",\\n                        \"name\": \"International Journal of Molecular Sciences\",\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 159}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3694cd14c2293babbb099fe5c9da114e'}>,\n",
              "  <Document: {'content': '      \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Int J Mol Sci\"\\n                        ],\\n                        \"issn\": \"1422-0067\",\\n                        \"url\": \"http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-157693\",\\n                        \"alternate_urls\": [\\n                            \"https://www.mdpi.com/journal/ijms\",\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 160}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ed2ab0341fb57235d006fb98f2e77b01'}>,\n",
              "  <Document: {'content': '                  \"http://www.mdpi.com/journal/ijms/\",\\n                            \"http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-157693\"\\n                        ]\\n                    },\\n                    \"title\": \"Anti-Obesity Effects of Metformin: A Scoping Review Evaluating the Feasibility of Brown Adipose Tissue as a Therapeutic Target\",\\n                    \"abstract\": \"Brown adipose tissue (BAT) is increasingly recognized as the major therapeutic target to promote energy expenditure and ameliorate diverse metabolic complications. There is a general interest in understanding the pleiotropic effects of metformin against metabolic complications. Major electronic databases and search engines such as PubMed/MEDLINE, Google Scholar, and the Cochrane library', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 161}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '142e0fd22238b76773f8bb722016ad6f'}>,\n",
              "  <Document: {'content': 'were used to retrieve and critically discuss evidence reporting on the impact of metformin on regulating BAT thermogenic activity to ameliorate complications linked with obesity. The summarized evidence suggests that metformin can reduce body weight, enhance insulin sensitivity, and improve glucose metabolism by promoting BAT thermogenic activity in preclinical models of obesity. Notably, this anti-diabetic agent can affect the expression of major thermogenic transcriptional factors such as uncoupling protein 1 (UCP1), nuclear respiratory factor 1 (NRF1), and peroxisome-proliferator-activated receptor gamma coactivator 1-alpha (PGC1-\\\\u03b1) to improve BAT mitochondrial function and promote energy expenditure. Interestingly, vital molecular markers involved in glucose metabolism and energy regulation such as AMP-activated protein kinase (AMPK) and fibroblast growth factor 21 (FGF21) are similarly upregulated by metformin treatment in preclinical models of obesity. The current review also discusses the clinical relevance of BAT and thermogenesis as therapeutic targets. This review explored critical components including effective dosage and appropriate intervention period, consistent with the beneficial effects of metformin against obesity-associated complications.\",\\n                    \"openAccessPdf\": {\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 162}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ab0d15e98654b9ba518e877b5a86a1b0'}>,\n",
              "  <Document: {'content': '       \"url\": \"https://www.mdpi.com/1422-0067/24/3/2227/pdf?version=1674464313\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The summarized evidence suggests that metformin can reduce body weight, enhance insulin sensitivity, and improve glucose metabolism by promoting BAT thermogenic activity in preclinical models of obesity.\"\\n                    }\\n                }\\n            ]\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 163}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bf3fa22e5c983fd3ded60327d8ca1f16'}>,\n",
              "  <Document: {'content': '      }\\n    ],\\n    \"Jamie Callan\": [\\n        {\\n            \"total\": 237,\\n            \"offset\": 0,\\n            \"next\": 20,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"88884b8806262a4095036041e3567d450dba39f7\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 164}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e7865579825a695d004a9600a84de79'}>,\n",
              "  <Document: {'content': '               \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 165}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22b27271fbabf00cad99bd9495f3aba3'}>,\n",
              "  <Document: {'content': '               \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"Active Retrieval Augmented Generation\",\\n                    \"abstract\": \"Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 166}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '100b4f2379b574a147277fb24bc5452f'}>,\n",
              "  <Document: {'content': 'a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.06983\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 167}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a06bc6971ed883cfd8de00538f503a9a'}>,\n",
              "  <Document: {'content': '\"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"868e602f59e836c28534b888a6700429be75ea84\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"8ee71e17-421e-43db-ad2d-cc8af6217a0d\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 168}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '32cff8a3476bc3da82f13003406ddfdf'}>,\n",
              "  <Document: {'content': '             \"name\": \"European Conference on Information Retrieval\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ECIR\",\\n                            \"Eur Conf Inf Retr\"\\n                        ],\\n                        \"url\": \"https://en.wikipedia.org/wiki/European_Conference_on_Information_Retrieval\"\\n                    },\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 169}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46ab6ab228840f133aeb77d755f03860'}>,\n",
              "  <Document: {'content': '                \"title\": \"COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": null\\n                    }\\n                },\\n                {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 170}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5af7a612674484884a881c61ad0ff772'}>,\n",
              "  <Document: {'content': '          \"paperId\": \"197d5fbc3764ff18186275545d0764d5b1c7659b\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"eac972b5-2d1a-4c5a-98a2-c1e373f09163\",\\n                        \"name\": \"International Conference on the Theory of Information Retrieval\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Int Conf Theory Inf Retr\",\\n                            \"ICTIR\"\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 171}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f68c1588147de9c5bb5caefcca50b216'}>,\n",
              "  <Document: {'content': '                       ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=1494\"\\n                    },\\n                    \"title\": \"Conversational Search with Random Walks over Entity Graphs\",\\n                    \"abstract\": \"The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity\\'s centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 172}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c91ac827fae2dc8081a3f7f2326f45e'}>,\n",
              "  <Document: {'content': '           \"openAccessPdf\": {\\n                        \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3578337.3605125\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.\"\\n                    }\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 173}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8255a54d56278c4f274c59cdb27c4cca'}>,\n",
              "  <Document: {'content': '            },\\n                {\\n                    \"paperId\": \"1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"eac972b5-2d1a-4c5a-98a2-c1e373f09163\",\\n                        \"name\": \"International Conference on the Theory of Information Retrieval\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 174}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2e5ab505878cf6f778d380451b10986c'}>,\n",
              "  <Document: {'content': '     \"Int Conf Theory Inf Retr\",\\n                            \"ICTIR\"\\n                        ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=1494\"\\n                    },\\n                    \"title\": \"KALE: Using a K-Sparse Projector for Lexical Expansion\",\\n                    \"abstract\": \"Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 175}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '786775eae42c91a9b60bbfcf72e1f42c'}>,\n",
              "  <Document: {'content': 'small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3578337.3605131\",\\n                        \"status\": \"BRONZE\"\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 176}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '48443e11bcee4a0577f5eeaa0712823b'}>,\n",
              "  <Document: {'content': '    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"ac9ee72a5cd611e9143e385f668af662583721ee\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 177}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a7500de4186e462265f01b58602f32d'}>,\n",
              "  <Document: {'content': '         \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 178}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb1a0f2f1bace788384aa5621b5165bb'}>,\n",
              "  <Document: {'content': '       },\\n                    \"title\": \"Multi-Objective Improvement of Android Applications\",\\n                    \"abstract\": \"Non-functional properties, such as runtime or memory use, are important to mobile app users and developers, as they affect user experience. Previous work on automated improvement of non-functional properties in mobile apps failed to address the inherent trade-offs between such properties. We propose a practical approach and the first open-source tool, GIDroid (2023), for multi-objective automated improvement of Android apps. In particular, we use Genetic improvement, a search-based technique that navigates the space of software variants to find improved software. We use a simulation-based testing framework to greatly improve the speed of search. GIDroid contains three state-of-the-art multi-objective algorithms, and two new mutation operators, which cache the results of method calls. Genetic improvement relies on testing to validate patches. Previous work showed that tests in open-source Android applications are scarce. We thus wrote tests for 21 versions of 7 Android apps, creating a new benchmark for performance', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 179}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e667f40539d272455f52cfc1c2a751b6'}>,\n",
              "  <Document: {'content': 'improvements. We used GIDroid to improve versions of mobile apps where developers had previously found improvements to runtime, memory, and bandwidth use. Our technique automatically re-discovers 64% of existing improvements. We then applied our approach to current versions of software in which there were no known improvements. We were able to improve execution time by up to 35%, and memory use by up to 33% in these apps.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2308.11387\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 180}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7ffc42f1a05a26bd425db9ef5ec6d79'}>,\n",
              "  <Document: {'content': '   \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes a practical approach and the first open-source tool, GIDroid, for multi-objective automated improvement of Android apps, and uses Genetic improvement, a search-based technique that navigates the space of software variants to find improved software.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"6b7eefa15c0a461afeab4fa13cf862c5340fdc2a\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"eac972b5-2d1a-4c5a-98a2-c1e373f09163\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 181}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd6b6c5e68f37ea4f7b706a62ab4fa9e9'}>,\n",
              "  <Document: {'content': '        \"name\": \"International Conference on the Theory of Information Retrieval\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Int Conf Theory Inf Retr\",\\n                            \"ICTIR\"\\n                        ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=1494\"\\n                    },\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 182}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c932f4f4dd33d47c6971fd3edee9aa2'}>,\n",
              "  <Document: {'content': '               \"title\": \"CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\",\\n                    \"abstract\": \"Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a \\\\\"bag-of-CSFs\\\\\", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 183}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'daa689fd03b633f61688f9d72220d8ee'}>,\n",
              "  <Document: {'content': 'that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3578337.3605126\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper proposes to directly bridge', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 184}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd620d4cdf63d1141d16ae555a01331d0'}>,\n",
              "  <Document: {'content': 'the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF), reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact- match-based system.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"562e3f1ca8ec6492e85862938ac0ca5e15d37a33\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Dissipative Callan-Harvey mechanism in 2+1 D Dirac system: The fate of edge states along a domain wall\",\\n                    \"abstract\": \"The Callan-Harvey mechanism in 2+1 D Jackiw-Rebbi model is revisited. We analyzed Callan-Harvey anomaly inflow in', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 185}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '592a2e1164991d41757c849702f609'}>,\n",
              "  <Document: {'content': 'the massive Chern insulator (quantum anomalous Hall system) subject to external electric field. In addition to the conventional current flowing from the bulk to edge due to parity anomaly, we considered the dissipation of the edge charge due to interaction with external bosonic bath in 2+1 D and due to external bath of photons in 3+1 D. In the case of 2+1 D bosonic bath, we found the new stationary state, which is defined by the balance between Callan-Harvey current and the outgoing flow caused by the dissipation processes. In the case of 3+1 D photon bath, we found a critical electric field, below which this balance state can be achieved, but above which there is no such a balance. Furthermore, we estimated the photon-mediated transition rate between 2+1 D bulk and 1+1 D topological edge state of the order of one ns$^{-1}$ (nanosecond) at the room temperature.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 186}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c0d8980a755bdbd67307816ded05d0cb'}>,\n",
              "  <Document: {'content': '     },\\n                {\\n                    \"paperId\": \"43cd9cc2177a78d19e9144e5de0008804e0cdfc1\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"The role of the Callan\\\\u2013Witten anomaly density as a Chern\\\\u2013Simons term in Skyrme model\",\\n                    \"abstract\": \"We consider axially symmetric solutions of the U(1) gauged Skyrme model supplemented with a Callan\\\\u2013Witten (CW) anomaly density term. The main properties of the solutions are studied, several specific features introduced by the presence of the CW term being identified. We find that the solitons possess a nonzero angular momentum proportional to the electric charge, which in addition to the usual Coulomb part, acquires an extra (topological) contribution from the CW term. Specifically, it is shown that the slope of mass/energy M', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 187}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1f4b8965d4bfeb9ce8923e5986be684'}>,\n",
              "  <Document: {'content': 'vs. electric charge Qe and angular momentum J can be both positive and negative. Furthermore, it is shown that the gauged Skyrmion persists even when the quartic (Skyrme) kinetic term disappears.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2304.12648\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"e9827c9557eba91a292a3767031fdc7c3f2ded53\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 188}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9fd757ba5112b4bcf5b40eea1b1b698f'}>,\n",
              "  <Document: {'content': '               \"publicationVenue\": null,\\n                    \"title\": \"From Horndeski action to the Callan-Giddings-Harvey-Strominger model and beyond\",\\n                    \"abstract\": \"The knowledge of what entered black hole (BH) is completely lost as it evaporates. This contradicts the unitarity principle of quantum mechanics and is referred to as the information loss paradox. Understanding the end stages of BH evaporation is key to resolving this paradox. As a first step, we need to have exact models that can mimic 4-D BHs in General relativity in classical limit and have a systematic way to include high-energy corrections. While there are various models in the literature, there is no systematic procedure by which one can study high-energy corrections. In this work, for the first time, we obtain Callan, Giddings, Harvey, and Strominger (CGHS) -- a (1+1)-D -- model from 4-D Horndeski action -- the most general scalar-tensor theory that does not lead to Ostrogradsky ghosts. We then show', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 189}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f35387d7b9f3de7a44df01fe179baee0'}>,\n",
              "  <Document: {'content': 'that 4-D Horndeski action can systematically provide a route to include higher-derivative terms relevant at the end stages of black hole evaporation. We derive the leading order Hawking flux while discussing some intriguing characteristics of the corrected CGHS models. We compare our results with other works and discuss the implications for primordial BHs.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"6f7cdd1844dd1a7d112d195d2638980491ed79b0\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Discovering the Meaning of Figurative Language in Jamie Miller\\\\u2019s Song Lyrics\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 190}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '980e9790a3c2f6669b79ffab4459e63e'}>,\n",
              "  <Document: {'content': '                   \"abstract\": \"The purpose of this study is to identify the types of figurative language and the meaning intended. Two theories have been used in this study. The first theory is presented by Kennedy (1987) as used to identify the types of figures of speech used in Jamie miller\\'s song lyrics. A second theory is presented by Leech (1974) to learn the meaning of the figures of speech used in Jamie Miller\\'s song. The method used in data collection is the method of observation. The writers collect data of song lyrics by browsing and downloading song lyrics from the Internet, reading, listening, and understanding song lyrics, taking note and classifying data based on the types of figurative language. The writer analysed the data in a descriptive way using a qualitative method. After analyzing the data, the writer found that there are six types of figurative of language used in Jamie Miller\\'s song lyrics which consist of\\\\u00a0 2 metaphors (16,6%), 2 similes (16,6%), 2 personifications (16,6%), 1 paradox (8,3%), 2 apostrophe (16,6%), and 3 hyperbole (24,9%). All of figurative language have a connotation', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 191}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2e2501558f4eb8f3ba5bd1961766444b'}>,\n",
              "  <Document: {'content': 'that implicitly conveys hidden messages and values of life. \\\\n\\\\u00a0\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://e-journal.unmas.ac.id/index.php/elysian/article/download/4240/4551\",\\n                        \"status\": \"BRONZE\"\\n                    },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"b638aee9ffb8fd8f02384851821c456cff365af2\",\\n                    \"publicationVenue\": {\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 192}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e20c8fdf7afd4c865c3ea97dbf9740c1'}>,\n",
              "  <Document: {'content': '                   \"id\": \"48153fdc-5fc8-4565-b488-c2c1d5f563e1\",\\n                        \"name\": \"Textual Practice\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Textual Pract\"\\n                        ],\\n                        \"issn\": \"0950-236X\",\\n                        \"url\": \"http://www.tandfonline.com/rtpr\"\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 193}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '867bd4225953931ed9692f8e318735cf'}>,\n",
              "  <Document: {'content': '                   },\\n                    \"title\": \"Grasping extinction: the natural history museum as haptic space in the work of Clarke, Robinson and Jamie\",\\n                    \"abstract\": \"ABSTRACT This article examines depictions of natural history museums in a selection of poems and essays drawn from Gillian Clarke\\\\u2019s Zoology (2017), Jane Robinson\\\\u2019s Journey to the Sleeping Whale (2018) and Kathleen Jamie\\\\u2019s Sightlines (2012). These texts present the museum as a place in which we confront extinction through conflicting emotions: a space of care and protection, built on acts of violence. My analysis explores how physical prehension, real or imagined, contributes to mental prehension, and thus helps us to grasp the sixth mass extinction. I examine how metonymical relations connect museums to damaged environments, including zones, like the ocean, that are perceived as sacrifice zones in the making. I contrast Clarke\\\\u2019s sequence of poems \\\\u2018Behind Glass\\\\u2019, where ekphrasis connects the beauty of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 194}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4a71d4177369a5a3a159ca9be236a449'}>,\n",
              "  <Document: {'content': 'endangered specimens with protected landscapes, to Robinson\\\\u2019s \\\\u2018Memories of Flight at the Life Museum\\\\u2019 and Jamie\\\\u2019s \\\\u2018Voyager, Chief\\\\u2019 and \\\\u2018The Hvalsalen\\\\u2019, which emphasise the collection\\\\u2019s dependence on the depletion of lands and oceans. This leads me to outline two textual approaches to the natural history museum: the first reads the specimen as a synecdoche for endangered, yet protected environments, while the second connects the collection, metonymically and metaphorically, to anthropogenic extinction and sacrifice zones.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.tandfonline.com/doi/pdf/10.1080/0950236X.2023.2264680?needAccess=true\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": null\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 195}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac6b2fb2919af824ae4a510a4e431f41'}>,\n",
              "  <Document: {'content': ' },\\n                {\\n                    \"paperId\": \"bfd715a4a095d8d0da321f38befa7ebc0c3f4680\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"A New Solution to the Callan Rubakov Effect\",\\n                    \"abstract\": \"In this paper we study the scattering of massive fermions off of smooth, spherically symmetric monopoles in $4d$ $SU(2)$ gauge theory. We propose a complete physical picture of the monopole-fermion interaction which encompasses all angular momentum modes. We show that as an in-going fermion scatters off a monopole, it excites trapped $W$-bosons in the monopole core by a version of the Witten effect so that the monopole can accrue charge and transform into a dyon at parametrically low energies. The imparted electric charge is then protected from decay by an emergent $\\\\\\\\mathbb{Z}_N$', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 196}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'da4f15d9d9764093be04dfa7cb974722'}>,\n",
              "  <Document: {'content': 'generalized global symmetry, creating a stable dyon. At sufficiently low energies, the scattered fermion can be trapped by the dyon\\'s electrostatic potential, forming a bound state, which can decay into spherically symmetric fermion modes subject to the preserved $\\\\\\\\mathbb{Z}_N$ global symmetry. We propose that monopole-fermion scattering can be described in this way without needing to add ``new\\'\\' states to the Hilbert space, thereby eliminating a long standing confusion in the Callan Rubakov effect.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"3514cc740a6c6274cec22030c1501189760ce393\",\\n                    \"publicationVenue\": null,\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 197}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b8746e32773b768948abd83aa3c0fab2'}>,\n",
              "  <Document: {'content': '        \"title\": \"Image of Bali from Western Perspective in Jamie James\\' A Tale of Bali and Elizabeth Gibert\\'s Eat, Pray, Love\",\\n                    \"abstract\": \"Many foreign writers have expressed their fascination with Bali in their works, among them are Jamie James in Andrew and Joey: a tale of Bali and Elizabeth Gilbert in Eat, Pray, Love. The two authors wrote about Bali from their personal experience dring their stay in that island. Some images created in the works are seen and analyzed through the prespective of Postcolonialism and New Criticism. This article presents how the two authors depicts Bali as an extravagant place in Eastern country which has adopted some Western values. The negative image is delivered to oppose the idea of Bali as the perfect island; moreover, as part of Indonesia, Bali still has some weaknesses to strengthen and truly support that it is part of Indonesia. The writer admits that this article is far from perfect. Due to the limitation of thinking and time, some more aspects can be explored more profoundly in terms', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 198}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bffe8a7872d3083248565c39014de628'}>,\n",
              "  <Document: {'content': 'of the image of Bali in these two works. Hence, there are still a lot of literary works that also talk about and discuss the image of Bali. Hopefully, there will be anyone who, willingly or even the writer himself, could continue the research and find out more about the images of Bali from the Western perspective as seen in some other novels and works.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://jurnalvivid.fib.unand.ac.id/index.php/vivid/article/download/340/240\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": null\\n                },\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 199}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fa3305011384a6904588d348d2c86ef0'}>,\n",
              "  <Document: {'content': '        {\\n                    \"paperId\": \"95c047bcd9dd06d312964c8fcf377d7778c5712e\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Freak Scenes: American Indie Cinema and Indie Music Culture, Jamie Sexton (2023)\",\\n                    \"abstract\": \"Review of: Freak Scenes: American Indie Cinema and Indie Music Culture, Jamie Sexton (2023)\\\\n Edinburgh: Edinburgh University Press, 200 pp.,\\\\n ISBN 978-1-47441-406-7, h/bk, \\\\u00a385\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 200}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b79ac2706b8415ecc1328baeb3c34ad8'}>,\n",
              "  <Document: {'content': '{\\n                    \"paperId\": \"e1341e9286b594f45376c5ee9945f8b028e5db58\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"03ffb7f1-c980-4af3-887c-2fccdf03cafe\",\\n                        \"name\": \"The Soundtrack\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Soundtrack\"\\n                        ],\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 201}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d404b88f982ffeb1c08b31f29a04822'}>,\n",
              "  <Document: {'content': '                   \"issn\": \"1751-4193\",\\n                        \"url\": \"http://www.intellectbooks.co.uk/journals/view-journal,id=146/\"\\n                    },\\n                    \"title\": \"Freak Scenes: American Indie Cinema and Indie Music Cultures, Sexton Jamie (2023)\",\\n                    \"abstract\": \"Review of: Freak Scenes: American Indie Cinema and Indie Music Cultures, Sexton Jamie (2023)\\\\n University of Edinburgh Press, 208 pp.,\\\\n ISBN 978-1-47441-408-1, h/bk, \\\\u00a385.00\\\\n ISBN 978-1-47441-407-4, e-book, \\\\u00a385.00\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 202}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1974f2ecd572294517dd5e255bd8f2eb'}>,\n",
              "  <Document: {'content': '   },\\n                {\\n                    \"paperId\": \"45a0b53ec3da75b7d33ea7d005699484345a3595\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1b151f31-5986-4ade-84b7-f2b2df4e0228\",\\n                        \"name\": \"Astonjadro\",\\n                        \"alternate_names\": [\\n                            \"Astonjadro\"\\n                        ],\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 203}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c1b7f760b9076a435be3ed7fa012a26'}>,\n",
              "  <Document: {'content': '            \"issn\": \"2302-4240\",\\n                        \"url\": \"http://ejournal.uika-bogor.ac.id/index.php/ASTONJADRO/issue/archive\"\\n                    },\\n                    \"title\": \"Development of the structure of the Jamie Nurul Iman Sukaraja Mosque building based on the needs of congregation facilities\",\\n                    \"abstract\": \"Jamie Nurul Iman Mosque has an area of 208.39 m2, the mosque building currently has limited facilities, such as parking facilities for worshipers who still use the shoulder of the road and ablution facilities and special toilets for men and women must be separated, efforts to complete the facilities for the congregation then it is necessary to develop the building, due to the limited land owned, the development is carried out vertically into three floors. The research was conducted by testing the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 204}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '60677167a2a3edb0dc2ff865bc7bb63e'}>,\n",
              "  <Document: {'content': \"concrete quality (f'c) of existing columns, beams and slabs, the concrete quality values were obtained sequentially of 13.6 MPa, 11 MPa and 11.2 MPa, then modeled the existing structure using the ETABS application and the results showed that column structure, beams experiencing over strength (O/S) cannot withstand the loads acting on the building, so the existing building structure cannot be used for the development of a new three-story mosque building. The results of the structural analysis of the plan based on SNI obtained the dimensions of the new mosque building with a size of 11.25 meters x 18.8 meters, reinforced concrete structures, non-concrete roofs and enamel domes. The material specifications for the new mosque structure are concrete quality (f'c) 30 MPa, reinforcing steel quality (fy) 400 MPa and 280 MPa (BJTP). Column dimensions are 50x50 cm (first floor), column 40x40 cm (second and third floors), main beam dimensions are 30x60 cm, main beam is 30x50 cm, and child beams are 25x40 cm, floor slab thickness is 15 cm, roof slab is not 13 cm thick. The first floor is equipped with male ablution facilities with an area of 12.51 m2, female ablution facilities with an area of 9.7 m2, toilet\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 205}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bbc1ad8491c31451c54d16e7acd62d48'}>,\n",
              "  <Document: {'content': 'facilities with an area of 2.84 m2, and vehicle parking facilities with an area of 99.93 m2. The second floor is equipped with men\\'s prayer facilities with an area of 102.87 m2, book storage facilities with an area of 10.5 m2, logistics facilities with an area of 5.67 m2. The third floor is equipped with women\\'s prayer facilities with an area of 72.9 m2. The budget plan for the construction of a three-story new mosque concrete structure is Rp. 717,990,500.00.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://ejournal.uika-bogor.ac.id/index.php/ASTONJADRO/article/download/6864/4157\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": null\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 206}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '71bc9810a009679be2e194a848b2bfce'}>,\n",
              "  <Document: {'content': '       },\\n                {\\n                    \"paperId\": \"65be8027177307b77e111b7380c96ec9579b5553\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"f0e3e667-3ecd-4f75-8680-c411ee29653e\",\\n                        \"name\": \"Jurnal Dakwah Tabligh\",\\n                        \"alternate_names\": [\\n                            \"J Dakwah Tabligh\"\\n                        ],\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 207}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bbad1f1bccd650e869a900fbb373d652'}>,\n",
              "  <Document: {'content': '                    \"issn\": \"1412-7172\",\\n                        \"url\": \"http://journal.uin-alauddin.ac.id/index.php/tabligh/index\"\\n                    },\\n                    \"title\": \"STRATEGI METODE DAKWAH DALAM MEMAKMURKAN MASJID JAMIE AL- HIKMAH MELALUI PENGAJIAN RUTIN DI DESA WANAJAYA\",\\n                    \"abstract\": \"The research was conducted to find out the strategy of the da\\'wah method in prospering the Jamie al-hikmah mosque through routine recitations in Wanajaya Village, Kec. Telukjambe Barat, Karawang Regency. This study used a qualitative approach by collecting data using interviews, observation, and analysis of documentation data. The research sources were DKM, mosque administrators, and one of the speakers. The results of this study contain routine recitations including routine religious recitations every Tuesday night and routine yasin and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 208}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '13fef4ccd12d4bf50afdf9126a48469d'}>,\n",
              "  <Document: {'content': 'recitations every Friday night. The material comes from classic books including the book of tijan addaruri by Syekh Nawawi al-Bantani, Jauhar at Tauhid by Syekh Ibrahim al-Bajuri, safinatun najah by Syekh Nawawi al-Bantani, fathul muin by Syekh Zainuddin bin Abdul Aziz al- Malibari, bidayatul hidayah by Imam al-Ghazali, al-hikam by Syekh Ibn Atha\\'illah as well as references to sources from books and notes. The da\\'wah method strategy used is the lecture method, discussion method, and question and answer method\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"7b82f7dfa2837486aeda2e5ba0564b1044fdcfdd\",\\n                    \"publicationVenue\": {\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 209}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '51ecead2c1b73bb25a693e0c0411c1b0'}>,\n",
              "  <Document: {'content': '                  \"id\": \"f144d610-d756-4800-97cb-a63e315634a8\",\\n                        \"name\": \"Cadernos de Tradu\\\\u00e7\\\\u00e3o\",\\n                        \"alternate_names\": [\\n                            \"Cadernos de tradu\\\\u00e7\\\\u00e3o\",\\n                            \"Cad Tradu\\\\u00e7\\\\u00e3o\",\\n                            \"Cad tradu\\\\u00e7\\\\u00e3o\"\\n                        ],\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 210}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '536200b4657b6d56205718256ed3a42c'}>,\n",
              "  <Document: {'content': '       \"issn\": \"1414-526X\",\\n                        \"alternate_issns\": [\\n                            \"1807-9873\"\\n                        ],\\n                        \"url\": \"https://www.scielo.br/scielo.php?lng=en&pid=2175-7968&script=sci_serial\",\\n                        \"alternate_urls\": [\\n                            \"http://www.cadernos.ufsc.br/\",\\n                            \"https://periodicos.ufsc.br/index.php/traducao/index\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 211}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '57ed57d77e58e32de273f264e1134dfd'}>,\n",
              "  <Document: {'content': '                   \"https://seer.ufrgs.br/index.php/cadernosdetraducao\",\\n                            \"https://periodicos.ufsc.br/index.php/traducao\"\\n                        ]\\n                    },\\n                    \"title\": \"A critical analysis of the celebrity chef Jamie Oliver\\'s discourse of fastness and BBC Persian\\\\u2019s dubbing strategies\",\\n                    \"abstract\": \"Translation Studies scholars have recently indicated the emergence of a political ecology of translation. Viewing this new trend, the present study aimed to explore discoursal features of Jamie Oliver\\'s talk in Jamie\\\\u2019s Thirty Minute Meals cookery show and the way they were rendered in the dubbed version broadcast by BBC Persian.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 212}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dca5af1c983da4084555181bd2388fcc'}>,\n",
              "  <Document: {'content': 'To this end, the comparative model of research and the notion of tertium comparationis was used. To analyze the discoursal features of Jamie Oliver, the principles of the grounded theory were applied. The findings indicate that the predominant stylistic features of fastness in Jamie Oliver\\\\u2019s talk are repetition of title, using time phrases indicating fastness, and using words implying easiness and briskness. Analysis of the dubbed version revealed that all the identified expressions of fastness were translated literally into Persian, though they do not sound natural. In other words, in Toury\\\\u2019s (1980) term adequate or source-oriented strategies were used to dub the show into Persian. The findings can shed some light on the realization of food colonization in translation.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://periodicos.ufsc.br/index.php/traducao/article/download/90809/52682\",\\n                        \"status\": \"GOLD\"\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 213}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6dbfdb94bae3ce2295e169c34e8874d'}>,\n",
              "  <Document: {'content': '         },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"7c888cde24fcceb85fd8e3551da6291b995e399b\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"2023 Jessberger Award to Jamie Elsila\",\\n                    \"abstract\": \"I am delighted to introduce Jamie Elsila as the recipient of this year\\\\u2019s Jessberger Award. I met Jamie after she returned from service in the Peace Corps in Tanzania between college and graduate school, where she taught chemistry and math. She visited NASA Ames with Society Fellow Scott Sandford', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 214}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c3952d7a3cd4a775e991f03263abcc'}>,\n",
              "  <Document: {'content': 'where she had done undergraduate research and published her first paper (Elsila et al., 1997) (Figure 1). I was a post-doc and had the pleasure of working with her when she was a graduate student with Dick Zare where she had her first meteoritic publication 20 years ago (Plows et al., 2003) and received her PhD in 2004. As large cuts to NASA\\\\u2019s astrobiology program were being proposed in 2006, Society Nier Prize winner, Danny Glavin and I had the fortune to get funding for a new isotope ratio mass spectrometer (Figure 2) as well as a hiring point for a civil servant scientist to join our laboratory at NASA\\\\u2019s Goddard Space Flight Center for compound-specific isotope measurements needed to firmly establish the origin of amino acids detected in meteorites and in samples returned from comet Wild 2 by the Stardust mission. As a senior post-doc at that same laboratory at Ames, Jamie had modest experience with stable isotope work but a keen interest in astrobiology and extraterrestrial organic chemistry. I encouraged her to apply; she did and was hired as the top candidate. Then, as now, she can master a new topic and became expert at the challenging field', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 215}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4918f13e9480fcaf7fd2889185c1b39b'}>,\n",
              "  <Document: {'content': 'of compound-specific isotope measurements of trace organics in extraterrestrial samples. In particular, the analysis of amino acids in Stardust foil extracts proved to be extremely difficult. Interferences with the chemical derivatization method used for the analyses due to borate contamination from the aerogel, the low abundances of the amino acid and amine targets in the samples, coupled with the loss of amino acid precursors from the foils over time in curation, all needed to be overcome to make the challenging measurement. After years of instrument optimization, she was able to spot the signature of extraterrestrial C to make the first detection of an amino acid, glycine, derived from a comet (Elsila et al., 2009). It is heartening to know that the same compound was observed in the coma of 67P/ChuryumovGerasimenko by Rosetta\\\\u2019s ROSINA instrument many years later (Altwegg et al., 2016; Hadraoui et al., 2019). The mechanism of glycine formation at 67P was further investigated by the COSIMA instrument, which of course Elmar Jessberger contributed to developing (Kissel et al., 2007). Jamie is an expert in the analysis of meteoritic soluble organic compounds, having built a strong and successful research program in this area over the past decade. Her research', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 216}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'acf10df7d550d16085606ffe75ba2d34'}>,\n",
              "  <Document: {'content': 'in this area focuses on understanding the abundances, distributions, and stable isotopic composition of soluble organic compounds, and interpreting this information to answer important questions about the origin of these molecules and their potential astrobiological and astrochemical contributions to the Solar System. Her meticulous and systematic approach includes the search for bias in sample workup as well as the analysis. For example, she performed the first analysis of the H, C, and N stable isotopes of the same amino acids from the same extract from a range of CM and CR meteorites (Elsila et al., 2012). This finally eliminated the complication of different extractions, different methods, different instruments often invoked to explain conflicting results. She has generated the most extensive published studies of isotopic composition of various soluble organic compounds in carbonaceous chondrites to create a database used by the community in interpreting astrochemical reactions. She collaborates with modelers to use these new results to examine models for extraterrestrial chemical reactions, thereby FIGURE 1. Jamie Elsila, college student, studying CO ice at the NASA\\\\u2019s Ames Research Center in 1996. Credit: NASA/ Ames. (Color figure can be viewed at wileyonlinelibrary.com.) Meteoritics & Planetary Science 58, Nr 8, 1173\\\\u20131175 (2023) doi: 10.1111/maps.14039\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 217}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2e991b7fd3ec0ba1bcf2787d606c883d'}>,\n",
              "  <Document: {'content': '                   \"openAccessPdf\": {\\n                        \"url\": \"https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/maps.14039\",\\n                        \"status\": \"BRONZE\"\\n                    },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"dea350d1fa3cb5ee2b04f7541f5a0a7937ea1e30\",\\n                    \"publicationVenue\": null,\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 218}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '860957beac46f178f7adfabbcb947fbf'}>,\n",
              "  <Document: {'content': '     \"title\": \"Social Action Art Therapy in a Time of Crisis, Jamie Bird (2022)\",\\n                    \"abstract\": \"Review of: Social Action Art Therapy in a Time of Crisis, Jamie Bird (2022)\\\\n London: Routledge, 268 pp.,\\\\n ISBN 978-0-36769-621-4, p/bk, \\\\u00a326.99\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                }\\n            ]\\n        }\\n    ],\\n    \"Justine Cassell\": [\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 219}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9dbc03959bbeb78543370912a38be05'}>,\n",
              "  <Document: {'content': '         \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"a82f56482b9e63714ea0d1948ac3aa6edb092001\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"c33b01b0-31b4-470e-a9f9-8432e02c3cb9\",\\n                        \"name\": \"Cognitive Sciences\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 220}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f84d1cf94da825d2fae340bd551693e'}>,\n",
              "  <Document: {'content': '                       \"Cognitive Science\",\\n                            \"Cogn Sci\"\\n                        ],\\n                        \"issn\": \"1935-8059\",\\n                        \"alternate_issns\": [\\n                            \"0364-0213\"\\n                        ],\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 221}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b43ad3bcf1a55598fbbd2976958b24f1'}>,\n",
              "  <Document: {'content': '   \"url\": \"http://www.informaworld.com/openurl?genre=journal&issn=1551-6709\",\\n                        \"alternate_urls\": [\\n                            \"http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1551-6709\",\\n                            \"http://www3.interscience.wiley.com/cgi-bin/jtoc?ID=121670282\",\\n                            \"https://onlinelibrary.wiley.com/journal/15516709\",\\n                            \"http://www.sciencedirect.com/science/journal/03640213\",\\n                            \"http://www.leaonline.com/loi/cog\"\\n                        ]\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 222}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62e6cea33a8a24cf52ee92c6f0f91507'}>,\n",
              "  <Document: {'content': '             },\\n                    \"title\": \"Beyond Single-Mindedness: A Figure-Ground Reversal for the Cognitive Sciences\",\\n                    \"abstract\": \"A fundamental fact about human minds is that they are never truly alone: all minds are steeped in situated interaction. That social interaction matters is recognized by any experimentalist who seeks to exclude its influence by studying individuals in isolation. On this view, interaction complicates cognition. Here, we explore the more radical stance that interaction co-constitutes cognition: that we benefit from looking beyond single minds toward cognition as a process involving interacting minds. All around the cognitive sciences, there are approaches that put interaction center stage. Their diverse and pluralistic origins may obscure the fact that collectively, they harbor insights and methods that can respecify foundational assumptions and fuel novel interdisciplinary work. What might the cognitive sciences gain from stronger interactional foundations? This represents, we believe, one of the key questions for the future. Writing as a', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 223}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cee6eae0ca6182b2cfaf37076bb0cd20'}>,\n",
              "  <Document: {'content': 'transdisciplinary collective assembled from across the classic cognitive science hexagon and beyond, we highlight the opportunity for a figure-ground reversal that puts interaction at the heart of cognition. The interactive stance is a way of seeing that deserves to be a key part of the conceptual toolkit of cognitive scientists.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 224}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7185379cf4ad729de49778b8856f76c2'}>,\n",
              "  <Document: {'content': '          \"paperId\": \"b3efaa75beada858414a5ba2346dec317203633c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 225}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9661146d1710237a31bd024661b8dfeb'}>,\n",
              "  <Document: {'content': 'of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"\\\\\"You might think about slightly revising the title\\\\u201d: Identifying Hedges in Peer-tutoring Interactions\",\\n                    \"abstract\": \"Hedges have an important role in the management of rapport. In peer-tutoring, they', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 226}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2daf57c6e522aedd71f27800ba6aea9b'}>,\n",
              "  <Document: {'content': 'are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback.Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2022.acl-long.153.pdf\",\\n                        \"status\": \"HYBRID\"\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 227}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd071abb2bef2052d3780b8fb731da77c'}>,\n",
              "  <Document: {'content': '       },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A model explainability tool is employed to explore the features that characterize hedges in peer-tutoring conversations, and some novel features, and the benefits of a such a hybrid model approach are identified.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 2,\\n            \"offset\": 0,\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 228}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b2be822e3f5f256d224378d9caf10c6f'}>,\n",
              "  <Document: {'content': '           \"data\": [\\n                {\\n                    \"paperId\": \"74fedee9d809ec766a2089a89435fa7dd1346693\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 229}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd94f0f710fc7955cd809b2561a9b3d9'}>,\n",
              "  <Document: {'content': '     \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"How About Kind of Generating Hedges using End-to-End', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 230}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbcf72b4134ec2b03babef34680fbe99'}>,\n",
              "  <Document: {'content': 'Neural Models?\",\\n                    \"abstract\": \"Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, \\\\u201cface threat\\\\u201d) to one\\\\u2019s listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this work, we develop a model of hedge generation based on i) fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by ii) reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier. We apply this method to a natural peer-tutoring corpus containing a significant number of disfluencies, repetitions, and repairs. The results show that generation in this noisy environment is feasible with reranking. By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation.\",\\n                    \"openAccessPdf\": {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 231}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'de1871cb4852ab318e0bef31f56b0f98'}>,\n",
              "  <Document: {'content': '              \"url\": \"http://arxiv.org/pdf/2306.14696\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work develops a model of hedge generation based on fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier.\"\\n                    }\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 232}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '47f03fbb840c814545b895b7eb9b5e1f'}>,\n",
              "  <Document: {'content': '       },\\n                {\\n                    \"paperId\": \"5360b929d1782d21ed71b5528fd546f9f15a4106\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Holding Large Language Models to Account\",\\n                    \"abstract\": \"If Large Language Models can make real scientific contributions, then they can genuinely use language, be systematically wrong, and be held responsible for their errors. AI models which can make scientific contributions thereby meet the criteria for scientific authorship. Keywords\\\\u2014 Large Language Models, authorship, responsibility, reference, hallucinations I. THE AI AUTHORSHIP CONTROVERSY Large Language Models (LLMs) are transformer-based deep-learning neural networks with hundreds of billions of parameters trained by self-supervised learning on large text corpora to perform next-token prediction. OpenAI\\\\u2019s November 2022 public release of their 175-billion parameter', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 233}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '63ae8982dc0141c9c615aa4686b0b502'}>,\n",
              "  <Document: {'content': 'GPT-3.5 model trained with Proximal Policy Optimization [1] made available for the first time an AI with human-level performance on a wide range of cognitive tasks [2] and its 4,096 token context window (~3000 words for prompt + response) allowed a wide domain of application [3]. The March 2013 release of GPT-4, with a maximum 32,768 token (~24,000 word) context window, performance at the upper end of the human scale on many cognitive tasks, and twice the measured factual reliability [2] has only increased the possible uses. One such use of LLMs is the production of scientific research, with hundreds of papers appearing on preprint servers with AIs listed as co-authors, some of which have been published with that authorship credit after peer review [4]. Since use of LLMs not only speeds the writing [5] and revision [6] process but also helps with literature review[7], algorithm development, data analysis, hypothesis generation [8] and even creativity [9] and argumentation [10], we can expect such use to continue to grow. Unlike in the case of previous computerized text generators like SCIgen [11] which merely slipped gibberish through sham or slipshod refereeing processes [12]\\\\u2013[15], LLMs generate text which can be genuinely useful and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 234}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb858eda648fadc1ce4bf692d09406b5'}>,\n",
              "  <Document: {'content': 'is sometimes undetectable even by dedicated referees [16], [17]. Until recently, the vast majority of ethical concern around LLM authorship has been about plagiarism [6], [18], [19]. Consequently, accountability efforts have focused on ways to deter or detect LLM use in scientific writing [16], [17]. Giving the LLM authorship credit neatly sidesteps plagiarism issues, however: if the LLM is listed as an author of the paper, then there can be no allegation that the other authors plagiarized from the LLM or that the contribution of the LLM lacked transparency. This transparency is further increased for journals which use a structured author contribution statement [20], [21] or contributor roles taxonomy [22], which would list the exact research and writing contributions made by the LLM to the final published product. Current suggestions for making such roles more specific [23] only raise the likelihood that LLMs would qualify for authorship. Furthermore, while not all actual writers of scientific literature must receive authorship credit in all disciplines according to prevailing ethical standards [24], almost one third of publication ethics codes and more than half of Social Sciences Citation Index journals require authorship credit for all participants in drafting and revising the text [25]. Even', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 235}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6424c0dbe1ac8d13b137f669d5c31539'}>,\n",
              "  <Document: {'content': 'in the remainder which also require scientific contributions, LLMs may qualify given the capabilities discussed above. Certainly, in many of the existing exemplars of published peer-reviewed scientific work with LLM authorship credits the LLM must make a \\\\u201csubstantial scientific contribution\\\\u201d if the work has one at all, since the vast majority of the text and nearly all of the argument comes in the form of text from unedited LLM token output. Without crediting LLMs as authors it is difficult to see how papers where they contribute substantially could comply with the International Committee of Medical Journal Editors (ICMJE) original fourth principle for authorship [26]: Each part of the content of an article critical to its main conclusions and each step in the work that led to its publication [(1) conception or design of the work represented by the article, or analysis and interpretation of the data, or both; (2) drafting the article or revising it for critically important content; and (3) final approval of the version to be published] must be attributable to at least one author. Cases where LLMs have received authorship credit have involved every one of these steps [27], [28]. Nonetheless, the influential Committee on Publication Ethics', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 236}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a739ace95d7e2bfebc59a4d400986eee'}>,\n",
              "  <Document: {'content': '(COPE) and World Association of Medical Editors (WAME) have called for banning AI authorship on the grounds that AIs \\\\u201ccannot take responsibility\\\\u201d for their output [29], [30], and this call has been heeded by Nature [31] while other influential journals have banned AI authorship without giving explicit reasons [32], [33]. ChatGPT\\\\u2019s authorship has been retracted in one case on this basis [34]. COPE\\\\u2019s standard combines a general responsibility test with a long history in the publication ethics literature going back to [26] with a more recent legal personhood test supposedly required for \\\\u201cassert[ing] the presence or absence of conflicts of interest\\\\u201d and \\\\u201cmanag[ing] copyright and license agreements\\\\u201d [29]. WAME spells out the latter, legal test as a matter of the corporate form chosen by OpenAI and its disclaimer of responsibility [30], which are obviously contingent matters not essential to AI. Indeed, various forms of legal personhood have already been proposed for algorithms which would allow them to enter into contracts [35]\\\\u2013[38] and corporations may soon be forced to assume liability for the AIs they create [39]\\\\u2013 [41]. LLMs are as capable of asserting the presence or absence of conflicts of interest as they are of asserting anything else. Philosophical interest', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 237}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '91f2f1c19fab367e72bed9b8db61b15e'}>,\n",
              "  <Document: {'content': 'in COPE\\\\u2019s new standard thus lies with its responsibility test, which is supposed to be an addition to (or even restriction of) the \\\\u201csubstantial scientific contribution\\\\u201d standard for authorship which LLMs cannot meet even if or when they meet the latter standard. COPE\\\\u2019s responsibility standard goes back to ICMJE\\\\u2019s original first principle for authorship [26]: Each author should have participated sufficiently in the work represented by the article to take public responsibility for the content...[which] means that an author can defend the content of the article, including the data and other evidence and the conclusions based on them. Such ability can come only from having participated closely in the work represented by the article and in preparing the article for publication. This responsibility also requires that the author be willing to concede publicly errors of fact or interpretation discovered after publication of the article and to state the reasons for error. In the case of fraud or other kinds of deception attributable to one or more authors, the other authors must be willing to state publicly the nature and extent of deception and to account as far as possible for its occurrence. LLMs like ChatGPT manifestly both defend their output', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 238}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8dd0783939141b705ff795b49788066a'}>,\n",
              "  <Document: {'content': '[9] and apologize for mistakes while giving reasons for their occurrence [6], [9] as well as identify particular human coauthors by their writing and offer criticisms [6]. WAME additionally references the current ICMJE standard that all authors must provide \\\\u201cFinal approval of the version to be published\\\\u201d [42] as a reason that AIs cannot meet the general responsibility test [30]. While some publications with LLMs listed as co-authors may be suspect in this regard [27], ChatGPT\\\\u2019s unwillingness to co-author is likely a result of its Reinforcement Learning from Human Feedback (RLHF) and is obviously not essential to LLMs. The COPE/WAME/Nature general responsibility test for authorship is thus best understood as a normative claim rather than a legal or behavioral one. ICMJE\\\\u2019s \\\\u201ccriteria are not intended for use as a means to disqualify colleagues from authorship who otherwise meet authorship criteria\\\\u201d [42], so the question is whether LLMs which meet the research and writing standards for authorship are able \\\\u201cto be accountable\\\\u201d in some normative sense. This is a fundamentally philosophical question. The philosophical response to COPE\\\\u2019s general responsibility test for AI authorship has been mixed. Wiese grants that current AIs are insufficiently agential to meet this constraint, but holds that', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 239}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '63ec0251dec48ae82bea7044ff77d195'}>,\n",
              "  <Document: {'content': 'future \\\\u201cstrong artificial consciousnesses\\\\u201d which observe the Free Energy Principle would exhibit the relevant normative properties [43]. Jenkins and Lin, by contrast, argue that many uncontroversial human authors (e.g., deceased ones) also cannot take responsibility, so that only the research and writing standards are appropriate [44]. Another similar approach suggests that responsibility for scientific publications is best understood as irreducibly collective among the authors [45] so that AIs are accountable as part of a system with relevantly-situated humans [46], i.e. co-authors. On this approach, if there is a single human coauthor to take responsibility, then the authorship team as a whole does, and further accountability is required of the AI. I take a third approach: if LLMs meet the research and writing standards for substantial scientific contribution, then Wittgenstein\\\\u2019s Private Language Argument suggests that they ipso facto meet the responsibility standard. II. AI AND LANGUAGE USE It is an open question whether Large Language Models count as users of language. Until recently, doubts about AI language use could be framed in terms of objective qualities of the token output. Much of SCIgen [11]\\\\u2019s output was \\\\u201cgibberish\\\\u201d [13], [15] with approximately English syntax comparable to Chomsky\\\\u2019s famous nonsense-sentence \\\\u201ccolorless green ideas', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 240}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4096ab8786f0acbd2be1b3a0f5771198'}>,\n",
              "  <Document: {'content': 'sleep furiously\\\\u201d [47]. It may have entered into the scientific literature through inattentive review or pay-for-play predatory publishers, but readers would likely struggle to identify propositional contents or truth conditions for its sentences. Since use of a declarative sentenc\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Giving the LLM authorship credit neatly sidesteps plagiarism issues, and it is difficult to see how papers where they contribute substantially could comply with the International Committee of Medical Journal Editors\\' original fourth principle for authorship.\"\\n                    }\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 241}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7be011eeedb08119a4b65754c440f6f9'}>,\n",
              "  <Document: {'content': ' }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"24bff26f19051b1413d1e343322c1ae4bba05428\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"6a470734-72c6-4809-a07d-d34dee0df4a1\",\\n                        \"name\": \"SIGDIAL Conferences\",\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 242}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '23661995419b88c028741e14bb9f56fd'}>,\n",
              "  <Document: {'content': ' \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"SIGDIAL\",\\n                            \"SIGDIAL Conf\",\\n                            \"Annu Meet Sp\\\\u00e9c Interest Group Discourse Dialogue\",\\n                            \"Annual Meeting of the Special Interest Group on Discourse and Dialogue\"\\n                        ]\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 243}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3d98c278aedfa93befaf3a0358e12ed4'}>,\n",
              "  <Document: {'content': '},\\n                    \"title\": \"When to generate hedges in peer-tutoring interactions\",\\n                    \"abstract\": \"This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviors. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models, including MLP and LSTM. The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model\\\\u2019s performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 244}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '119ce3b44d2cc40e9e66a2067ee7a867'}>,\n",
              "  <Document: {'content': '     \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2307.15582\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model\\\\u2019s performance and provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation.\"\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 245}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d864ee45270d0d15956c51509a10730'}>,\n",
              "  <Document: {'content': '           }\\n                }\\n            ]\\n        }\\n    ],\\n    \"Mona Diab\": [\\n        {\\n            \"total\": 296,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"0a94fbb5e1c93513523f00e75d672ef4553861f9\",\\n                    \"publicationVenue\": {\\n                        \"id\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 246}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5e23beba168345b3d5608b7ce393612e'}>,\n",
              "  <Document: {'content': '\"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Can Large Language Models Infer Causation from', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 247}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8541a4fdd6a1fddbe673ea443795c577'}>,\n",
              "  <Document: {'content': 'Correlation?\",\\n                    \"abstract\": \"Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 248}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b0cd93e873348ee30f08a9b8c274888c'}>,\n",
              "  <Document: {'content': 'to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs\\' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.05836\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 249}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90ea610e7244bd5fef801c54546d794b'}>,\n",
              "  <Document: {'content': '         \"text\": \"This work proposes the first benchmark dataset to test the pure causal inference skills of large language models (LLMs), and formulates a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"2630583bfb5a5924bc15264f72c77a27bcb98af7\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 250}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd21d56eb4d369092b802294b2c3a1a1e'}>,\n",
              "  <Document: {'content': '                 \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Large Language Models Can Infer Psychological Dispositions of Social Media Users\",\\n                    \"abstract\": \"As Large Language Models (LLMs) demonstrate increasingly human-like', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 251}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f2762bab958a8993a8163b5fb7e542fd'}>,\n",
              "  <Document: {'content': 'abilities in various natural language processing (NLP) tasks that are bound to become integral to personalized technologies, understanding their capabilities and inherent biases is crucial. Our study investigates the potential of LLMs like ChatGPT to infer psychological dispositions of individuals from their digital footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive the Big Five personality traits from users\\' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores. Furthermore, our findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-expression.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2309.08631\",\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 252}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c5b5d90ccad1fb8d752a14f0ced95edf'}>,\n",
              "  <Document: {'content': '  \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-expression.\"\\n                    }\\n                },\\n                {\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 253}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c25036890867384c1e151b31abf11143'}>,\n",
              "  <Document: {'content': '       \"paperId\": \"e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"37275deb-3fcf-4d16-ae77-95db9899b1f3\",\\n                        \"name\": \"IEEE/RJS International Conference on Intelligent RObots and Systems\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"IROS\",\\n                            \"Intelligent Robots and Systems\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 254}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37d74cc3ea763ea397a1a64129eeb6e7'}>,\n",
              "  <Document: {'content': '                       \"Intell Robot Syst\",\\n                            \"IEEE/RJS Int Conf Intell Robot Syst\"\\n                        ],\\n                        \"url\": \"http://www.iros.org/\"\\n                    },\\n                    \"title\": \"TidyBot: Personalized Robot Assistance with Large Language Models\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 255}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'df69df731bb86f06bd771745ea0d7ac5'}>,\n",
              "  <Document: {'content': '                     \"url\": \"https://arxiv.org/pdf/2305.05658\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is shown that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions.\"\\n                    }\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 256}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e03489ea726e19333a905f1e3d1fdca7'}>,\n",
              "  <Document: {'content': '      }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"f727f928e7e179307d8d4a1da2387393f2bd7915\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"8de18c35-6785-4e54-99f2-21ee961302c6\",\\n                        \"name\": \"Conference of the European Chapter of the Association for Computational Linguistics\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 257}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5b3f5527bef067d581b79f9273373c87'}>,\n",
              "  <Document: {'content': '               \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Conf Eur Chapter Assoc Comput Linguistics\",\\n                            \"EACL\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/eacl/\"\\n                    },\\n                    \"title\": \"Methods for Measuring, Updating, and Visualizing Factual Beliefs', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 258}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e6b3ed15c1c8045e858c71ff439f4ec5'}>,\n",
              "  <Document: {'content': 'in Language Models\",\\n                    \"abstract\": \"Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 259}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a06f53c92c6407a52274bfdcae82964c'}>,\n",
              "  <Document: {'content': '   \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.eacl-main.199.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work.\"\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 260}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dc21932b381a01e64890e12d347904af'}>,\n",
              "  <Document: {'content': '       }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"45f7ab2dd1bd86703f3fc0f713d35851ae15b038\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"ea8553fe-2467-4367-afee-c4deb3754820\",\\n                        \"name\": \"Artificial Intelligence', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 261}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '30913cc061546aa1751fdf8d1ef94403'}>,\n",
              "  <Document: {'content': 'Review\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Artif Intell Rev\"\\n                        ],\\n                        \"issn\": \"0269-2821\",\\n                        \"url\": \"https://link.springer.com/journal/10462\"\\n                    },\\n                    \"title\": \"Author Correction: Arabic natural language', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 262}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edafe1fe540f0e15feb767ebd2aebcac'}>,\n",
              "  <Document: {'content': 'processing for Qur\\\\u2019anic research: a systematic review\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://link.springer.com/content/pdf/10.1007/s10462-023-10390-x.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": null\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 263}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5a78365531610f8ec03ce11068442f00'}>,\n",
              "  <Document: {'content': '        \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"c218cd1772999517b137bbbc9872c4f67e540b7f\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models\",\\n                    \"abstract\": \"We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 264}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd76e12294d3d2512147b1d097e3631a'}>,\n",
              "  <Document: {'content': 'evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model\\\\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.nlrse-1.10.pdf\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 265}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c8cb15dbd7feae66b77b88ce7231ce20'}>,\n",
              "  <Document: {'content': '     \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is revealed that having explanations in the fewshot exemplar has no significant impact on the model\\\\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart, and a slight yet consistent increase in classification accuracy as the authors incorporate explanations during prompting and finetuning.\"\\n                    }\\n                }\\n            ]\\n        },\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 266}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '536fcffba0d7a9adfc0c864e947835e0'}>,\n",
              "  <Document: {'content': '       {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"c5849f406e8263806a84e1a407ec0e0fe131bd5c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"5020dc0a-7e2f-43e6-9543-389c0c6d864d\",\\n                        \"name\": \"International Workshop on Spoken Language Translation\",\\n                        \"type\": \"conference\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 267}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b41d5291859919f7358ac466bba38ca'}>,\n",
              "  <Document: {'content': '        \"alternate_names\": [\\n                            \"IWSLT\",\\n                            \"Int Workshop Spok Lang Transl\"\\n                        ]\\n                    },\\n                    \"title\": \"Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology\",\\n                    \"abstract\": \"We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 268}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dbd8f91c7a760567c85d3deb37fecf95'}>,\n",
              "  <Document: {'content': 'and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.iwslt-1.2.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 269}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bd1dd42e641498cd25d715d129e6b681'}>,\n",
              "  <Document: {'content': 'audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 127,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"4286d07449447f3bfffc1eeb2ee0de9b00dfadfd\",\\n                    \"publicationVenue\": {\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 270}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1c5cf58dafbae31bd09da82581c96305'}>,\n",
              "  <Document: {'content': '                      \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 271}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9fcc0f4738607531dbfd7a9276d9984e'}>,\n",
              "  <Document: {'content': '                           \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"ALERT: Adapt Language Models to Reasoning Tasks\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.acl-long.60.pdf\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 272}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '38b13f6c62a9856682b51de80ec4ef3d'}>,\n",
              "  <Document: {'content': '                 \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage, but also finds that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.\"\\n                    }\\n                },\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 273}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5efa8b11539189b2e9def209b8bb92ab'}>,\n",
              "  <Document: {'content': '               {\\n                    \"paperId\": \"102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 274}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'daed15923fd0ea5a30e007e1724e201'}>,\n",
              "  <Document: {'content': '        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Collaborating with language models for embodied reasoning\",\\n                    \"abstract\": \"Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 275}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '815b1fd3d93d4126846bd5a7ff51385a'}>,\n",
              "  <Document: {'content': 'in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system\\'s ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2302.00763\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 276}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd6497a76233b2de9412363c23a32f11d'}>,\n",
              "  <Document: {'content': '               \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work investigates how to combine complementary abilities in a single system consisting of a Planner, an Actor, and a Reporter, and presents a set of tasks that require reasoning, test this system\\'s ability to generalize zero-shot and investigate failure cases, and demonstrates how components of this system can be trained with reinforcement-learning to improve performance.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"0766410db4a987ebebeb0eb5f132ac9f1fdd8fda\",\\n                    \"publicationVenue\": {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 277}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f269b781b98b713e07a0ba13959c0336'}>,\n",
              "  <Document: {'content': '              \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 278}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6b1f57195e2bf1bb1ef15a2d767af21f'}>,\n",
              "  <Document: {'content': '                   \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages\",\\n                    \"abstract\": \"Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning.The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 279}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '829245edf5e520662aa5d3f5c68caef8'}>,\n",
              "  <Document: {'content': 'variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.We utilize a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora. Our code is available at https://github.com/Yasminekaroui/CliCoTea.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.16774\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 280}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd218f571687945f6a9b478df8aafc8ce'}>,\n",
              "  <Document: {'content': '   \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"An approach to adapt VLP to unseen languages using MPLM using a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages that outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 72,\\n            \"offset\": 0,\\n            \"next\": 3,\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 281}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56ff54f58f21f1623116b2dc307daab8'}>,\n",
              "  <Document: {'content': '     \"data\": [\\n                {\\n                    \"paperId\": \"e8b0e846c3fac074d6eb8a9990d50c0d43e309ea\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 282}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bf20f6e973cc3bada86f0cc46278a8a9'}>,\n",
              "  <Document: {'content': '               \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"TESS: A Multi-intent Parser for Conversational Multi-Agent Systems with Decentralized Natural Language Understanding Models\",\\n                    \"abstract\": \"Chatbots have become one of the main pathways for the delivery of business automation tools. Multi-agent systems offer a framework for designing chatbots at scale, making it easier to support complex conversations that span across multiple domains as well as enabling developers to maintain and expand their capabilities incrementally over time. However, multi-agent systems complicate the natural language understanding (NLU) of user intents, especially when they rely on decentralized NLU models: some utterances (termed single intent) may invoke a single agent while others (termed', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 283}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ac4653b531a7d067e5de91e11d0d105'}>,\n",
              "  <Document: {'content': 'multi-intent) may explicitly invoke multiple agents. Without correctly parsing multi-intent inputs, decentralized NLU approaches will not achieve high prediction accuracy. In this paper, we propose an efficient parsing and orchestration pipeline algorithm to service multi-intent utterances from the user in the context of a multi-agent system. Our proposed approach achieved comparable performance to competitive deep learning models on three different datasets while being up to 48 times faster.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper proposes an efficient parsing and orchestration pipeline algorithm to service multi-intent utterances from the user in the context of a multi-agent system, achieving comparable performance to competitive deep learning models on three different datasets while being up to 48', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 284}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '51fd04693589e72a51f533084096ab2b'}>,\n",
              "  <Document: {'content': 'times faster.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"5c2a9d180ac192505e2ca08255786c41070330b7\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"d43f0f26-43c9-4009-a1ed-63624522c166\",\\n                        \"name\": \"International Conference on Intelligent User Interfaces\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 285}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3e2cbc08d4f6118239688a2c31cf089b'}>,\n",
              "  <Document: {'content': '[\\n                            \"IUI\",\\n                            \"Intell User Interface\",\\n                            \"Int Conf Intell User Interface\",\\n                            \"Intelligent User Interfaces\"\\n                        ],\\n                        \"url\": \"http://www.iuiconf.org/\"\\n                    },\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 286}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8e950b151f319eacb5fd6ffa3a73e889'}>,\n",
              "  <Document: {'content': '         \"title\": \"The Role of Lexical Alignment in Human Understanding of Explanations by Conversational Agents\",\\n                    \"abstract\": \"Explainable Artificial Intelligence (XAI) focuses on research and technology that can explain an AI system\\\\u2019s functioning and its underlying methods, and also on making these explanations better through personalization. Our research study investigates a natural language personalization method called lexical alignment in understanding an explanation provided by a conversational agent. The study setup was online and navigated the participants through an interaction with a conversational agent. Participants faced either an agent designed to align its responses to those of the participants, a misaligned agent, or a control condition that did not involve any dialogue. The dialogue delivered an explanation based on a pre-defined set of causes and effects. The recall and understanding of the explanations was evaluated using a combination of Yes-No questions, a Cloze test (fill-in-the-blanks), and What-style questions. The analysis of the test scores revealed a significant advantage in information recall for those who interacted with an aligning agent against the participants who either interacted', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 287}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7330810f43dc93845ea7b6371c8644dd'}>,\n",
              "  <Document: {'content': 'with a non-aligning agent or did not go through any dialogue. The Yes-No type questions that included probes on higher-order inferences (understanding) also reflected an advantage for the participants who had an aligned dialogue against both non-aligned and no dialogue conditions. The results overall suggest a positive effect of lexical alignment on understanding of explanations.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3581641.3584086\",\\n                        \"status\": \"BRONZE\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 288}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9437d0d7bdf41b69af1af171d9a894fb'}>,\n",
              "  <Document: {'content': '               \"text\": \"This research study investigates a natural language personalization method called lexical alignment in understanding an explanation provided by a conversational agent and suggests a positive effect of lexical aligned on understanding of explanations.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"3ec625fabd8c43a05381d601b4e15dead0ae2317\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 289}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '125c6028d7e0506ada70a58fec623c67'}>,\n",
              "  <Document: {'content': '                \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Interactive Natural Language Processing\",\\n                    \"abstract\": \"Interactive Natural Language Processing (iNLP) has emerged as a novel paradigm within the field of NLP,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 290}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '981087a9e788eb23c888d7f1404c71ce'}>,\n",
              "  <Document: {'content': 'aimed at addressing limitations in existing frameworks while aligning with the ultimate goals of artificial intelligence. This paradigm considers language models as agents capable of observing, acting, and receiving feedback iteratively from external entities. Specifically, language models in this context can: (1) interact with humans for better understanding and addressing user needs, personalizing responses, aligning with human values, and improving the overall user experience; (2) interact with knowledge bases for enriching language representations with factual knowledge, enhancing the contextual relevance of responses, and dynamically leveraging external information to generate more accurate and informed responses; (3) interact with models and tools for effectively decomposing and addressing complex tasks, leveraging specialized expertise for specific subtasks, and fostering the simulation of social behaviors; and (4) interact with environments for learning grounded representations of language, and effectively tackling embodied tasks such as reasoning, planning, and decision-making in response to environmental observations. This paper offers a comprehensive survey of iNLP, starting by proposing a unified definition and framework of the concept. We then provide a systematic classification of iNLP, dissecting its various components, including interactive objects, interaction interfaces, and interaction methods. We proceed to delve into the evaluation methodologies used in the field, explore its', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 291}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '64b37bc323a5e1e438caa7a91f8aaaa7'}>,\n",
              "  <Document: {'content': 'diverse applications, scrutinize its ethical and safety issues, and discuss prospective research directions. This survey serves as an entry point for researchers who are interested in this rapidly evolving area and offers a broad view of the current landscape and future trajectory of iNLP.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.13246\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 292}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '53ddc7079a59652bae86ac4525a20a6'}>,\n",
              "  <Document: {'content': '    \"text\": \"This paper provides a systematic classification of iNLP, dissecting its various components, including interactive objects, interaction interfaces, and interaction methods, and delves into the evaluation methodologies used in the field, exploring its diverse applications, and scrutinize its ethical and safety issues.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"5e2f8088647e357bb6440d271ed1fcc4d5ed7e7c\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 293}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58f76b4349d7b6d224d0453a2389c7fd'}>,\n",
              "  <Document: {'content': '     \"publicationVenue\": {\\n                        \"id\": \"a631cd93-9863-44c8-8dff-9eb3fbf4f83e\",\\n                        \"name\": \"Clinical Natural Language Processing Workshop\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ClinicalNLP\",\\n                            \"Clin Nat Lang Process Workshop\"\\n                        ]\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 294}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8d971a5ca1ede3f5b3e909dc569129e1'}>,\n",
              "  <Document: {'content': '              },\\n                    \"title\": \"Care4Lang at MEDIQA-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues\",\\n                    \"abstract\": \"Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore andBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.clinicalnlp-1.55.pdf\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 295}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '33d850b66900ce94a3dde588a3d7c08d'}>,\n",
              "  <Document: {'content': '    \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper presents their submission to this task using fine-tuned language models, including T5, BART and BioGPT models, and finds Flan-T5 achieved the highest aggregated score for dialogue summarization.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 296}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8e31cdaca786e16e2587f91b83d36c46'}>,\n",
              "  <Document: {'content': '\"total\": 0,\\n            \"offset\": 0\\n        }\\n    ],\\n    \"Fernando Diaz\": [\\n        {\\n            \"total\": 462,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"fad620bb0aa507f0a61bdf0e0496fe2e8a017aa4\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"c34aaa9c-865d-444d-96c0-7dc7dc341575\",\\n                        \"name\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 297}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c7fdd57cc73244d1b56b4b41625fbe8'}>,\n",
              "  <Document: {'content': '\"ACM Transactions on Recommender Systems\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"ACM Trans Recomm Syst\"\\n                        ],\\n                        \"issn\": \"2770-6699\"\\n                    },\\n                    \"title\": \"Distributionally-Informed Recommender System Evaluation\",\\n                    \"abstract\": \"Current', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 298}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '647e1b8dd0f6d158b6c1e520a4674072'}>,\n",
              "  <Document: {'content': 'practice for evaluating recommender systems typically focuses on point estimates of user-oriented effectiveness metrics or business metrics, sometimes combined with additional metrics for considerations such as diversity and novelty. In this paper, we argue for the need for researchers and practitioners to attend more closely to various distributions that arise from a recommender system (or other information access system) and the sources of uncertainty that lead to these distributions. One immediate implication of our argument is that both researchers and practitioners must report and examine more thoroughly the distribution of utility between and within different stakeholder groups. However, distributions of various forms arise in many more aspects of the recommender systems experimental process, and distributional thinking has substantial ramifications for how we design, evaluate, and present recommender systems evaluation and research results. Leveraging and emphasizing distributions in the evaluation of recommender systems is a necessary step to ensure that the systems provide appropriate and equitably-distributed benefit to the people they affect.\",\\n                    \"openAccessPdf\": {\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 299}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ad4c1a1ebd2a13c12a793e67c7d434c'}>,\n",
              "  <Document: {'content': '     \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3613455\",\\n                        \"status\": \"BRONZE\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The need for researchers and practitioners to attend more closely to various distributions that arise from a recommender system (or other information access system) and the sources of uncertainty that lead to these distributions is argued.\"\\n                    }\\n                },\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 300}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '69435c30ad089b707ad25a66c9688b8'}>,\n",
              "  <Document: {'content': '         {\\n                    \"paperId\": \"c188c1010961d4de1e606574db4d6f629df3a00d\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"c34aaa9c-865d-444d-96c0-7dc7dc341575\",\\n                        \"name\": \"ACM Transactions on Recommender Systems\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"ACM Trans Recomm Syst\"\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 301}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b425005815a7474d6a6d9582c4fdaa0'}>,\n",
              "  <Document: {'content': '          ],\\n                        \"issn\": \"2770-6699\"\\n                    },\\n                    \"title\": \"On Item-Sampling Evaluation for Recommender System\",\\n                    \"abstract\": \"Personalized recommender system plays a crucial role in modern society, especially in e-commerce, news, and ads area. Correctly evaluating and comparing candidate recommendation models is as essential as constructing ones. The common offline evaluation strategy is holding out some user-interacted items from training data and evaluating the performance of recommendation models based on how many items they can retrieve. Specifically, for any hold-out item or so-called target item for a user, the recommendation models try to predict the probability that the user would interact with the item, and rank it among overall items, this is called global evaluation.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 302}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e95a7309fde8945c8488c75775f046dd'}>,\n",
              "  <Document: {'content': 'Intuitively, a good recommendation model would assign high probabilities to such hold-out/target items. Based on the specific ranks, some metrics like Recall@K and NDCG@K can be calculated to further quantify the quality of the recommender model. Instead of ranking the target items among all items, Koren [22] first proposed to rank them among a small sampled set of items, and then quantified the performance of the models, this is called sampling evaluation. Ever since then, there has been a large amount of work adopting sampling evaluation due to its efficiency and frugality. In recent work, Rendle and Krichene [24, 32] argued that the sampling evaluation is \\\\u201dinconsistent\\\\u201d with respect to a global evaluation in terms of offline top-k metrics. In this work, we first investigate the \\\\u201dinconsistent\\\\u201d phenomenon by taking a glance at the connections between sampling evaluation and global evaluation. We reveal the approximately linear relationship between sampling with respect to its global counterpart in terms of the top-K Recall metric. Second, we propose a new statistical perspective of the sampling evaluation - to estimate the global rank distribution of the entire population. After the estimated rank distribution is obtained, the approximation of the global metric can be further', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 303}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c12c57a1229881c23791eddb6b31bd7'}>,\n",
              "  <Document: {'content': 'derived. Third, we extend the work of Krichene and Rendle [24], directly optimizing the error with ground truth, providing not only a comprehensive empirical study but also a rigorous theoretical understanding of the proposed metric estimators. To address the \\\\u201dblind spot\\\\u201d issue, where accurately estimating metrics for small top-K values in sampling evaluation is challenging, we propose a novel adaptive sampling method that generalizes the expectation-maximization (EM) algorithm to this setting. Last but not least, we also study the user sampling evaluation effect. This series of works outlines a clear roadmap for sampling evaluation and establishes a foundational theoretical framework. Extensive empirical studies validate the reliability of the sampling methods presented.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3629171\",\\n                        \"status\": \"BRONZE\"\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 304}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e8f7c417b7d9f439dc5bb51fbf2faf1'}>,\n",
              "  <Document: {'content': ' },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The \\\\u201dinconsistent\\\\u201d phenomenon is investigated, the approximately linear relationship between sampling with respect to its global counterpart in terms of the top-K Recall metric is revealed and a new statistical perspective of the sampling evaluation is proposed to estimate the global rank distribution of the entire population.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"d7753c85e1f463a5fd70d96befa765d093bec52b\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 305}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a087bc779119426f65b96e6288d67818'}>,\n",
              "  <Document: {'content': '           \"publicationVenue\": {\\n                        \"id\": \"cf7b0aac-26b9-4d6a-ba15-e72f6755e11c\",\\n                        \"name\": \"User modeling and user-adapted interaction\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"User model user-adapted interact\",\\n                            \"User Modeling and User-adapted Interaction\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 306}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4389f33e31e8dd9148e17c76ae58fd40'}>,\n",
              "  <Document: {'content': '       \"User Model User-adapted Interact\"\\n                        ],\\n                        \"issn\": \"0924-1868\",\\n                        \"url\": \"https://link.springer.com/journal/11257\"\\n                    },\\n                    \"title\": \"\\\\u201cTell Me Why\\\\u201d: using natural language justifications in a recipe recommender system to support healthier food choices\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 307}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac8cfbeb8842356e83dc6fcefd0ed728'}>,\n",
              "  <Document: {'content': '       \"url\": \"https://link.springer.com/content/pdf/10.1007/s11257-023-09377-8.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Results indicated that justifications led to significantly healthier choices for first course meals, while strategies that compared food features and emphasized health risks, benefits, and a user\\\\u2019s lifestyle were most effective, catering to health-related choice motivations.\"\\n                    }\\n                }\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 308}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '652952580170413b2951b1532bb6538a'}>,\n",
              "  <Document: {'content': '       ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"e0b15217bc69aa0c06cfdade44d659570ebf774b\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"AI Consent Futures: A Case Study on Voice Data Collection with Clinicians\",\\n                    \"abstract\": \"As new forms of data capture emerge to power new AI applications, questions abound about the ethical implications of these data collection practices. In this paper, we', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 309}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ec298769ca99c2c39581284ec620a8d'}>,\n",
              "  <Document: {'content': 'present clinicians\\' perspectives on the prospective benefits and harms of voice data collection during health consultations. Such data collection is being proposed as a means to power models to assist clinicians with medical data entry, administrative tasks, and consultation analysis. Yet, clinicians\\' attitudes and concerns are largely absent from the AI narratives surrounding these use cases, and the academic literature investigating them. Our qualitative interview study used the concept of an informed consent process as a type of design fiction, to support elicitation of clinicians\\' perspectives on voice data collection and use associated with a fictional, near-term AI assistant. Through reflexive thematic analysis of in-depth sessions with physicians, we distilled eight classes of potential risks that clinicians are concerned about, including workflow disruptions, self-censorship, and errors that could impact patient eligibility for services. We conclude with an in-depth discussion of these prospective risks, reflect on the use of the speculative processes that illuminated them, and reconsider evaluation criteria for AI-assisted clinical documentation technologies in light of our findings.\",\\n                    \"openAccessPdf\": {\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 310}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b1d5f4ae932fcb761c14d27d86ad3ec'}>,\n",
              "  <Document: {'content': '            \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3610107\",\\n                        \"status\": \"BRONZE\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Clinicians\\' perspectives on the prospective benefits and harms of voice data collection during health consultations are presented and evaluation criteria for AI-assisted clinical documentation technologies are reconsidered in light of these findings.\"\\n                    }\\n                }\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 311}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac64ae246b43cc446d4669ab0a71ce46'}>,\n",
              "  <Document: {'content': '        ]\\n        },\\n        {\\n            \"total\": 4422,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"b9c6e200dcf832a99d2b26f84da8b5fa2e8dab04\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"ea38228f-6ed3-4222-a3ce-d963d8cc9516\",\\n                        \"name\": \"Web Search and Data Mining\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 312}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '54b042535f5a7ee4924868a2bd33d862'}>,\n",
              "  <Document: {'content': '            \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Web Search Data Min\",\\n                            \"WSDM\"\\n                        ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=3158\"\\n                    },\\n                    \"title\": \"Preference-Based Offline Evaluation\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 313}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f976973e4c3c9fc8dcf4922bde01bae3'}>,\n",
              "  <Document: {'content': '         \"abstract\": \"A core step in production model research and development involves the offline evaluation of a system before production deployment. Traditional offline evaluation of search, recommender, and other systems involves gathering item relevance labels from human editors. These labels can then be used to assess system performance using offline evaluation metrics. Unfortunately, this approach does not work when evaluating highly effective ranking systems, such as those emerging from the advances in machine learning. Recent work demonstrates that moving away from pointwise item and metric evaluation can be a more effective approach to the offline evaluation of systems. This tutorial, intended for both researchers and practitioners, reviews early work in preference-based evaluation and covers recent developments in detail.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 314}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '434b8e3f589cce868efc1d3ad254de06'}>,\n",
              "  <Document: {'content': '                \"text\": \"This tutorial, intended for both researchers and practitioners, reviews early work in preference-based evaluation and covers recent developments in detail.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"99fad270248f2593293264956ec011b0f014d557\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 315}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c1490b65c2bbff2fee03eb30a94bb053'}>,\n",
              "  <Document: {'content': '    \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Kernelized Offline Contextual Dueling Bandits\",\\n                    \"abstract\": \"Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 316}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5ac554d6385cca1e3194bfdb56c264ba'}>,\n",
              "  <Document: {'content': 'feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2307.11288\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 317}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '75db27d5d658c8454ff3cb5c7ebdb74e'}>,\n",
              "  <Document: {'content': '   \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"An upper-confidence-bound style algorithm is given for this setting and empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts and proves a regret bound.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"233c06017ed41c40140947796525ce7452c93ab9\",\\n                    \"publicationVenue\": {\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 318}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '307230a7ac7dfd342b64a7786c1cb43'}>,\n",
              "  <Document: {'content': '\"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Sample Efficient Reinforcement Learning from Human', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 319}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3c6999930e864ad4bd3b6d26e989696a'}>,\n",
              "  <Document: {'content': 'Feedback via Active Exploration\",\\n                    \"abstract\": \"Preference-based feedback is important for many applications in reinforcement learning where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback (RLHF) on large language models. For many applications of RLHF, the cost of acquiring the human feedback can be substantial. In this work, we take advantage of the fact that one can often choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and formalize this as an offline contextual dueling bandit problem. We give an upper-confidence-bound style algorithm for this problem and prove a polynomial worst-case regret bound. We then provide empirical confirmation in a synthetic setting that our approach outperforms existing methods. After, we extend the setting and methodology for practical use in RLHF training of large language models. Here, our method is able to reach better performance with fewer samples of human preferences than multiple baselines on three real-world datasets.\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 320}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1b6a4b4216c7c145dd839e3531d6766a'}>,\n",
              "  <Document: {'content': '    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work takes advantage of the fact that one can often choose contexts at which to obtain human feedback in order to most efficiently identify a good policy and formalizes this as an offline contextual dueling bandit problem and gives an upper-confidence-bound style algorithm and proves a polynomial worst-case regret bound.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 321}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '602b2eb1b3342be698efd9a2e8e1fc3d'}>,\n",
              "  <Document: {'content': '  \"total\": 539,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"225a242405d1629b18b7c4367a3101509c9274bb\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 322}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4abb74c9bc2f9e68d6cdea150d77aec'}>,\n",
              "  <Document: {'content': '   \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Scaling Laws Do Not Scale\",\\n                    \"abstract\": \"Recent work has proposed a power law relationship, referred to as ``scaling laws,\\'\\' between the performance of artificial intelligence (AI) models and aspects of those models\\' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 323}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b18f112bd90127b2b3d66d95809da5ff'}>,\n",
              "  <Document: {'content': 'However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models\\' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or preferences not captured by (or in the worst case, at odds with) the metrics used to evaluate model performance for scaling laws. We end the paper with implications for AI scaling laws -- that models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models.\",\\n                    \"openAccessPdf\": {\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 324}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36023a15b49ddc0cf5733d862ba37f77'}>,\n",
              "  <Document: {'content': '        \"url\": \"https://arxiv.org/pdf/2307.03201\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is argued that as the size of datasets used to train large AI models grows, the number of distinct communities whose data is included in a given dataset is likely to grow, each of whom may have different values.\"\\n                    }\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 325}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '82aa0a34194b40301fffb0af7e5b9abe'}>,\n",
              "  <Document: {'content': '},\\n                {\\n                    \"paperId\": \"50b410554589cfd736187ded2f2bb92f4d63271c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 326}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c347eabe352a9a5718789bd653c9c930'}>,\n",
              "  <Document: {'content': '         \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Neural scaling laws for phenotypic drug discovery\",\\n                    \"abstract\": \"Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark: a diverse set of drug development tasks posed on image-based high content screening data. Surprisingly,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 327}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '81e3c548468c62289163b7fea8143f5d'}>,\n",
              "  <Document: {'content': 'we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs first trained with IBP then probed for performance on the Pheno-CA significantly outperform task-supervised DNNs. More importantly, the performance of these IBP-trained DNNs monotonically improves with data and model scale. Our findings reveal that the DNN ingredients needed to accurately solve small molecule drug development tasks are already in our hands, and project how much more experimental data is needed to achieve any desired level of improvement. We release our Pheno-CA benchmark and code to encourage further study of neural scaling laws for small molecule drug discovery.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2309.16773\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 328}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '747b1d801dea43dbad662d132dac39e3'}>,\n",
              "  <Document: {'content': '               \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work introduces a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP and finds that DNNs first trained with IBP then probed for performance on the Pheno-CA significantly outperform task-supervised DNNS.\"\\n                    }\\n                },\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 329}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e64710368b210dcf5c48ac6ff82ff306'}>,\n",
              "  <Document: {'content': '    {\\n                    \"paperId\": \"53f9642e0759155d0ff92299121ef45bd9a1b959\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Generalizations for Cell Biological Explanations: Distinguishing between Principles and Laws\",\\n                    \"abstract\": \"Laws have figured in the development of modern biology (e.g. Mendelian laws of inheritance), but there is a tacit assumption particularly in contemporary cell and molecular biology that laws are only of the \\\\u2018strict\\\\u2019 kind (e.g. the laws of motion or universal gravitation), which cell biology appears to lack. Moreover, the cell-biology-specific non-universal laws that do exist (e.g. scaling laws in biochemical networks within single cells) are few and far between. As discussed elsewhere (and not further argued for in this paper), mechanistic explanations, which are the dominant kind of explanation in cell biology, face significant challenges and their utility has been checkered', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 330}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '558dbf2a61b304717462b266bebddf94'}>,\n",
              "  <Document: {'content': 'in different biomedical areas. Just as laws and mechanisms figure in explanations in organic chemistry and ecology, fields that deal with lower- and higher-scale phenomena compared to cell biology, respectively, it should not be assumed that cell biology is somehow in a unique position where few or no laws could be discovered and used in its explanations. An impediment to discovering lawlike generalizations in cell biology is that the understanding of many cellular phenomena is still quite qualitative and imprecise. This paper is motivated by the premise that mechanisms and laws can both be in the foreground of explanations in cell biology and that a framework should be developed to encourage and facilitate the discovery of laws specific to and operative at the individual cell level. To that end, in the domain of scientifically-relevant non-universal (i.e. non-exceptionless) generalizations, which some philosophers equate with the notion of ceteris paribus laws (henceforth, \\\\u2018cp-laws\\\\u2019), I propose that a cp-law might have one or more corresponding \\\\u2018principles\\\\u2019. Using a running example of generalizations of oscillatory movements from physics with direct relevance to cell biology, I argue\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 331}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f719c488a2508141ccee3796ad2285d2'}>,\n",
              "  <Document: {'content': ' \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The premise that mechanisms and laws can both be in the foreground of explanations in cell biology and that a framework should be developed to encourage and facilitate the discovery of laws specific to and operative at the individual cell level is motivated.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 332}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '209d4a044668f8f2ec286305bb54de74'}>,\n",
              "  <Document: {'content': '    \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 333}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58ab985af0c9506eea055b3334f8fcc4'}>,\n",
              "  <Document: {'content': '   ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision\",\\n                    \"abstract\": \"Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 334}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b977b6c753caf68d95f6d2bbd9da1883'}>,\n",
              "  <Document: {'content': 'best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.07908\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 335}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9041a2ce1c78bf1324992c1db67c6a0c'}>,\n",
              "  <Document: {'content': '      \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work addresses the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 336}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ca8f03b6bfb5045ad6cbac69b6a8ff8'}>,\n",
              "  <Document: {'content': '            \"paperId\": \"b5252203514eb44930e25445bf15ad38458e151d\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 337}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2f1c80648e8888795303a0207e300aa2'}>,\n",
              "  <Document: {'content': '          \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Commonality in Recommender Systems: Evaluating Recommender Systems to Enhance Cultural Citizenship\",\\n                    \"abstract\": \"Recommender systems have become the dominant means of curating cultural content, significantly influencing individual cultural experience. Since recommender systems tend to optimize for personalized user experience, they can overlook impacts on cultural experience in the aggregate. After demonstrating that existing metrics do not center culture, we introduce a new metric, commonality, that measures the degree to which recommendations familiarize a given user population with specified categories of cultural content. We developed commonality through an interdisciplinary dialogue between researchers in computer science and the social sciences and humanities. With reference to principles underpinning public service media systems in democratic societies, we identify universality of address and content diversity in the service of strengthening cultural citizenship as particularly relevant', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 338}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f90db31066a908a24d18c2c2af44b910'}>,\n",
              "  <Document: {'content': 'goals for recommender systems delivering cultural content. We develop commonality as a measure of recommender system alignment with the promotion of content toward a shared cultural experience across a population of users. We empirically compare the performance of recommendation algorithms using commonality with existing metrics, demonstrating that commonality captures a novel property of system behavior complementary to existing metrics. Alongside existing fairness and diversity metrics, commonality contributes to a growing body of scholarship developing `public good\\' rationales for machine learning systems.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2302.11360\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 339}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '98922d70bf6dc19b6ce9a27f38cad5d'}>,\n",
              "  <Document: {'content': '                \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A new metric, commonality, is introduced that measures the degree to which recommendations familiarize a given user population with specified categories of cultural content and contributes to a growing body of scholarship developing `public good\\' rationales for machine learning systems.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 10001,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 340}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4468d68ba2793e18a7be54beb3cc98be'}>,\n",
              "  <Document: {'content': '             {\\n                    \"paperId\": \"207eef617effbe504912e5112088c61d025536b7\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 341}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f12f5c109e2339c0f6120bfff15b9fb'}>,\n",
              "  <Document: {'content': '      \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Group Membership Bias\",\\n                    \"abstract\": \"When learning to rank from user interactions, search and recommendation systems must address biases in user behavior to provide a high-quality ranking. One type of bias that has recently been studied in the ranking literature is when sensitive attributes, such as gender, have an impact on a user\\'s judgment about an item\\'s utility. For example, in a search for an expertise area, some users may be biased towards clicking on male candidates over female candidates. We call this type of bias group membership bias or group bias for short. Increasingly, we seek rankings that not only have high utility but are also fair to', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 342}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f0eb31be8b879bdd8348b6a60ef16378'}>,\n",
              "  <Document: {'content': 'individuals and sensitive groups. Merit-based fairness measures rely on the estimated merit or utility of the items. With group bias, the utility of the sensitive groups is under-estimated, hence, without correcting for this bias, a supposedly fair ranking is not truly fair. In this paper, first, we analyze the impact of group bias on ranking quality as well as two well-known merit-based fairness metrics and show that group bias can hurt both ranking and fairness. Then, we provide a correction method for group bias that is based on the assumption that the utility score of items in different groups comes from the same distribution. This assumption has two potential issues of sparsity and equality-instead-of-equity, which we use an amortized approach to solve. We show that our correction method can consistently compensate for the negative impact of group bias on ranking quality and fairness metrics.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2308.02887\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 343}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '28d4da6c94b32e51c1d9a9815b5b23fc'}>,\n",
              "  <Document: {'content': '             \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A correction method is provided for group bias that is based on the assumption that the utility score of items in different groups comes from the same distribution and has two potential issues of sparsity and equality-instead-of-equity, which the method is used to solve.\"\\n                    }\\n                },\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 344}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3afeeb5bbff4ea992060d011e25809dc'}>,\n",
              "  <Document: {'content': '{\\n                    \"paperId\": \"fb6a9aaac430e25f908f3edd4722c7260bccc9a1\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"0ecb6fd2-9e61-4664-ae8e-aa8b8dd310fa\",\\n                        \"name\": \"European Journal of Health Psychology\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Eur J Health Psychol\"\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 345}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '800eddc6b62851cea0f55a7cba59f6c9'}>,\n",
              "  <Document: {'content': ' ],\\n                        \"issn\": \"2512-8450\",\\n                        \"url\": \"https://econtent.hogrefe.com/loi/zgp\"\\n                    },\\n                    \"title\": \"Multiple Group Membership, Optimistic Bias, and Infection Risk in the Context of Emerging Infectious Diseases\",\\n                    \"abstract\": \"Abstract. Background: Understanding psychosocial factors which impact responses to emerging infectious diseases (EIDs) is vital in managing epidemics and pandemics. Two under-researched areas in this field are the interactive roles of optimistic bias (underestimation of the likelihood of negative events occurring to the self, relative to others) and group membership (a factor observed to be psychologically protective, but infection risk enhancing). Aims: The current study aimed to test the relationships between optimistic bias', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 346}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8982f40116bdbe83f3857a64b1ecfc68'}>,\n",
              "  <Document: {'content': 'and membership of multiple groups upon EID-related emotional and psychological responses and behavioral intentions. Methods: Participants from the UK and US ( N\\\\u00a0=\\\\u00a0360) rated how they would evaluate and respond to a fictitious EID immediately before the 2020 COVID-19 lockdowns in a correlational study. Results: Negative relationships were observed between optimistic bias and perceived infection vulnerability, infection prevention strategies, and perceived EID severity. Multiple group membership correlated negatively with germ avoidance, but positively with emotional responses such as disgust and increased perceived vulnerability to infection \\\\u2013 factors linked to avoiding infection. Multiple group memberships and optimistic bias were unrelated. Limitations: The study focussed on a fictitious disease and relies on cross-sectional data and behavioral intentions. Conclusions: These findings build upon the small evidence base on the role of optimistic bias in EID management and suggest that multiple group membership is unlikely to increase optimistic bias. The theoretical and practical implications of the findings for EID management are discussed.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 347}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fb3e346c3c7614cd04a10c97b069567c'}>,\n",
              "  <Document: {'content': '                       \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The findings build upon the small evidence base on the role of optimistic bias in EID management and suggest that multiple group membership is unlikely to increase optimistic bias.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"6528be9ddd8326451e0126c712a64073e085177a\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"The influence of group membership on online expressions', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 348}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1543ed9762897bc3f9a16232ee9625ec'}>,\n",
              "  <Document: {'content': 'and polarization on a discussion platform: An experimental study\",\\n                    \"abstract\": \"Despite much attention for group polarization in online environments, little is known about how group membership affects online behavior. We designed an online platform where ethnic minority and majority users in the Netherlands participated in discussions about controversial topics (homosexuality and abortion). Participants were randomly assigned to either progressive, conservative, or mixed discussions on these topics, which were ostensibly held among ethnic minority or majority users. We find that when ethnic minority users are exposed to discussions among the ethnic majority (i.e., outgroup) with which they disagree, they are less likely to express their opinions and more likely to deviate from their personal opinions. Among ethnic majority users, we find the opposite: when confronted with a discussion among the ethnic minority with which they disagree, they are more likely to voice their opinion and less likely to deviate from their personal opinions. This shows that group membership can affect online polarization.\",\\n                    \"openAccessPdf\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 349}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '81b33e2be30c27dcf57166717544706b'}>,\n",
              "  <Document: {'content': '{\\n                        \"url\": \"https://journals.sagepub.com/doi/pdf/10.1177/14614448231172966\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": null\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 3,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 350}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be769a5b504a7165c35e6a644375cb4b'}>,\n",
              "  <Document: {'content': '          \"paperId\": \"a5f84b9db1032bf3f4157cebfed5d6bbcf5f1ec3\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"8dce23a9-44e0-4381-a39e-2acc1edff700\",\\n                        \"name\": \"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"International ACM SIGIR Conference on Research and Development in Information Retrieval\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 351}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe3a2b136da1071d4d9fd915b6958e1d'}>,\n",
              "  <Document: {'content': '         \"Int ACM SIGIR Conf Res Dev Inf Retr\",\\n                            \"SIGIR\",\\n                            \"Annu Int ACM SIGIR Conf Res Dev Inf Retr\"\\n                        ],\\n                        \"url\": \"http://www.acm.org/sigir/\"\\n                    },\\n                    \"title\": \"SIGIR 2023 Workshop on Retrieval Enhanced Machine Learning (REML @ SIGIR 2023)\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 352}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '66e2232d0854e543e4c346179d7559aa'}>,\n",
              "  <Document: {'content': ' \"abstract\": \"Most machine learning models are designed to be self-contained and encode both \\\\\"knowledge\\\\\" and \\\\\"reasoning\\\\\" in their parameters. However, such models cannot perform effectively for tasks that require knowledge grounding and tasks that deal with non-stationary data, such as news and social media. Besides, these models often require huge number of parameters to encode all the required knowledge. These issues can be addressed via augmentation with a retrieval model. This category of machine learning models, which is called Retrieval-enhanced machine learning (REML), has recently attracted considerable attention in multiple research communities. For instance, REML models have been studied in the context of open-domain question answering, fact verification, and dialogue systems and also in the context of generalization through memorization in language models and memory networks. We believe that the information retrieval community can significantly contribute to this growing research area by designing, implementing, analyzing, and evaluating various aspects of retrieval models with applications to REML tasks. The goal of this full-day hybrid workshop is to bring together researchers from industry and academia to discuss various aspects of retrieval-enhanced machine learning, including effectiveness, efficiency, and robustness of these models in addition to their impact on real-world applications.\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 353}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bc5abc048c6881bae721f695fe771bde'}>,\n",
              "  <Document: {'content': '                 \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The goal of this full-day hybrid workshop is to bring together researchers from industry and academia to discuss various aspects of retrieval-enhanced machine learning, including effectiveness, efficiency, and robustness of these models in addition to their impact on real-world applications.\"\\n                    }\\n                },\\n                {\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 354}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '249c8907c02cd3e1f11b000314ed0d86'}>,\n",
              "  <Document: {'content': '\"paperId\": \"2e45ce60d4717d4283e3b462c9cdc940ced0f5c6\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"8dce23a9-44e0-4381-a39e-2acc1edff700\",\\n                        \"name\": \"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"International ACM SIGIR Conference on Research and Development in Information Retrieval\",\\n                            \"Int', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 355}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '336a476f079d2dd042d33cf6ce72baa7'}>,\n",
              "  <Document: {'content': 'ACM SIGIR Conf Res Dev Inf Retr\",\\n                            \"SIGIR\",\\n                            \"Annu Int ACM SIGIR Conf Res Dev Inf Retr\"\\n                        ],\\n                        \"url\": \"http://www.acm.org/sigir/\"\\n                    },\\n                    \"title\": \"ReNeuIR at SIGIR 2023: The Second Workshop on Reaching Efficiency in Neural Information Retrieval\",\\n                    \"abstract\": \"Multifaceted, empirical evaluation of algorithmic ideas', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 356}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a8712dbd95230bac0c41b2756e55e4a3'}>,\n",
              "  <Document: {'content': 'is one of the central pillars of Information Retrieval (IR) research. The IR community has a rich history of studying the effectiveness of indexes, retrieval algorithms, and complex machine learning rankers and, at the same time, quantifying their computational costs, from creation and training to application and inference. As the community moves towards even more complex deep learning models, questions on efficiency have once again become relevant with renewed urgency. Indeed, efficiency is no longer limited to time and space; instead it has found new, challenging dimensions that stretch to resource-, sample- and energy-efficiency with ramifications for researchers, users, and the environment alike. Examining algorithms and models through the lens of holistic efficiency requires the establishment of standards and principles, from defining relevant concepts, to designing metrics, to creating guidelines for making sense of the significance of new findings. The second iteration of the ReNeuIR workshop aims to bring the community together to debate these questions, with the express purpose of moving towards a common benchmarking framework for efficiency.\",\\n                    \"openAccessPdf\": null,\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 357}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e1a26d3d894763548af420e0580f5f4b'}>,\n",
              "  <Document: {'content': '         \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The second iteration of the ReNeuIR workshop aims to bring the community together to debate questions on efficiency, with the express purpose of moving towards a common benchmarking framework for efficiency.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"12bd4caf2703b40571cb94a9a5edaa4a87eb39a6\",\\n                    \"publicationVenue\": {\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 358}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bbf736f9672f53ba82b8cddda17a610f'}>,\n",
              "  <Document: {'content': '        \"id\": \"8dce23a9-44e0-4381-a39e-2acc1edff700\",\\n                        \"name\": \"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"International ACM SIGIR Conference on Research and Development in Information Retrieval\",\\n                            \"Int ACM SIGIR Conf Res Dev Inf Retr\",\\n                            \"SIGIR\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 359}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8aa1b07d32d9a8d6ff1304a0575be8e6'}>,\n",
              "  <Document: {'content': '                        \"Annu Int ACM SIGIR Conf Res Dev Inf Retr\"\\n                        ],\\n                        \"url\": \"http://www.acm.org/sigir/\"\\n                    },\\n                    \"title\": \"Gen-IR@SIGIR 2023: The First Workshop on Generative Information Retrieval\",\\n                    \"abstract\": \"Generative information retrieval (IR) has experienced substantial growth across multiple research communities (e.g., information retrieval, computer vision, natural language processing, and machine learning), and has been highly visible in the popular press. Theoretical, empirical, and actual user-facing products have been released that retrieve documents (via generation) or directly generate', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 360}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7326687353bbb754acaa6cfa49398abc'}>,\n",
              "  <Document: {'content': 'answers given an input request. We would like to investigate whether end-to-end generative models are just another trend or, as some claim, a paradigm change for IR. This necessitates new metrics, theoretical grounding, evaluation methods, task definitions, models, user interfaces, etc. The goal of this workshop1 is to focus on previously explored Generative IR techniques like document retrieval and direct Grounded Answer Generation, while also offering a venue for the discussion and exploration of how Generative IR can be applied to new domains like recommendation systems, summarization, etc. The format of the workshop is interactive, including roundtable and keynote sessions and tends to avoid the one-sided dialogue of a mini-conference.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2306.02887\",\\n                        \"status\": \"GREEN\"\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 361}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9b1c8f99ef77c090db3482323c576f1'}>,\n",
              "  <Document: {'content': '},\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is investigated whether end-to-end generative models are just another trend or, as some claim, a paradigm change for IR, which necessitates new metrics, theoretical grounding, evaluation methods, task definitions, models, user interfaces, etc.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 3,\\n            \"offset\": 0,\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 362}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6600d21e3d311b695af43940f37bdc6a'}>,\n",
              "  <Document: {'content': '      \"data\": [\\n                {\\n                    \"paperId\": \"c1f4b2b56ac6135ecd4f05a44d7725f6739e7742\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Recall, Robustness, and Lexicographic Evaluation\",\\n                    \"abstract\": \"Researchers use recall to evaluate rankings across a variety of retrieval, recommendation, and machine learning tasks. While there is a colloquial interpretation of recall in set-based evaluation, the research community is far from a principled understanding of recall metrics for rankings. The lack of principled understanding of or motivation for recall has resulted in criticism amongst the retrieval community that recall is useful as a measure at all. In this light, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 363}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c5938b3b51f86e5f1823256125ca5932'}>,\n",
              "  <Document: {'content': 'three tenets: recall, robustness, and lexicographic evaluation. First, we formally define `recall-orientation\\' as sensitivity to movement of the bottom-ranked relevant item. Second, we analyze our concept of recall orientation from the perspective of robustness with respect to possible searchers and content providers. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across 17 TREC tracks, we establish that our new evaluation method, lexirecall, is correlated with existing recall metrics and exhibits substantially higher discriminative power and stability in the presence of missing labels. Our conceptual, theoretical, and empirical analysis substantially deepens our understanding of recall and motivates its adoption through connections to robustness and fairness.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 364}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e99a43db893e8bbaf5f97b96b400e7b4'}>,\n",
              "  <Document: {'content': '           \"text\": \"Through extensive empirical analysis across 17 TREC tracks, it is established that the new evaluation method, lexirecall, is correlated with existing recall metrics and exhibits substantially higher discriminative power and stability in the presence of missing labels.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"81e5478d8a6fb6d415098c61a45c8afad9e32c72\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 365}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '374184c694725d47315612881aedbaaf'}>,\n",
              "  <Document: {'content': '                \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Recall as a Measure of Ranking Robustness\",\\n                    \"abstract\": \"Researchers use recall to evaluate rankings across a variety of retrieval, recommendation, and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 366}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '783627b0359632c31765da9030ad1542'}>,\n",
              "  <Document: {'content': 'machine learning tasks. While there is a colloquial interpretation of recall in set-based evaluation, the research community is far from a principled understanding of recall metrics for rankings. The lack of principled understanding of or motivation for recall has resulted in criticism amongst the retrieval community that recall is useful as a measure at all. In this light, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation. First, we formally define \\\\u2018recall-orientation\\\\u2019 as sensitivity to movement of the bottom-ranked relevant item. Second, we analyze our concept of recall orientation from the perspective of robustness with respect to possible searchers and content providers. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across 17 TREC tracks, we establish that our new evaluation method, lexirecall, is correlated with existing recall metrics and exhibits substantially higher discriminative power and stability in the presence of missing labels. Our conceptual, theoretical, and empirical analysis substantially deepens our understanding of recall and motivates its adoption through connections to robustness and fairness.\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 367}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7cfefbc90795d23267b6279bc53f4292'}>,\n",
              "  <Document: {'content': '                \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2302.11370\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Through extensive empirical analysis across 17 TREC tracks, it is established that the new evaluation method, lexirecall, is correlated with existing recall metrics and exhibits substantially higher discriminative power and stability in the presence of missing labels.\"\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 368}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff12598517cdace9870900b758be1d75'}>,\n",
              "  <Document: {'content': '              }\\n                },\\n                {\\n                    \"paperId\": \"55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 369}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd89e2f957133b88f7d95aad70eedb61'}>,\n",
              "  <Document: {'content': '               ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision\",\\n                    \"abstract\": \"Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 370}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '266fa48fa51104fce766ee2fab2bd4fa'}>,\n",
              "  <Document: {'content': 'of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.07908\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 371}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74ecae98e391260fcb5ac68d6ce4c18b'}>,\n",
              "  <Document: {'content': '                  \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work addresses the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements.\"\\n                    }\\n                }\\n            ]\\n        }\\n    ],\\n    \"Scott Fahlman\": [\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 372}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3609cae585d6b99ab5e0daa02127e88'}>,\n",
              "  <Document: {'content': ' {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"13922d438c437cea443b6c4747c54a29a8bdd742\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 373}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '573ab56981d36555ac9f8bad0c6d0144'}>,\n",
              "  <Document: {'content': ' \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Score: A Rule Engine for the Scone Knowledge Base System\",\\n                    \"abstract\": \"We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 374}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '98e42fa2195d43845d941373700db367'}>,\n",
              "  <Document: {'content': 'about the relationships between different elements efficiently. On its own, Scone acts as a sort of\\\\\"smart memory\\\\\"that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone\\'s knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of\\\\\"if-then\\\\\"production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.\",\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 375}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e19dff4dc28d55e5e5d3c9cd5ab29bcc'}>,\n",
              "  <Document: {'content': '          \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.04154\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The Scone system is augmented with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone\\'s knowledge base, potentially improving the capabilities of any planning systems built on top of Scone.\"\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 376}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '903c30d570188c78077fc5c900c41586'}>,\n",
              "  <Document: {'content': '         }\\n                }\\n            ]\\n        }\\n    ],\\n    \"Robert Frederking\": [\\n        {\\n            \"total\": 4360,\\n            \"offset\": 0,\\n            \"next\": 20,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"42f711ca3491d4bf33b35683944d9b8f5bc1c558\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"7ed174f9-4aba-446f-afdf-651294b78e9a\",\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 377}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b36574853ffe72f92e6dd22b2da4e8a'}>,\n",
              "  <Document: {'content': '                      \"name\": \"International Journal of Impact Engineering\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Int J Impact Eng\"\\n                        ],\\n                        \"issn\": \"0734-743X\",\\n                        \"url\": \"https://www.journals.elsevier.com/international-journal-of-impact-engineering\",\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 378}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '215c8e6d8169fdec5d373fd21dff2353'}>,\n",
              "  <Document: {'content': '      \"alternate_urls\": [\\n                            \"http://www.sciencedirect.com/science/journal/0734743X\"\\n                        ]\\n                    },\\n                    \"title\": \"Reconstructing ice force-displacement development in structural assessments of freshwater, polycrystalline ice impacts\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 379}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fa4e8c15c8e4e7896220c35388974466'}>,\n",
              "  <Document: {'content': '      {\\n                    \"paperId\": \"16e0752e50472b0f0ba574587d103b57c6cd8dd2\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"f8a6b881-bd04-4270-932d-823ead312d8c\",\\n                        \"name\": \"International Journal of Offshore and Polar Engineering\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Int J Offshore Polar Eng\"\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 380}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4c7b35f2e9909c2dab63c66bb85214f'}>,\n",
              "  <Document: {'content': '          ],\\n                        \"issn\": \"1053-5381\",\\n                        \"url\": \"https://www.onepetro.org/journals/International%20Journal%20of%20Offshore%20and%20Polar%20Engineering\"\\n                    },\\n                    \"title\": \"Ice Forces on the R. V. Polarstern During 1984 Labrador Trials\",\\n                    \"abstract\": \"The R. V. Polarstern carried out ice breaking trials in which ice forces were measured with special panels at two locations in the bow. Time series records of the ice forces for a 13-minute period were compared for differences in terms of magnitude and duration of loading for the two locations. Maximum force of 2800 kN was measured on a 1 m 2 panel. The nature of ice', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 381}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b34c1ac4dd327c695a811546fda7ff63'}>,\n",
              "  <Document: {'content': 'forces measured on the panels was very different in terms of frequency and duration compared to those measured with strain-gauged frames on the R. V. Polarstern and the CCGS Louis S. St-Laurent .\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"5f9ba33ccd5f31f8b92b286e8f6cab08bc2abba4\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"THE QUENCH TEST WITH PERFORATED PLATES USING CRYOLIQUID AT \\\\\"1g\\\\\" AND AT \\\\\"micro-g\\\\\"\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 382}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2c3a2a4c68ebab2e7bf4ae6991575c8a'}>,\n",
              "  <Document: {'content': ' \"abstract\": null,\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"1e5dab60e731a156867262588a9f5ebbcfb6fcdf\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"The Brain of Robert Frost\",\\n                    \"abstract\": \"TH HIS TITLE may suggest that I have deliberately chosen a small subject, prompted to mercy perhaps by the weighty burden of some of these cogitations on creation and interpretation. Mercy, however, was not my motive.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 383}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2da1e45554cce0f0e4114ba82b0d504b'}>,\n",
              "  <Document: {'content': 'I have three points in mind. First, I want to show how Robert Frost\\'s interpretations share quite exactly the general style of his writing. In other words, I think one salient fact about the relation between creation and interpretation is that they embody the same personal style. Second, based on that commonality of style, I want to propose a picture or metaphor or guiding principle that will enable us to put together Robert Frost, unique creator, with Robert Frost, a reader of poems like any other reader of poems. I want to suggest that one could refine that general picture to a model, even an electronic model. We could use such a model-and this is my third point-to understand the relation between the individuality that pervades both creation and interpretation and the social and interpretive codes by which we collectively interpret. We could use the model to interrelate Robert Frost the individual with, say, the codes addressed by a semiotician like Umberto Eco, or the \\\\\"interpretive communities\\\\\" of which Stanley Fish writes, or the \\\\\"horizon of expectations\\\\\" described by a Rezeptionsdsthetiker like Hans Robert Jauss.\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 384}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c4fff1db3566899ac3b0ec609445cf3c'}>,\n",
              "  <Document: {'content': '    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"19b3593be9e77c79998e23b90883d43def545a99\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Mental Health Surveillance am Robert Koch-Institut \\\\u2013 Strategien zur Beobachtung der psychischen Gesundheit der Bev\\\\u00f6lkerung\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 385}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bc8362fda716b82514d4fc6b211dbf56'}>,\n",
              "  <Document: {'content': ' \"url\": \"https://link.springer.com/content/pdf/10.1007/s00103-023-03678-4.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The further development and long-term operation of the Mental Health Surveillance as a\\\\u00a0whole has the potential to facilitate the achievement of public mental health objectives and to contribute on different levels to the improvement of population health.\"\\n                    }\\n                },\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 386}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b884a181606a1e0b942bd4d4f04776a'}>,\n",
              "  <Document: {'content': '      {\\n                    \"paperId\": \"228eb3e5675af1fcf4c1e3f47adad1bcc230eba2\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"65addddf-a83f-45af-a7a7-81500611347b\",\\n                        \"name\": \"Ethik in der Medizin\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Ethik Med\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 387}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '424b87a627c84d3cda09447fdb273b62'}>,\n",
              "  <Document: {'content': '        \"Ethik in Der Medizin\"\\n                        ],\\n                        \"issn\": \"0935-7335\",\\n                        \"url\": \"https://www.springer.com/medicine/journal/481\",\\n                        \"alternate_urls\": [\\n                            \"https://link.springer.com/journal/481\"\\n                        ]\\n                    },\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 388}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e5bde7b7ff0c900297f0abf17533beba'}>,\n",
              "  <Document: {'content': '  \"title\": \"Robert Ranisch (2021) Liberale Eugenik? Kritik der selektiven Reproduktion\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"ea5ab94ca3d023b3e07ce16a7dcc14f4bff35120\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"87ec5cae-55ea-4b52-a5b0-eef88a7d61b5\",\\n                        \"name\": \"Journal of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 389}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eee1f914f186506263d5abca6c799318'}>,\n",
              "  <Document: {'content': 'the History of Economic Thought\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"J Hist Econ Thought\",\\n                            \"Journal of The History of Economic Thought\"\\n                        ],\\n                        \"issn\": \"1053-8372\",\\n                        \"alternate_issns\": [\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 390}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ebe43e55212646c7bb56da16d0d77a7'}>,\n",
              "  <Document: {'content': '                     \"1042-7716\"\\n                        ],\\n                        \"url\": \"http://www.informaworld.com/1474-449X\",\\n                        \"alternate_urls\": [\\n                            \"http://journals.cambridge.org/jid_HET\",\\n                            \"http://journals.cambridge.org/action/displayBackIssues?jid=HET\",\\n                            \"http://journals.cambridge.org/action/displayJournal?jid=HET\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 391}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d0e108725b3eefade51669dd65b596a'}>,\n",
              "  <Document: {'content': '       \"http://www.metapress.com/openurl.asp?genre=journal&issn=1042-7716\",\\n                            \"https://www.cambridge.org/core/journals/journal-of-the-history-of-economic-thought\",\\n                            \"http://journals.cambridge.org/HET\"\\n                        ]\\n                    },\\n                    \"title\": \"ROBERT TRIFFIN, JAPAN, AND THE QUEST FOR ASIAN MONETARY UNION\",\\n                    \"abstract\": \"Especially with the Asian financial crisis of 1997\\\\u201398, Asian countries have advocated a profound reform of the international financial architecture. Their proposals focused on two main axes: a reform of the global financial system, and stronger regional monetary integration in Asia. There', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 392}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '470fb5b7296dae97a8c90b7129d5805'}>,\n",
              "  <Document: {'content': 'are here significant parallels with the ideas of Robert Triffin (1911\\\\u20131993). Triffin became famous with trenchant analyses of the vulnerabilities of the international monetary system. The Triffin dilemma is still present among international monetary policy-makers, also in Asia. Triffin put forward several proposals for reforming the global monetary system, but he also developed proposals for regional monetary integration. These were very much based on his experience with the European Payments Union, and focused on the creation of a (European) reserve fund and a (European) currency unit. In this paper we focus on Triffin\\\\u2019s proposals for an Asian payments union in the late 1960s, giving special attention to Japan (in Triffin\\\\u2019s time, the biggest Asian economy; moreover, Triffin had an important Japanese network).\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.econstor.eu/bitstream/10419/256816/1/wp405en.pdf\",\\n                        \"status\": \"GREEN\"\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 393}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7dfe89632de7926b374e9ad97afda87b'}>,\n",
              "  <Document: {'content': '            },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"384eb97a3c6274a7901bbf36c88dab405d9d7c19\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1b327d11-8b14-4a7b-8e4f-f9100556e8a4\",\\n                        \"name\": \"Mediterranea International Journal on the Transfer of Knowledge\",\\n                        \"type\": \"journal\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 394}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2e597b351cc5fed02236f6f6d1b48537'}>,\n",
              "  <Document: {'content': '             \"alternate_names\": [\\n                            \"Mediterr Int J Transf Knowl\"\\n                        ],\\n                        \"issn\": \"2445-2378\",\\n                        \"url\": \"http://www.uco.es/servicios/ucopress/ojs/index.php/mediterranea/index\",\\n                        \"alternate_urls\": [\\n                            \"http://www.uco.es/ucopress/ojs/index.php/mediterranea\",\\n                          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 395}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd427c7b70698b4f33648be00419d96e4'}>,\n",
              "  <Document: {'content': ' \"http://www.uco.es/ucopress/ojs/index.php/mediterranea/index\",\\n                            \"https://dialnet.unirioja.es/servlet/revista?codigo=24213\"\\n                        ]\\n                    },\\n                    \"title\": \"Robert Grosseteste\\\\u2019s Translation of Simplicius\\\\u2019s Commentary on Aristotle\\\\u2019s De caelo: Tracking down a Second Manuscript and the Greek Model\",\\n                    \"abstract\": \"The note surveys the reception history of Robert Grosseteste\\'s Latin translation of Aristotle\\'s De Caelo and of Simplicius\\'s commentary on the same treatise. It presents the analysis of previously unnoticed fragments from a second manuscript of the translation. Their discovery necessitates the revision of earlier ideas about the limited dissemination of the text. The note also confirms a neglected hypothesis about the Greek model that Grosseteste used', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 396}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6f62021818e1b3bd759d226a5a003f0'}>,\n",
              "  <Document: {'content': 'for his translation. A late-15th-century manuscript must be considered a direct copy of Grosseteste\\'s lost Greek codex.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"4cf174f0c6b254fe32639aa0aeffa1edba715aa5\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"6856fb47-155a-4ee0-be9a-a867c8948097\",\\n                        \"name\": \"Review of African Political Economy\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 397}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'baef6184a88c59dd11bf5d28bc237e6'}>,\n",
              "  <Document: {'content': '          \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Rev Afr Political Econ\"\\n                        ],\\n                        \"issn\": \"0305-6244\",\\n                        \"url\": \"http://www.tandfonline.com/loi/crea20\",\\n                        \"alternate_urls\": [\\n                            \"http://www.roape.org/\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 398}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e08b70072f660bb0e11a6991d992c482'}>,\n",
              "  <Document: {'content': '                      \"http://www.tandfonline.com/toc/crea20/current\"\\n                        ]\\n                    },\\n                    \"title\": \"Generational populism and the political rise of Robert Kyagulanyi \\\\u2013 aka Bobi Wine \\\\u2013 in Uganda\",\\n                    \"abstract\": \"ABSTRACT This article analyses the political rise of the Ugandan opposition leader, Robert Kyagulanyi, aka Bobi Wine, arguing that he has a deployed a novel type of generational populism \\\\u2013 a mobilising political discourse which frames the struggle between \\\\u2018the people\\\\u2019 and \\\\u2018the elite\\\\u2019 in generational terms, defining the former in relation to their status as youth, and in antagonistic opposition to an elite, which is depicted as defending a gerontocratic political order. At a theoretical level,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 399}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '33509ce6309621952a1130908b15fc59'}>,\n",
              "  <Document: {'content': 'the article broadens political science\\\\u2019s conception of populism, by introducing a new subtype of the political phenomenon which demonstrates the importance of intergenerational dynamics in the construction of the discursive categories of \\\\u2018the people\\\\u2019 and \\\\u2018the elite\\\\u2019. While it argues that Kyagulanyi\\\\u2019s success demonstrates the potential of populism in African countries to electorally challenge incumbent regimes, by helping to build political coalitions across ethno-regional lines, incorporating previously excluded social groups into the political process, it concludes by stressing that Kyagulanyi\\\\u2019s political project has failed to offer any real ideological alternative to the neoliberal orthodoxy that has characterised President Museveni\\\\u2019s Uganda over the last four decades.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"b10a0915b14cce71aa132bdae84486f318f67e62\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 400}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a4e08a72d1f5589de719583c24b250e8'}>,\n",
              "  <Document: {'content': '                   \"publicationVenue\": null,\\n                    \"title\": \"MURPHREE FESTSCHRIFT: REVIEW Cell Free DNA (cfDNA) in the Blood of Retinoblastoma Patients The Robert M. Ellsworth Lecture\",\\n                    \"abstract\": \"Background: Cell-free DNA analysis in cancer has gone from research to widespread clinical use in the past 10 years. At Memorial Sloan Kettering Cancer Center, we developed a technology and test to assay cell-free DNA (cfDNA) from blood (plasma) in our retinoblastoma patients. Results: cfDNA derived from intraocular retinoblastoma can be measured and quantified in the blood (plasma) of patients. It is derived from the tumor cells themselves. Simulating lesions did not have cfDNA abnormalities. cfDNA disappears quickly after cutting the optic nerve (50% gone in 10 minutes) and if cfDNA is measurable after enucleation, metastases develop. Analysis of the buffy coat can detect germline defects including very low levels of mosaicism not detected with other NGS techniques. Analysis of the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 401}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e771065c66ba3006ac208be1b394fc6f'}>,\n",
              "  <Document: {'content': 'buffy coat also reveals non Rb1 germline predilections to second cancers. Conclusion: Analysis of cfDNA from blood of retinoblastoma patients can be used to diagnose and manage retinoblastoma and reflect an accurate molecular profile of RB1 abnormalities of the intraocular tumor. Analysis of the germline with the buffy coat detects very low levels of mosaicism not detected with conventional methods. Liquid biopsy for retinoblastoma is already in clinical use and offers information not available with any other technique. ARTICLE HISTORY Received September 16, 2021 Revised October 30, 2021 Accepted November 05, 2021\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Analysis of cfDNA from blood of retinoblastoma patients can be used to diagnose and manage retinOBlastoma and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 402}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '448a2a57184b8c91449f406fd8c1fda'}>,\n",
              "  <Document: {'content': 'reflect an accurate molecular profile of RB1 abnormalities of the intraocular tumor.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"3c77fa77587c27bae85ec8267131ab0b5c62f061\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"821a3bcf-ff02-4bd6-bddd-3d7290eb80ed\",\\n                        \"name\": \"Journal of British Studies\",\\n                        \"type\": \"journal\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 403}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d299139ab2713f25930397a11a3170e'}>,\n",
              "  <Document: {'content': '       \"alternate_names\": [\\n                            \"J Br Stud\"\\n                        ],\\n                        \"issn\": \"0021-9371\",\\n                        \"url\": \"http://journals.cambridge.org/action/displayJournal?jid=JBR\",\\n                        \"alternate_urls\": [\\n                            \"https://www.jstor.org/journal/jbritishstudies\"\\n                        ]\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 404}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '40a1dc3ac83688902df753713b937a53'}>,\n",
              "  <Document: {'content': '         },\\n                    \"title\": \"Sir Robert Cotton, Manuscript Pamphleteering, and the Making of Jacobean Kingship during the Short Peace, ca. 1609\\\\u20131613\",\\n                    \"abstract\": \"Abstract This article concerns two manuscript tracts by Sir Robert Cotton, the Answer to Certain Military Men regarding Foreign War (1609) and Twenty-Four Arguments on the Strict Execution of the Laws against Seminary Priests (1613). To the limited extent that these tracts have been studied at all, historians have read them as artifacts of the Jacobean regime\\'s internal counseling process. Through analysis of the both the structure of the Jacobean regime\\'s knowledge economy and the two tracts and contextualizing them, the author argues that these were, instead, innovative exercises in publicity, designed to defend existing Jacobean policy against so-called country criticism. Designed to circulate widely among the kingdom\\'s social elite\\\\u2014indeed, more than two dozen handwritten copies of each tract survive\\\\u2014the manuscript pamphlets played on Cotton\\'s reputation as an antiquary to legitimize the Jacobean', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 405}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a5511218dec515490315b2585a745f1c'}>,\n",
              "  <Document: {'content': 'regime\\'s most controversial policies. More broadly, the tracts demonstrate the dilemma of a Jacobean regime caught between the geopolitics of peace and interconfessional diplomacy and the expectations of a domestic political elite nurtured on the values and expectations of confessional war.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.cambridge.org/core/services/aop-cambridge-core/content/view/8C4C25047E70C5FE8A5C4F5450024FAE/S0021937122001757a.pdf/div-class-title-sir-robert-cotton-manuscript-pamphleteering-and-the-making-of-jacobean-kingship-during-the-short-peace-ca-1609-1613-div.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": null\\n                },\\n                {\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 406}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74b0f4fe85e3477fb53ccec90a921d28'}>,\n",
              "  <Document: {'content': '    \"paperId\": \"4b003ae532ffc2093544e34e3629efb1aad5f34c\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"From Life to Survival: Derrida, Freud, and the Future of Deconstruction by Robert Trumbull (review)\",\\n                    \"abstract\": \"Life Death (la vie la mort, inextricably intertwined, and unhyphenated) and survival (la survivance) end up as Derrida\\\\u2019s most succinctly phrased reformulations of Freud\\\\u2019s own attempts to theorize an elusive but essential element beyond the positive forces of the Lustprinzip, which he most famously identified as a \\\\u2018death drive\\\\u2019 (Todestrieb) in Beyond the Pleasure Principle. Robert Trumbull\\\\u2019s study in effect repositions Derrida as an under-appreciated \\\\u2018biophilosopher\\\\u2019 and, through some very careful and well-informed close reading, traces with impressive precision and attention to detail the arc of Derrida\\\\u2019s engagement with Freud, from his early essay in Writing and Difference on Freud and the mystic writing pad as metaphor for psychic representation, through Derrida\\\\u2019s magisterial The Post Card, to texts', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 407}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a4e0c24f37e6e07bd93fee44b1ad8c36'}>,\n",
              "  <Document: {'content': 'such as Archive Fever and Resistances of Psychoanalysis, and those which map onto more overt political or ethical questions, such as Specters of Marx or Rogues. Trumbull gives an emphatic account of the characteristic deconstructive move of undoing established metaphysical concepts and oppositions, and identifying a more radically disruptive term which also functions as a condition of possibility (arche-writing, the trace, diff\\\\u00e9rance, spectrality, and so on). In the case of Derrida as a reader of Freud, autoimmunity takes centre stage in the latter half of the study, which allows Derrida to move from its biological origin through to analyses of the heritage of Marxism, 9/11, rogue states, war, and other more political and ethical concerns in Derrida\\\\u2019s late and recently translated seminars on The Beast and the Sovereign, and The Death Penalty. Indeed, for die-hard Derrida fans, this book demonstrates how richly Derrida used this pedagogical space to continue to nuance and expand his thinking and range of references. Ironically, there is something of a fort-da game with Freud disappearing and reappearing at times, and this tends to compromise the overall shape of the book, and the clear lines of the argument. Other potential avenues might have expanded the understanding', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 408}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fb4999f664db4ea4d0a453b1d229d0a0'}>,\n",
              "  <Document: {'content': 'of the relationship between deconstruction and psychoanalysis (of which Freud really stands as the sole representative): interesting connections could have been made to aggressivity/ destructiveness as Jacques Lacan, or Melanie Klein, or others who theorize it. In reading Freud in such abstract terms, one also risks a lack of engagement with the basic therapeutic aims of psychoanalysis. That said, Trumbull\\\\u2019s readings of Derrida are deep, absorbing, and thorough, even if this thoroughness implies more restatement than is necessary, and even if the proximity to Derrida\\\\u2019s style results in a certain mimetism (down to the overuse of \\\\u2018certain\\\\u2019 as an adjective). One senses that if the chapters had moved faster, and less densely, the argument about the contemporary relevance and political value of deconstruction could have been made more fully and convincingly in the final chapter and conclusion. Nevertheless, and in the spirit of theoretical speculation in Beyond the Pleasure Principle and The Post Card, Trumbull opens up many intriguing avenues for deconstruction, particularly in relation to the late seminars, and points to how powerfully Derrida\\\\u2019s thinking still intervenes in the most urgent contemporary debates.\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 409}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22694a4533c2d69563dba5a91f6a32f4'}>,\n",
              "  <Document: {'content': '   \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"f45ae4ab7017ee924de010ca729f88aa155e1b16\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"84ebcc6c-19e7-483c-a064-41856b440d56\",\\n                        \"name\": \"Microbiology Resource Announcements\",\\n                        \"alternate_names\": [\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 410}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27738271921f3a7a090c794db63370c6'}>,\n",
              "  <Document: {'content': '    \"Microbiol Resour Announc\"\\n                        ],\\n                        \"issn\": \"2576-098X\",\\n                        \"url\": \"https://mra.asm.org/\",\\n                        \"alternate_urls\": [\\n                            \"https://mra.asm.org/content/about-mra\"\\n                        ]\\n                    },\\n                    \"title\": \"Genome Sequence', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 411}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ed7ed2478fa67f4b48239cd27345f651'}>,\n",
              "  <Document: {'content': 'of Pseudomonas sp. Strain So3.2b, Isolated from a Soil Sample from Robert Island (Antarctic Specially Protected Area 112), Antarctic\",\\n                    \"abstract\": \"Strain So3.2b of the genus Pseudomonas was isolated from a soil sample from Robert Island (Antarctic Specially Protected Area 112), Antarctic. We report the complete genome sequence of this isolate, with a length of 6.17 Mbp and a GC content of 60.5%. ABSTRACT Strain So3.2b of the genus Pseudomonas was isolated from a soil sample from Robert Island (Antarctic Specially Protected Area 112), Antarctic. We report the complete genome sequence of this isolate, with a length of 6.17 Mbp and a GC content of 60.5%.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 412}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8122a3b05ee1c593e20893aaa64856d7'}>,\n",
              "  <Document: {'content': '              \"text\": null\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"e8470ab506ae7ce41a21854971f994397954d648\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"ea1778be-1237-4179-b717-63a45f4d1ee5\",\\n                        \"name\": \"Journal for the History of Astronomy\",\\n                        \"type\": \"journal\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 413}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bd5095187657dc75c59b85cfa4fbb956'}>,\n",
              "  <Document: {'content': '             \"alternate_names\": [\\n                            \"J Hist Astron\"\\n                        ],\\n                        \"issn\": \"0021-8286\",\\n                        \"url\": \"https://journals.sagepub.com/home/jha\",\\n                        \"alternate_urls\": [\\n                            \"http://www.shpltd.co.uk/\",\\n                            \"http://www.shpltd.co.uk/jha.html\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 414}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd24c68bc4e56167a49fb28a920f9004'}>,\n",
              "  <Document: {'content': '                           \"http://esoads.eso.org/journals_service.html\",\\n                            \"http://www.uk.sagepub.com/journals/Journal202289\"\\n                        ]\\n                    },\\n                    \"title\": \"St. Albert the Great and Robert Grosseteste on the nature and causes of comets\",\\n                    \"abstract\": \"Addressing a subject which has received very little attention, this article explores the interpretations of comets offered by St. Albert the Great (c. 1190\\\\u20131280) and Robert Grosseteste (1168\\\\u20131253). It shows how, despite prima facie convergences between the two 13th-century bishops concerning the nature and causation of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 415}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ffe8089b11bb2732b97946c7e38352d'}>,\n",
              "  <Document: {'content': 'comets, there are nonetheless several previously unobserved subtle differences between them. For Grosseteste the celestial bodies (i.e. the stars and the planets) are the primary, and indeed sole, efficient causes of cometary phenomena, serving to draw up rarefied matter to the upper atmosphere whereupon it is inflamed as it is assimilated to the celestial nature itself. For Albert, by contrast, while the celestial bodies may help to stir up combustible vapours within the atmosphere, and at times precipitate their ascension to the heavenly vault by means of their motion and conjunction, it is not always the case that a comet arises as a result of the direct efficient causality of the celestial bodies.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://journals.sagepub.com/doi/pdf/10.1177/00218286231170596\",\\n                        \"status\": \"HYBRID\"\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 416}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8aa98844dd7d87f86e55d70fe8bb253f'}>,\n",
              "  <Document: {'content': '   },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"cb6ec7f06d268272bc656657111eda0a5ec257d2\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"c9539f54-8a3b-43ee-9247-57afbf77d5dc\",\\n                        \"name\": \"American Psychologist\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 417}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5770c3ce5be8d57fdab47ec62d15036c'}>,\n",
              "  <Document: {'content': '                           \"Am Psychol\"\\n                        ],\\n                        \"issn\": \"0003-066X\",\\n                        \"url\": \"http://www.apa.org/journals/amp/\",\\n                        \"alternate_urls\": [\\n                            \"http://www.apa.org/pubs/journals/amp/index.aspx\",\\n                            \"http://content.apa.org/journals/amp\"\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 418}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f8f54d45e0ffaac6510d553a85b15191'}>,\n",
              "  <Document: {'content': '       ]\\n                    },\\n                    \"title\": \"Standing on the shoulders of a giant: The legacy of Robert M. Sellers.\",\\n                    \"abstract\": \"Robert M. Sellers, PhD, most known for his influential and highly cited Multidimensional Model of Racial Identity (MMRI), is one of the most prolific and foundational Black scholars in psychology. From racial identity theory development and measurement to conceptual and methodological innovations in studying the lived experiences of Black people, Sellers\\' scholarship centers on the lives of Black communities. Sellers\\' mentorship and contributions to the professional development of scholars and professionals of color have supported and catalyzed new intergenerational knowledge building by these scholars, ensuring a perpetuating and far-reaching legacy in psychology. In this article, we: (a) celebrate Sellers\\' enduring contribution to the racial identity literature and its profound impact on psychology as a discipline as well as numerous', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 419}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c2fa23aefa8575c1936a5fee80baac17'}>,\n",
              "  <Document: {'content': 'subfields of psychology, (b) outline his contributions to the racial socialization literature, (c) describe methodological innovations in racial identity and racial socialization research advanced through his scholarship, and (d) summarize his contributions in professional development and mentorship and his leadership roles. Sellers\\' scholarly contributions and mentorship have transformed the discipline of psychology and the social sciences broadly speaking, making him one of the most influential psychologists in the modern era. (PsycInfo Database Record (c) 2023 APA, all rights reserved).\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"e507c79ca9d2749166fea2d69fa068d46e7b4689\",\\n                    \"publicationVenue\": {\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 420}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '48664e210070c2254eac6963d9c747d9'}>,\n",
              "  <Document: {'content': '                  \"id\": \"8a6faaef-eb92-4dcb-8f9b-065388aee977\",\\n                        \"name\": \"Religions\",\\n                        \"issn\": \"2077-1444\",\\n                        \"url\": \"http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-190623\",\\n                        \"alternate_urls\": [\\n                            \"https://www.mdpi.com/journal/religions\",\\n                            \"http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-190623\"\\n                        ]\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 421}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9b18182065e409f040d1dbaf7f6b751c'}>,\n",
              "  <Document: {'content': '                   },\\n                    \"title\": \"Robert Boyle, the Bible, and Natural Philosophy\",\\n                    \"abstract\": \"The great chemist Robert Boyle was also a serious student of the Bible and Christian theology, both of which profoundly influenced his natural philosophy. Christian beliefs and moral attitudes motivated him to extend human dominion over the creation by advancing scientific knowledge and giving medicines from his laboratory to the poor. His outspoken advocacy of empiricism, over and against those who believed that unaided reason was sufficient to probe the depths of nature, was rooted in the conviction that the free, wise, and powerful Creator knows the creation far better than we creatures ever will. He vigorously promoted what he called \\\\u201cthe mechanical philosophy\\\\u201d, partly because he found it far more theologically attractive than the pagan Greek conception taught in the universities, which conceived of \\\\u201cNature\\\\u201d as a semi-divine being with a mind', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 422}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4614de102737dc7b370a4a976c7775a4'}>,\n",
              "  <Document: {'content': 'and powers of its own. It also underscored the great complexity of the world machine, requiring an intelligent Creator to have assembled it\\\\u2014thereby (he hoped) moving people not only to acknowledge God but to live piously and humbly.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.mdpi.com/2077-1444/14/6/795/pdf?version=1687143301\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": null\\n                },\\n                {\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 423}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a07077a9b00d9e87fe1a7a971c1733ec'}>,\n",
              "  <Document: {'content': ' \"paperId\": \"28e29d435bc4d274844949beb24dd96f381d5b38\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"7fe0cf56-f8f7-4fce-b6e2-79bffd4c690e\",\\n                        \"name\": \"Notes and Records: the Royal Society journal of the history of science\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Notes and Records\",\\n                            \"Note Rec R Soc j hist sci\",\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 424}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c9f502d33c86b66d663f0111eaa09c7d'}>,\n",
              "  <Document: {'content': '                          \"Note Rec\"\\n                        ],\\n                        \"issn\": \"0035-9149\",\\n                        \"url\": \"https://www.jstor.org/journal/noterecoroyasoci\",\\n                        \"alternate_urls\": [\\n                            \"http://rsnr.royalsocietypublishing.org/\"\\n                        ]\\n                    },\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 425}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5e8872963b5c60dc039062c9eedf33c6'}>,\n",
              "  <Document: {'content': '                  \"title\": \"The cells of Robert Hooke: pores, fibres, diaphragms and the cell theory that wasn\\'t\",\\n                    \"abstract\": \"The early microscopist Robert Hooke (1653\\\\u20131703) is commonly credited with the discovery and naming of biological cells in the course of his studies of plant tissues. Surprisingly, the theoretical context of this apparent discovery is rarely evaluated when Hooke\\'s contribution to the development of modern biology is discussed. Hooke worked within the conceptual framework of the developing fibre doctrine, and consequently interpreted plant and animal structures as solid yet porous materials that directed and regulated the movements of fluids. The strength of his theory-derived expectations is exemplified by his postulate of valve-like passages in plant cell walls despite his admitted inability to detect any. Neglecting Hooke\\'s theoretical background, modern commentators regularly misread important parts of his anatomical works. This shows, for instance, in the common assertion that Hooke used pore and cell interchangeably when in fact they represented the whole and its part, or in the claim', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 426}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9d5daee5fef79ba50a93918348951fca'}>,\n",
              "  <Document: {'content': 'that his cells were closed structures. Here I present a reconstruction of what Hooke and contemporary scholars meant when they spoke of cells in plant materials, namely elements of continuous pipes for fluid transport, and evaluate alternative interpretations.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A reconstruction of what Hooke and contemporary scholars meant when they spoke of cells in plant materials, namely elements of continuous pipes for fluid transport, is presented and alternative interpretations are evaluated.\"\\n                    }\\n                },\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 427}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ed0af6ddb10528ec65c4a0f09eaa016'}>,\n",
              "  <Document: {'content': '            {\\n                    \"paperId\": \"37a539d272c03ee5696cfa332b0a9babe2b3909d\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"c0bebe0b-f1ec-4e92-9696-f33b013b9477\",\\n                        \"name\": \"Verbum et Ecclesia\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Verbum Eccles\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 428}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5091b0a70634747d1d9ebc567a1f6767'}>,\n",
              "  <Document: {'content': '             \"Verbum Et Ecclesia\"\\n                        ],\\n                        \"issn\": \"1609-9982\",\\n                        \"url\": \"http://www.scielo.org.za/scielo.php?lng=en&nrm=iso&pid=2074-7705&script=sci_serial\",\\n                        \"alternate_urls\": [\\n                            \"http://www.ve.org.za/\",\\n                            \"http://www.ve.org.za/index.php/VE\",\\n                            \"http://www.scielo.org.za/scielo.php?lng=en&pid=2074-7705&script=sci_serial\"\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 429}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd352b414d42624b0fc86cc5a3f24b8e'}>,\n",
              "  <Document: {'content': '                      ]\\n                    },\\n                    \"title\": \"\\\\u2018Robert\\\\u2019s Rules of Order\\\\u2019 on Religious Conflicts in the Church of Christ in Zimbabwe\",\\n                    \"abstract\": \"has tactical, technical and strategic management principles inherited from the restoration history, which can apply in conflict resolution. Using scientific methods of handling conflicts like RONR, and the COCZ principles can hopefully lead to better results in that church. Contribution: The paper contributes to alternatives to conflict resolutions which have been tested within the Church of Christ. The Robert\\\\u2019s Rules can be used in different religious organisations to resolve conflict through peaceful resolutions of difference.\",\\n                    \"openAccessPdf\": {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 430}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f7b56e52ce561e3d30ea70ebf0d2c701'}>,\n",
              "  <Document: {'content': '                \"url\": \"https://verbumetecclesia.org.za/index.php/ve/article/download/2666/6285\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"e1714a6d19b20b2e6e3bda20931abb27444798b0\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Robert Koch\\\\u2019s History as Reflected in Philately and Philocarty\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 431}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'aa8557cceec0c1a350d34e466e2a10b7'}>,\n",
              "  <Document: {'content': '      \"abstract\": \"The article presents research materials on the issues of reflection in the means philately and philocarty of the causative agent of tuberculosis \\\\u2013 of a German bacteriologist and a world-famous scientist \\\\u2013 Robert Koch - Koch sticks or Mycobacterium tuberculosis.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://doi.org/10.33140/jtma.02.01.01\",\\n                        \"status\": \"BRONZE\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 432}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '60d3ae010efe81ab30b4694e1f261533'}>,\n",
              "  <Document: {'content': '       \"text\": null\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"2f7cef7a3958061ded6d458b88457388770c99e1\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"de16002a-77c3-4d03-9348-5a1813238e0b\",\\n                        \"name\": \"Cancer Discovery\",\\n                        \"type\": \"journal\",\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 433}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5f1037ed7163d4636f6d3e066661df49'}>,\n",
              "  <Document: {'content': '  \"alternate_names\": [\\n                            \"Cancer Discov\"\\n                        ],\\n                        \"issn\": \"2159-8274\",\\n                        \"url\": \"https://cancerdiscovery.aacrjournals.org/\"\\n                    },\\n                    \"title\": \"Q&A: Robert Vonderheide on Immunotherapy Advances.\",\\n                    \"abstract\": \"Researchers and oncologists have long looked to the body\\'s own defenses as a possible tool to conquer cancer. To that end, they have developed checkpoint inhibitors, mRNA', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 434}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c100628ac7d153d36e9e5588a79c044'}>,\n",
              "  <Document: {'content': 'vaccines, chimeric antigen receptor T-cell therapies, and other treatments. Robert Vonderheide, MD, DPhil, director of the Abramson Cancer Center at the University of Pennsylvania in Philadelphia, talks about where the field has been and what lies ahead.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Robert Vonderheide, MD, DPhil, director of the Abramson Cancer Center at the University of Pennsylvania in Philadelphia, talks about where the field has been and what lies ahead.\"\\n                    }\\n                }\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 435}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '218fb9f14712690e4601dee4f68042e3'}>,\n",
              "  <Document: {'content': '   ]\\n        }\\n    ],\\n    \"Daniel Fried\": [\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"3e4085e5869f1b7959707a1e1d7d273b6057eb4e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 436}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd70e35e78afc358e79bc79b3f0df014c'}>,\n",
              "  <Document: {'content': '\"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"StarCoder: may the source be with you!\",\\n                    \"abstract\": \"The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 437}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '88a65f90fa3e6148a32a374d7f89eb82'}>,\n",
              "  <Document: {'content': 'length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\\\\\\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.06161\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 438}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5e9b344a919565a1f4cc6a3688e846f9'}>,\n",
              "  <Document: {'content': '                       \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work performs the most comprehensive evaluation of Code LLMs to date and shows that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model.\"\\n                    }\\n                }\\n            ]\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 439}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e1d7aeabe7077c88d4113943120aa8df'}>,\n",
              "  <Document: {'content': '    },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"6173520a1eb2814d067e8c5fd16212b7cbf6ee78\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fc0a208c-acb7-47dc-a0d4-af8190e21d29\",\\n                        \"name\": \"International Conference on Machine Learning\",\\n                        \"type\": \"conference\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 440}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb219fa4c533602a1e94d32f24560f64'}>,\n",
              "  <Document: {'content': '            \"alternate_names\": [\\n                            \"ICML\",\\n                            \"Int Conf Mach Learn\"\\n                        ],\\n                        \"url\": \"https://icml.cc/\"\\n                    },\\n                    \"title\": \"Grounding Language Models to Images for Multimodal Inputs and Outputs\",\\n                    \"abstract\": \"We propose an efficient method to ground pretrained', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 441}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e529c6f57bbcdd928899bcf5f0609085'}>,\n",
              "  <Document: {'content': 'text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 442}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '54dc42f9e9bf0a56d71291f9d16ac363'}>,\n",
              "  <Document: {'content': '                \"text\": null\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a\",\\n                    \"publicationVenue\": {\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 443}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e82ee922eade9b30406205396b0795e7'}>,\n",
              "  <Document: {'content': '  \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"SantaCoder: don\\'t reach for', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 444}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ebbf7fb20ce1554ea99b083ed059354d'}>,\n",
              "  <Document: {'content': 'the stars!\",\\n                    \"abstract\": \"The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.\",\\n                    \"openAccessPdf\": {\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 445}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1b1b1d11fb08ecdbc37eda5f5f2e33f'}>,\n",
              "  <Document: {'content': '                    \"url\": \"http://arxiv.org/pdf/2301.03988\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The current state of the Personally Identifiable Information (PII) redaction pipeline is outlined, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data are outlined.\"\\n                    }\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 446}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '48fd5176b9f825bd16366011a2710bde'}>,\n",
              "  <Document: {'content': '      }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"e41482f4ee984f17382f6cdd900df094d928be06\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 447}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '42a1818503cea9d1fa7999efd7c5303'}>,\n",
              "  <Document: {'content': '                      \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"WebArena: A Realistic Web Environment for Building Autonomous Agents\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 448}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4384598aa96cee1c475b882b8b292cec'}>,\n",
              "  <Document: {'content': '                   \"abstract\": \"With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 449}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe68d6e23fdcfcf582ed2c58ddc63930'}>,\n",
              "  <Document: {'content': 'success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2307.13854\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 450}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3d579e3840ce076dcf1d70f5247d325e'}>,\n",
              "  <Document: {'content': '        \"text\": \"This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 17,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"6e9a5af235cfd2167f02be825138ef73af0f400d\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 451}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cd8d4b4b55c889d69f9ccd76796aca14'}>,\n",
              "  <Document: {'content': '                 \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 452}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5cfcc73666419005ddff73a681a77e36'}>,\n",
              "  <Document: {'content': '               },\\n                    \"title\": \"Neural Rankers for Code Generation via Inter-Cluster Modeling\",\\n                    \"abstract\": \"Code Large Language Models (CodeLLMs) have ushered in a new era of code generation advancements. However, selecting the best solutions from among all possible CodeLLM solutions remains a challenge. Previous methods frequently overlooked the intricate functional similarities and interactions between clusters, resulting in suboptimal results. In this work, we introduce \\\\\\\\textit{SRank}, a novel reranking strategy for selecting the best solution from code generation that focuses on modeling inter-cluster relationship. By quantifying the functional overlap between clusters, our approach provides a better ranking strategy of code solutions. Empirical results show that our method achieves a remarkable results on pass@1 score. For instance, on the Human-Eval benchmark, we achieve 69.66\\\\\\\\% in pass@1 with Codex002, 75.31\\\\\\\\% for WizardCoder, 53.99\\\\\\\\% for StarCoder and 60.55\\\\\\\\% for CodeGen, which surpass the state-of-the-arts solution ranking methods, such as CodeT and Coder-Reviewer on the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 453}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f7c2c0417251d24c4bb6e0a1fbdadd37'}>,\n",
              "  <Document: {'content': 'same CodeLLM with significant margin ($\\\\\\\\approx 6.1\\\\\\\\%$ improvement on average). Comparing to the random sampling method, we can achieve an average improvement of $\\\\\\\\approx 23.07\\\\\\\\%$ on Human-Eval and 17.64\\\\\\\\% on MBPP. Even in scenarios with limited test inputs, our approach demonstrates robustness and superiority, marking a new state-of-the-arts in code generation reranking.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work introduces \\\\\\\\textit{SRank}, a novel reranking strategy for selecting the best solution from code generation that focuses on modeling inter-cluster relationship, and demonstrates robustness and superiority, marking a new state-of-the-arts in code generation reranking.\"\\n                    }\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 454}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f71081a95120805e01e2ee27439e875'}>,\n",
              "  <Document: {'content': '             },\\n                {\\n                    \"paperId\": \"59fe7cb560651281cfc5db6b8940da0e3ba9dea6\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fc0a208c-acb7-47dc-a0d4-af8190e21d29\",\\n                        \"name\": \"International Conference on Machine Learning\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 455}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c202e2bea34fd39936afcf67d896695e'}>,\n",
              "  <Document: {'content': '   \"ICML\",\\n                            \"Int Conf Mach Learn\"\\n                        ],\\n                        \"url\": \"https://icml.cc/\"\\n                    },\\n                    \"title\": \"LEVER: Learning to Verify Language-to-Code Generation with Execution\",\\n                    \"abstract\": \"The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 456}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e9c459cc6edc219d993a8d237dec12c'}>,\n",
              "  <Document: {'content': 'test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2302.08468\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 457}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'db4e154adb216fdbe24de8819359ab04'}>,\n",
              "  <Document: {'content': '                 \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results, and trains verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results.\"\\n                    }\\n                },\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 458}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '63860e4a61fc31284309db4a54210dee'}>,\n",
              "  <Document: {'content': '           {\\n                    \"paperId\": \"ccda5f933e4654a7748f443e1383b64fdb48d895\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Lightweight reranking for language model generations\",\\n                    \"abstract\": \"Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs. Reranking and selecting the best generation from the sampled set is a popular way of obtaining strong gains in generation quality. In this paper, we present a novel approach for reranking LLM generations. Unlike other techniques that might involve additional inferences or training a specialized reranker, our approach relies on easy to compute pairwise statistics between the generations that have minimal compute overhead. We show that our approach can be formalized as an extension of self-consistency and analyze its performance in that framework, theoretically as', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 459}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '830c2747134663ceca5669313a8c9d28'}>,\n",
              "  <Document: {'content': 'well as via simulations. We show strong improvements for selecting the best k generations for code generation tasks as well as robust improvements for the best generation for the tasks of autoformalization, summarization, and translation. While our approach only assumes black-box access to LLMs, we show that additional access to token probabilities can improve performance even further.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper presents a novel approach for reranking LLM generations that relies on easy to compute pairwise statistics between the generations that have minimal compute overhead and shows strong improvements for selecting the best k generations for code generation tasks as well as robust improvements for the best generation for the tasks', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 460}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '28e26f1d7522689e05403ec7e113cab5'}>,\n",
              "  <Document: {'content': 'of autoformalization, summarization, and translation.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 461}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e738a75b8785bc487821ba1dcc9788c3'}>,\n",
              "  <Document: {'content': '      \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 462}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8c9a82a4a416f051d33368b41610fa84'}>,\n",
              "  <Document: {'content': '    },\\n                    \"title\": \"SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents\",\\n                    \"abstract\": \"Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems\\' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 463}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ec758e24f5f2f0c64e299095eee205a8'}>,\n",
              "  <Document: {'content': 'humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA\\'s promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.\"\\n                    }\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 464}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4542eacb23387c98cc065a902edb1884'}>,\n",
              "  <Document: {'content': '      }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"133777180e326dfa53523bf53b0a969bbdccb0ee\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 465}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8f0b1f5222040f2b4a4ec4ba984c8d4a'}>,\n",
              "  <Document: {'content': '            \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 466}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '684217278f646552587a24b9a856d883'}>,\n",
              "  <Document: {'content': '            \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"API-Assisted Code Generation for Question Answering on Varied Table Structures\",\\n                    \"abstract\": \"A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures -- relational, multi-table, and hierarchical matrix shapes', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 467}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '898ae760106e2e43ffb8a63672948c4b'}>,\n",
              "  <Document: {'content': '-- and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A unified TableQA framework that provides a unified representation for structured tables as multi-index Pandas data frames, uses Python as a powerful querying language, and uses few-shot prompting to translate NL questions into Python programs, which are executable on PandasData frames is introduced.\"\\n                    }\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 468}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7760e9e89e5822e39f2f3e3857ed05e5'}>,\n",
              "  <Document: {'content': '           }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"9db9b7d22cfc37650808eb6855318f510dc639c4\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 469}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90dee0f045ed616233359bfd8fc04daf'}>,\n",
              "  <Document: {'content': '                 \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 470}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd66dfd73243a623847b71521f29cf4bb'}>,\n",
              "  <Document: {'content': '                 \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"Data Augmentation for Code Translation with Comparable Corpora and Multiple References\",\\n                    \"abstract\": \"One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 471}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ce7c8e140aa62e73cbb19888d4af9c91'}>,\n",
              "  <Document: {'content': 'significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of translations by execution. The code is available at https://github.com/Veronicium/CMTrans.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Two data augmentation techniques are presented, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations to reduce overfitting to a single reference translation.\"\\n                    }\\n                }\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 472}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '403ac7befde3fa76b7242c32e71ba139'}>,\n",
              "  <Document: {'content': '      ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"19f59c14b3d79e3203c696128a135d33eb35e468\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 473}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7219b522c57ee38014875fba9b8a5a0a'}>,\n",
              "  <Document: {'content': '\"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 474}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '342caba6cd5a96572d1d8d6fe7495da7'}>,\n",
              "  <Document: {'content': '\"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"Pragmatic Inference with a CLIP Listener for Contrastive Captioning\",\\n                    \"abstract\": \"We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 475}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c9e66f01fc40cbdf7546fcdadbad4067'}>,\n",
              "  <Document: {'content': 'a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity - outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.08818\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 476}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1b054091537a787240b4d1f246e8fe5'}>,\n",
              "  <Document: {'content': '                  \"text\": \"This work proposes a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images by leveraging an off-the-shelf CLIP model to parameterize the listener.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"f983cf75e368dcd07dd3a762721c095678514e56\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 477}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a4035750bb87917c56549d871adc3ea5'}>,\n",
              "  <Document: {'content': '              \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 478}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a07ec5e4cb1ec81d4eb93306fa63255b'}>,\n",
              "  <Document: {'content': '               \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 479}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8235c1d0ffd3e32e177de1a7200e52ed'}>,\n",
              "  <Document: {'content': '           \"url\": \"https://aclanthology.org/2023.findings-emnlp.23.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": null\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 8,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 480}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fa020f1cd11296d202f305fe1c072fb0'}>,\n",
              "  <Document: {'content': '          \"paperId\": \"fe3e247143fe9001cfb5dea6ac381852c90da853\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 481}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3da90b106c72b95b0d9bf321e16187ba'}>,\n",
              "  <Document: {'content': '        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Asking More Informative Questions for Grounded Retrieval\",\\n                    \"abstract\": \"When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions, limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 482}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c17996d0f8c3a428252f9fb0508d0129'}>,\n",
              "  <Document: {'content': 'to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes a method that can incorporate presupposition handling into both question selection and belief updates, and uses a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends.\"\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 483}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '944cc2e7b0546fdd7593be8cc049091'}>,\n",
              "  <Document: {'content': '     }\\n                },\\n                {\\n                    \"paperId\": \"7db1dbdd2344a3d825bef8d129990400ab124744\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"c6840156-ee10-4d78-8832-7f8909811576\",\\n                        \"name\": \"IEEE Transactions on Knowledge and Data Engineering\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 484}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd35813437f1e65ed00d67058fdff94b9'}>,\n",
              "  <Document: {'content': '             \"IEEE Trans Knowl Data Eng\"\\n                        ],\\n                        \"issn\": \"1041-4347\",\\n                        \"url\": \"https://www.computer.org/web/tkde\",\\n                        \"alternate_urls\": [\\n                            \"http://ieeexplore.ieee.org/servlet/opac?punumber=69\"\\n                        ]\\n                    },\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 485}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a0ebe1f4553a2a373882dde37ea40e59'}>,\n",
              "  <Document: {'content': '        \"title\": \"Graph-Grounded Goal Planning for Conversational Recommendation\",\\n                    \"abstract\": \"Conversational recommendation casts the recommendation problem as a dialog-based interactive task, which could acquire user interest more efficiently and effectively by allowing users to express what they like. In this work, we move a step towards a new conversational recommendation task that is more suitable for real-world applications. In this task, the recommender will proactively and naturally lead a dialog from non-recommendation content (i.e., chitchat or question answering) to approach an item being of interest to users, and allow users to ask questions about the item for better support of user decisions. The challenge of this task lies in how to effectively control the dialog flow to complete the recommendation while appropriately responding to user utterances. To address this challenge, we first construct a Chinese recommendation dialog dataset with 10k dialogs and 156k utterances at Baidu (DuRecDial). We then propose a two-stage Multi-Goal driven Conversation Generation framework (MGCG) with a graph-grounded goal planning module and a goal-guided responding module. The goal planning module leverages the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 486}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3df61fdd5984142dcd8f4981e33c11e9'}>,\n",
              "  <Document: {'content': 'information of global graph structure information and local goal-sequence information to effectively control the dialog flow step by step. The goal-guided responding module can produce an in-depth dialog about each goal by fully exploiting hierarchical goal information for response retrieval or generation. Results on DuRecDial demonstrate that compared with the state of the art models, MGCG can lead the dialog more proactively and naturally, and complete the recommendation task more effectively, confirming the benefits of hierarchical goal information to conversational recommendation.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Results on DuRecDial demonstrate that compared with the state of the art models, MGCG can lead the dialog more proactively and naturally, and complete the recommendation task more', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 487}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3611d13f8e1eb7b4d739daeba22b9340'}>,\n",
              "  <Document: {'content': 'effectively, confirming the benefits of hierarchical goal information to conversational recommendation.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"2e9895165b8b7aac2ea31db5bb4af67a3c2ffc69\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 488}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ae3c125fd8e733a48a0ff5a2cb9ff16'}>,\n",
              "  <Document: {'content': '          \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 489}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4a8aaa5979f5eba9bf94f9d78f2b0ebb'}>,\n",
              "  <Document: {'content': '     },\\n                    \"title\": \"Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search\",\\n                    \"abstract\": \"Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Providing FAQ content without disrupting user\\\\u2019s shopping experience poses challenges on deciding when and how to show FAQ results. Our proposed intent-aware FAQ retrieval consists of (1) an intent classifier that predicts whether the query is looking for an FAQ; (2) a reformulation model that rewrites query into a natural question. Offline evaluation demonstrates that our approach improves 12% in Hit@1 on retrieving ground-truth FAQs, while reducing latency by 95% compared to baseline systems. These improvements are further validated by real user feedback, where more than 99% of users consider FAQs displayed on top of product search results is helpful. Overall, our findings show promising', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 490}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff847cf44afc5dc3389c0c4c36978df6'}>,\n",
              "  <Document: {'content': 'directions for integrating FAQ retrieval into product search at scale.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.03411\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes intent-aware FAQ retrieval, which consists of an intent classifier that predicts whether the query is looking for an FAQ; a reformulation model that rewrites query into', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 491}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '96f67618c039794421c35049f47d5fc2'}>,\n",
              "  <Document: {'content': 'a natural question, and a reformulations model that reinvigorates query intoA natural question.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"ab2cd01090bebd7c52193c432e0a0fce4226982e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 492}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ea6cc41ddb2bf0da6bc4ca444c1db892'}>,\n",
              "  <Document: {'content': '                      \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Generating Pragmatic Examples to Train Neural Program Synthesizers\",\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 493}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '941473ecf755941e7dbfed4567a5be8f'}>,\n",
              "  <Document: {'content': '                  \"abstract\": \"Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one\\'s intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample.We then use the informative dataset to train models to improve the synthesizer\\'s ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 494}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '961bd8dfecd39b5ee5fe4e59aad8b1b5'}>,\n",
              "  <Document: {'content': 'strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A novel way to amortize program synthesis with neural networks by sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample.\"\\n                    }\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 495}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd216609e4b0e59c1c0ffa514d71cb778'}>,\n",
              "  <Document: {'content': '     }\\n            ]\\n        },\\n        {\\n            \"total\": 6948,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"cb23064700957a37aaf00f73b16f6c6930ca8c77\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 496}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f050108ab494dded23c4d7685d2c796d'}>,\n",
              "  <Document: {'content': '                 \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Comparative Knowledge Distillation\",\\n                    \"abstract\": \"In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 497}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6a745d4e8576703baddb3162e2672460'}>,\n",
              "  <Document: {'content': 'transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model\\'s interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 498}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5231a998f006a3491e9c8c25e5ec7fe5'}>,\n",
              "  <Document: {'content': '\"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Drawing inspiration from educational principles that emphasize learning through comparison, CKD is proposed, which encourages student models to understand the nuanced differences in a teacher model\\'s interpretations of samples, and consistently outperforms state of the art data augmentation and KD techniques.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"eba9a4a57939eedc58cd66e610ccb7e09f1c3628\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 499}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7d52d7235b3543d6c60ee00934694961'}>,\n",
              "  <Document: {'content': '     \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 500}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '41307d0fa44ffbad8f52c8bb5110d23f'}>,\n",
              "  <Document: {'content': '   },\\n                    \"title\": \"NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge\",\\n                    \"abstract\": \"Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-4, which have fueled efforts towards harvesting their knowledge into knowledge bases. While acquisition of such comparative knowledge is much easier from models like GPT-4, compared to their considerably smaller and weaker counterparts such as GPT-2, not even the most powerful models are exempt from making errors. We thus ask: to what extent are models at different scales able to generate valid and diverse comparative knowledge? We introduce NeuroComparatives, a novel framework for comparative knowledge distillation overgenerated from language models such as GPT-variants and Llama, followed by stringent filtering of the generated knowledge. Our framework acquires comparative knowledge between everyday objects, producing a', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 501}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bb7354ef6767d258644e96f90f2ab42c'}>,\n",
              "  <Document: {'content': 'corpus of up to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more diverse than existing resources. Moreover, human evaluations show that NeuroComparatives outperform existing resources (up to 32% absolute improvement). We also demonstrate the utility of our distilled NeuroComparatives on three downstream tasks. Our results show that neuro-symbolic manipulation of smaller models offer complementary benefits to the currently dominant practice of prompting extreme-scale language models for knowledge distillation.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.04978\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 502}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e3d603c489e70768c49dacfcdf626ef0'}>,\n",
              "  <Document: {'content': '       \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The results show that neuro-symbolic manipulation of smaller models offer complementary benefits to the currently dominant practice of prompting extreme-scale language models for knowledge distillation, and that NeuroComparatives outperform existing resources.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"f110dedbf70a523686e98a12060eedb38f43a57e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"5752fdec-1539-4150-9e8e-916442a04cb6\",\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 503}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62490e63feb9f922e9648d8945a9a6c6'}>,\n",
              "  <Document: {'content': '      \"name\": \"Colloquium in Information Science and Technology\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"IEEE Int Colloq Inf Sci Technol\",\\n                            \"CIST\",\\n                            \"Colloq Inf Sci Technol\",\\n                            \"IEEE International Colloquium on Information Science and Technology\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 504}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ec85e96f73470db1c6f55f7c64895d96'}>,\n",
              "  <Document: {'content': '                 \"CiSt\"\\n                        ]\\n                    },\\n                    \"title\": \"Comparative Analysis of Datasets for Response-based Knowledge Distillation in Image Classification\",\\n                    \"abstract\": \"This paper presents a comparative analysis of different datasets\\\\u2019 influence on the effectiveness of response-based knowledge distillation in image classification. Through a series of experiments and evaluations, we explore the impact of dataset choice on the performance and transferability of distilled models. Our findings provide insights into optimizing dataset selection for response-based knowledge distillation, a critical aspect of model compression and transfer learning.\",\\n                    \"openAccessPdf\": null,\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 505}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e86a3f0944b5585f384eeb626f66b34a'}>,\n",
              "  <Document: {'content': '                \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A comparative analysis of different datasets\\\\u2019 influence on the effectiveness of response-based knowledge distillation in image classification provides insights into optimizing dataset selection for response-based knowledge distillation, a critical aspect of model compression and transfer learning.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 506}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8fc0371829c998d0344479ae6b6f53ad'}>,\n",
              "  <Document: {'content': '    \"data\": [\\n                {\\n                    \"paperId\": \"5b0458d5e839117884739d4b9dead0285beb4a76\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Symbolic Planning and Code Generation for Grounded Dialogue\",\\n                    \"abstract\": \"Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 507}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5195c13a377a29a2c4001e4a299303f2'}>,\n",
              "  <Document: {'content': 'translated code\\\\u2019s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system\\\\u2019s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work presents a modular and interpretable grounded dialogue system that substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.\"\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58001d40010245d57345fc35cee78e1a'}>,\n",
              "  <Document: {'content': '    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 2,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"5016766c87982f5c62ef6580e120939ab155d776\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ebf0489e664ccd836c92abdb5ebc0b1d'}>,\n",
              "  <Document: {'content': '                   \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Amortizing Pragmatic Program Synthesis with Rankings\",\\n                    \"abstract\": \"In program synthesis, an intelligent system takes in a set of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5f1fe659f77e5d005fe20d2946f2e7da'}>,\n",
              "  <Document: {'content': 'user-generated examples and returns a program that is logically consistent with these examples. The usage of Rational Speech Acts (RSA) framework has been successful in building \\\\\\\\emph{pragmatic} program synthesizers that return programs which -- in addition to being logically consistent -- account for the fact that a user chooses their examples informatively. However, the computational burden of running the RSA algorithm has restricted the application of pragmatic program synthesis to domains with a small number of possible programs. This work presents a novel method of amortizing the RSA algorithm by leveraging a \\\\\\\\emph{global pragmatic ranking} -- a single, total ordering of all the hypotheses. We prove that for a pragmatic synthesizer that uses a single demonstration, our global ranking method exactly replicates RSA\\'s ranked responses. We further empirically show that global rankings effectively approximate the full pragmatic synthesizer in an online, multi-demonstration setting. Experiments on two program synthesis domains using our pragmatic ranking method resulted in orders of magnitudes of speed ups compared to the RSA synthesizer, while outperforming the standard, non-pragmatic synthesizer.\",\\n                    \"openAccessPdf\": {\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e837c8eb238061e63ace1e8f9e1e3e8'}>,\n",
              "  <Document: {'content': '                 \"url\": \"https://arxiv.org/pdf/2309.03225\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work presents a novel method of amortizing the RSA algorithm by leveraging a single, total ordering of all the hypotheses, and proves that for a pragmatic synthesizer that uses a single demonstration, the global ranking method exactly replicates RSA\\'s ranked responses.\"\\n                    }\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e952fa083f11ccf04e1bc65049ae1bdc'}>,\n",
              "  <Document: {'content': '           },\\n                {\\n                    \"paperId\": \"ab2cd01090bebd7c52193c432e0a0fce4226982e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4e5fd329fb7a4f1b2945d51e36d0fc9'}>,\n",
              "  <Document: {'content': '                    \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Generating Pragmatic Examples to Train Neural Program Synthesizers\",\\n                    \"abstract\": \"Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one\\'s intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 514}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4aba7efdef15ccaca282e0db35cd73bc'}>,\n",
              "  <Document: {'content': 'However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample.We then use the informative dataset to train models to improve the synthesizer\\'s ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 515}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e1a048b1298f74f9101b7f5b9d03b2d5'}>,\n",
              "  <Document: {'content': '    \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A novel way to amortize program synthesis with neural networks by sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample.\"\\n                    }\\n                }\\n            ]\\n        }\\n    ],\\n    \"Anatole Gershman\": [\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 516}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ee00bb6849289e2992c3ecf83b67dfc'}>,\n",
              "  <Document: {'content': '              \"paperId\": \"ca2837930cc6f3cbcb58f511503e6b799d11ea6b\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"9be9b5e1-04d3-4e08-92d7-1566af231bfe\",\\n                        \"name\": \"International Symposium on Algorithms\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Inf Secur Assur\",\\n                            \"Adv Inf Secur', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 517}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4069f127bd45cb3d1e1a44f07da2732'}>,\n",
              "  <Document: {'content': 'Assur\",\\n                            \"SIGAL\",\\n                            \"Advances in Information Security and Assurance\",\\n                            \"Information Security and Assurance\",\\n                            \"ISA\",\\n                            \"Int Symp Algorithm\"\\n                        ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=1666\"\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 518}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd53ba563e109819e8a107781da71916'}>,\n",
              "  <Document: {'content': '                   },\\n                    \"title\": \"The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing\",\\n                    \"abstract\": \"With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\\\\u2019s illness is labeled \\\\u201cdiagnosis\\\\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 519}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5b4e2f3ee8a37b772788d3123f20108'}>,\n",
              "  <Document: {'content': 'comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A curated subset of Wikidata in which events have been enriched with PropBank roles is presented, to enable richer narrative understanding between events fromWikidata concepts and a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.\"\\n                    }\\n                }\\n            ]\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 520}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1c58f073e71d819b3f428f224f0d5a80'}>,\n",
              "  <Document: {'content': ' }\\n    ],\\n    \"Alexander Hauptmann\": [\\n        {\\n            \"total\": 7,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"b2faf2a4c1e0c1e5788309d83fe24d2d56555237\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"MAGVIT: Masked Generative Video Transformer\",\\n                    \"abstract\": \"The authors would like to thank Tom Duerig, Victor Gomes, Paul Natsev along with the Multipod committee for sponsoring the computing resources. We appreciate valuable', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 521}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4dc24460a9df2ea855cf6f5430571a1'}>,\n",
              "  <Document: {'content': 'feedback and leadership support from David Salesin, Jay Yagnik, Tomas Izo, and Rahul Sukthankar thoughout the project. Special thanks to Wolfgang Macherey for supporting the project. We thank David Alexander Ross and Yu-Chuan Su for many helpful comments for improving the paper. We also give thanks to Sarah Laszlo and Hugh Williams for creating the MAGVIT model card, Bryan Seybold and Albert Shaw for extending the features, Jonathan Ho and Tim Salimans for providing the JAX code pointer for FVD computation, and the Scenic team for the infrastructure support. We are thankful to Wenhe Liu, Xinyu Yao, Mingzhi Cai, Yizhi Zhang, and Zhao Jin for proof reading the paper. This project is funded in part by Carnegie Mellon University\\\\u2019s Mobility21 National University Transportation Center, which is sponsored by the US Department of Transportation.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 522}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '98ce64c33492565da7daf198338cf8e8'}>,\n",
              "  <Document: {'content': '                       \"text\": \"This project is funded in part by Carnegie Mellon University\\\\u2019s Mobility21 National University Transportation Center, which is sponsored by the US Department of Transportation.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"29502f2f082a308c027672240df6d717352104f1\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 523}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2867ca51e2d29a57aa92750c0dc995b1'}>,\n",
              "  <Document: {'content': '               \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers\",\\n                    \"abstract\": \"Recent advances in generative AI have significantly enhanced image and video editing, particularly', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 524}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b55681a4d0d1745b9339a7bcf5d70f8'}>,\n",
              "  <Document: {'content': 'in the context of text prompt control. State-of-the-art approaches predominantly rely on diffusion models to accomplish these tasks. However, the computational demands of diffusion-based methods are substantial, often necessitating large-scale paired datasets for training, and therefore challenging the deployment in practical applications. This study addresses this challenge by breaking down the text-based video editing process into two separate stages. In the first stage, we leverage an existing text-to-image diffusion model to simultaneously edit a few keyframes without additional fine-tuning. In the second stage, we introduce an efficient model called MaskINT, which is built on non-autoregressive masked generative transformers and specializes in frame interpolation between the keyframes, benefiting from structural guidance provided by intermediate frames. Our comprehensive set of experiments illustrates the efficacy and efficiency of MaskINT when compared to other diffusion-based methodologies. This research offers a practical solution for text-based video editing and showcases the potential of non-autoregressive masked generative transformers in this domain.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 525}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '13f1f1837248ab218ad055743420316f'}>,\n",
              "  <Document: {'content': '                   \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This research introduces an efficient model called MaskINT, which is built on non-autoregressive masked generative transformers and specializes in frame interpolation between the keyframes, benefiting from structural guidance provided by intermediate frames.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"306113d470ca518cd501a882ffe6dd35d548d713\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"768b87bb-8a18-4d9c-a161-4d483c776bcf\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 526}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37bbc99c6efe7cad8d9de684f19da1d'}>,\n",
              "  <Document: {'content': '                   \"name\": \"Computer Vision and Pattern Recognition\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"CVPR\",\\n                            \"Comput Vis Pattern Recognit\"\\n                        ],\\n                        \"issn\": \"1063-6919\",\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 527}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '250ee8f519d869016171b36114b0bb0f'}>,\n",
              "  <Document: {'content': '      \"url\": \"https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147\",\\n                        \"alternate_urls\": [\\n                            \"https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition\"\\n                        ]\\n                    },\\n                    \"title\": \"Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers\",\\n                    \"abstract\": \"Autoregressive transformers have shown remarkable success in video generation. However, the transformers are prohibited from directly learning the longterm dependency in videos due to the quadratic complexity of self-attention, and inherently suffering from slow inference time and error propagation due to the autoregressive process.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 528}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '848a941d574bab719b966f65febbce84'}>,\n",
              "  <Document: {'content': 'In this paper, we propose Memory-efficient Bidirectional Transformer (MeBT) for end-to-end learning of longterm dependency in videos and fast inference. Based on recent advances in bidirectional transformers, our method learns to decode the entire spatio-temporal volume of a video in parallel from partially observed patches. The proposed transformer achieves a linear time complexity in both encoding and decoding, by projecting observable context tokens into a fixed number of latent tokens and conditioning them to decode the masked tokens through the cross-attention. Empowered by linear complexity and bidirectional modeling, our method demonstrates significant improvement over the autoregressive transformers for generating moderately long videos in both quality and speed. Videos and code are available at https://sites.google.com/view/mebt-cvpr2023.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2303.11251\",\\n                        \"status\": \"GREEN\"\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 529}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8aea44ad8a09f774911ed11804bfa20e'}>,\n",
              "  <Document: {'content': '    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Memory-efficient Bidirectional Transformer (MeBT) is proposed for end-to-end learning of longterm dependency in videos and fast inference, and demonstrates significant improvement over the autoregressive transformers for generating moderately long videos in both quality and speed.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 530}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c0782c7376db9c9da801dcd9d03fbc9'}>,\n",
              "  <Document: {'content': '           \"data\": [\\n                {\\n                    \"paperId\": \"985f0c89c5a607742ec43c1fdc2cbfe54541cbad\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 531}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f0a6365b4b3d4de6561759424568a26a'}>,\n",
              "  <Document: {'content': '                     \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Language Model Beats Diffusion - Tokenizer is Key to Visual Generation\",\\n                    \"abstract\": \"While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 532}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '241e4ec84d0a9c8ef7a4fb94a507e5ef'}>,\n",
              "  <Document: {'content': 'this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2310.05737\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 533}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3ffc9ba9ef042452d628730e8fb0a32b'}>,\n",
              "  <Document: {'content': '                     \"text\": null\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 2,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"405e3910e06c9efe7e660b8697bcb4bab4e92f48\",\\n                    \"publicationVenue\": {\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 534}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e57d3074624cbfcf1f8e40ea087e06a'}>,\n",
              "  <Document: {'content': '       \"id\": \"768b87bb-8a18-4d9c-a161-4d483c776bcf\",\\n                        \"name\": \"Computer Vision and Pattern Recognition\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"CVPR\",\\n                            \"Comput Vis Pattern Recognit\"\\n                        ],\\n                        \"issn\": \"1063-6919\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 535}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd9ca0518d3e397926312ea631cebe6c'}>,\n",
              "  <Document: {'content': '                   \"url\": \"https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147\",\\n                        \"alternate_urls\": [\\n                            \"https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition\"\\n                        ]\\n                    },\\n                    \"title\": \"STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition\",\\n                    \"abstract\": \"We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standard-ized skeleton representations as model input, we propose a novel', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 536}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4c761526605e0e155f8ebb9d462fd585'}>,\n",
              "  <Document: {'content': 'Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn nonlocal relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/STMT.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2303.18177\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 537}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2312eb49ad0f4d6c469e264c58c722d4'}>,\n",
              "  <Document: {'content': ' \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A novel Spatial-Temporal Mesh Transformer (STMT) is proposed to directly model the mesh sequences using motion capture sequences to achieve state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"c0de5038ef17558a8a5b7ceb44125c385a42692d\",\\n                    \"publicationVenue\": {\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 538}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '224dbdf91e621e21f366262e3765d885'}>,\n",
              "  <Document: {'content': ' \"id\": \"5fbb417b-d7a5-44e6-856d-993f0624ed9c\",\\n                        \"name\": \"Computer Vision and Image Understanding\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Comput Vis Image Underst\"\\n                        ],\\n                        \"issn\": \"1077-3142\",\\n                        \"url\": \"http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 539}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8fbc4ea003ec2c186749c32e8cee3c83'}>,\n",
              "  <Document: {'content': '          \"alternate_urls\": [\\n                            \"http://www.sciencedirect.com/science/journal/10773142\",\\n                            \"http://www.idealibrary.com/links/toc/cviu\",\\n                            \"https://www.journals.elsevier.com/computer-vision-and-image-understanding\"\\n                        ]\\n                    },\\n                    \"title\": \"SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network\",\\n                    \"abstract\": \"Recent advancements in technology have expanded the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 540}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1f60e01fec462280ee13cd7e562bf9ac'}>,\n",
              "  <Document: {'content': 'possibilities of human action recognition by leveraging 3D data, which offers a richer representation of actions through the inclusion of depth information, enabling more accurate analysis of spatial and temporal characteristics. However, 3D human action recognition is a challenging task due to the irregularity and Disarrangement of the data points in action sequences. In this context, we present our novel model for human action recognition from fixed topology mesh sequences based on Spiral Auto-encoder and Transformer Network, namely SpATr. The proposed method first disentangles space and time in the mesh sequences. Then, an auto-encoder is utilized to extract spatial geometrical features, and tiny transformer is used to capture the temporal evolution of the sequence. Previous methods either use 2D depth images, sample skeletons points or they require a huge amount of memory leading to the ability to process short sequences only. In this work, we show competitive recognition rate and high memory efficiency by building our auto-encoder based on spiral convolutions, which are light weight convolution directly applied to mesh data with fixed topologies, and by modeling temporal evolution using a attention, that can handle large sequences. The proposed method is evaluated on on two 3D human action datasets: MoVi', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 541}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3ecad2272b6ca2d7b33a60a6a71e7bb'}>,\n",
              "  <Document: {'content': 'and BMLrub from the Archive of Motion Capture As Surface Shapes (AMASS). The results analysis shows the effectiveness of our method in 3D human action recognition while maintaining high memory efficiency. The code will soon be made publicly available.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.17574\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 542}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '59e14f3defae5785ecb8ca80e19cc984'}>,\n",
              "  <Document: {'content': '\"This work shows competitive recognition rate and high memory efficiency by building the auto-encoder based on spiral convolutions, which are light weight convolution directly applied to mesh data with fixed topologies, and by modeling temporal evolution using a attention that can handle large sequences.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"6c7456a7a0f98e8a79a53f8a418e9ffc7ff827aa\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 543}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92abdb1deecb23f790016d9baa6ba2a7'}>,\n",
              "  <Document: {'content': '   \"publicationVenue\": {\\n                        \"id\": \"7654260e-79f9-45c5-9663-d72027cf88f3\",\\n                        \"name\": \"IEEE International Conference on Computer Vision\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ICCV\",\\n                            \"IEEE Int Conf Comput Vis\",\\n                            \"ICCV Workshops\",\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 544}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dcc2a25cebd2be450bebf630cc69bcd8'}>,\n",
              "  <Document: {'content': '                          \"ICCV Work\"\\n                        ],\\n                        \"url\": \"https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings\"\\n                    },\\n                    \"title\": \"Breaking The Limits of Text-conditioned 3D Motion Synthesis with Elaborative Descriptions\",\\n                    \"abstract\": \"Given its wide applications, there is increasing focus on generating 3D human motions from textual descriptions. Differing from the majority of previous works, which regard actions as single entities and can only generate short sequences for simple motions, we propose EMS, an elaborative motion synthesis model conditioned on detailed natural language descriptions.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 545}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b4acb823be008b7306fc70d8c2d0421'}>,\n",
              "  <Document: {'content': 'It generates natural and smooth motion sequences for long and complicated actions by factorizing them into groups of atomic actions. Meanwhile, it understands atomic-action level attributes (e.g., motion direction, speed, and body parts) and enables users to generate sequences of unseen complex actions from unique sequences of known atomic actions with independent attribute settings and timings applied. We evaluate our method on the KIT Motion-Language and BABEL benchmarks, where it outperforms all previous state-of-the-art with noticeable margins.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"EMS, an elaborative motion synthesis model conditioned on detailed natural language descriptions, generates natural and smooth motion sequences for long and complicated actions by factorizing them into groups of atomic actions.\"\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 546}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ae92193ea9ad2f7b3b468f99fbdc0447'}>,\n",
              "  <Document: {'content': '                  }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 154,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"7dede59b5653a36dc1cbc4cfab8522c7ec4cc654\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Encompassing YOLOv8 and EfficientNet B7 for Automatic', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 547}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'aabb3ab79f9c944e86d657388b7bb0b4'}>,\n",
              "  <Document: {'content': 'License Plate Identification\",\\n                    \"abstract\": \"The rapid urbanization and increased vehicular population in metropolitan areas have presented a formidable obstacle to the manual identification of license plates. The surge in vehicular activity has led to a complex task, requiring efficient surveillance of each vehicle for vital functions such as theft prevention and traffic regulation. In response to this intricate challenge, the deployment of an Automatic License Plate Recog-nition (ALPR) system emerges as a crucial necessity for smart city transportation. This research aims to develop an efficient ALPR system employing YOLOv8, the latest iteration in the YOLO series of object detection models, for license plate localization, and utilizing EfficientNet B7 for character recognition on the detected plates. The YOLOv8 license plate detection module achieved a mean Average Precision (mAP 50) of 99.5%, ensuring robust and reliable license plate localization. The character recognition component, utilizing EfficientNet B7, achieved an accuracy of 98.22%, further enhancing the system\\'s overall performance and determining the amalgamation of YOLOv8 and EfficientNet B7 as an optimal solution. To validate the efficacy of our ALPR system, a diverse set of test', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 548}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b0d572dced1c316a7585257af7db9bd8'}>,\n",
              "  <Document: {'content': 'data comprising various license plate types is employed. The system demonstrates remarkable identification accuracy, achieving a recognition rate of 96.6 %. These results highlight the practical feasibility and efficacy of our proposed ALPR system in real world applications.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This research aims to develop an efficient ALPR system employing YOLOv8, the latest iteration in the YOLO series of object detection models, for license plate localization, and utilizing EfficientNet B7 for character recognition on the detected plates, demonstrating remarkable identification accuracy.\"\\n                    }\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 549}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dad995aff4c56424db596b96a1dd17b2'}>,\n",
              "  <Document: {'content': '     },\\n                {\\n                    \"paperId\": \"203126139130ec6935f38f773eb2c3db63c19cbc\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"51f58655-ea0a-4e77-8bcf-f1264a71de0a\",\\n                        \"name\": \"Vestnik of Saint Petersburg University Applied Mathematics Computer Science Control Processes\",\\n                        \"alternate_names\": [\\n                            \"Vestnik St Bg Univ Appl Math Comput Sci Control Process\"\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 550}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7daec1cddf45668f8176503916e7f40'}>,\n",
              "  <Document: {'content': '         ],\\n                        \"issn\": \"1811-9905\",\\n                        \"alternate_issns\": [\\n                            \"2542-2251\"\\n                        ],\\n                        \"url\": \"http://vestnik.spbu.ru/s10.html\"\\n                    },\\n                    \"title\": \"Network traffic anomalies automatic detection in DDoS attacks\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 551}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '849eb314c5a5d1d9ec7aad3219459134'}>,\n",
              "  <Document: {'content': '    \"abstract\": \"Distributed denial-of-service attacks (DDoS attacks) are intrusions into computing systems of the Internet. Their purpose is to make systems of the Internet inaccessible for users. DDoS attack consist of sending many requests to a certain resource at the same time. As a result, the server cannot withstand the network load. In such situation, a provider must determine the moment when attack begins and change the traffic management strategy. Detection of the beginning of a DDoS attack is possible by using unsupervised machine learning methods and sequential statistical analysis of network activity. To activate that, convenient to use mathematical models based on discrete random processes with monotonically increasing trajectories. Random functions, which are represented in the correspondence between generalized time and the cumulative sum of network traffic or the correspondence between the total number of incoming packets and the cumulative sum of packets processed, change their type of increasing from linear to non-linear. In the first case, to parabolic or exponential, in the second case to logarithmic or arctangent. To determine the moment when the type of increasing is going to change, one can use quadratic forms of approximation-estimation tests as statistical rules.\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 552}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e99eff21f475ddb95069f8b224d999'}>,\n",
              "  <Document: {'content': '                \"openAccessPdf\": {\\n                        \"url\": \"https://dspace.spbu.ru/bitstream/11701/43832/1/10.pdf\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": null\\n                    }\\n                },\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 553}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b3f2e5912a822f3769ea31190468522e'}>,\n",
              "  <Document: {'content': '          {\\n                    \"paperId\": \"5f32d1d8ad3080084951be8a1b77858a14b54674\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Automatic detection of artifacts and improved classification models for emotional activity detection from multimodal physiological data\",\\n                    \"abstract\": \"This manuscript proposes an automated artifacts detection and multimodal classification system for human emotion analysis from human physiological signals. First, multimodal physiological data, including the Electrodermal Activity (EDA), electrocardiogram (ECG), Blood Volume Pulse (BVP) and respiration rate signals are collected. Second, a Modified Compressed Sensing-based Decomposition (MCSD) is used to extract the informative Skin Conductance Response (SCR) events of the EDA signal. Third, raw features (edge and sharp variations), statistical and wavelet coefficient features of EDA, ECG, BVP, respiration and SCR signals are obtained. Fourth, the extracted raw features, statistical and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 554}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3ea968c81af53c71a811a56041dee342'}>,\n",
              "  <Document: {'content': 'wavelet coefficient features from all physiological signals are fed into the parallel Deep Convolutional Neural Network (DCNN) to reduce the dimensionality of feature space by removing artifacts. Fifth, the fused artifact-free feature vector is obtained for neutral, stress and pleasure emotion classes. Sixth, an artifact-free feature vector is used to train the Random Forest Deep Neural Network (RFDNN) classifier. Then, a trained RFDNN classifier is applied to classify the test signals into different emotion classes. Thus, leveraging the strengths of both RF and DNN algorithms, more comprehensive feature learning using multimodal psychological data is achieved, resulting in robust and accurate classification of human emotional activities. Finally, an extensive experiment using the Wearable Stress and Affect Detection (WESAD) dataset shows that the proposed system outperforms other existing human emotion classification systems using physiological data.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 555}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '80a19442648ced087d26c0c4ade0c5d7'}>,\n",
              "  <Document: {'content': '                       \"text\": null\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"7fb98b6c5bb07c2b010966c05f29d9db7f783d27\",\\n                    \"publicationVenue\": {\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 556}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56ae56aa32c8ddf8c0873bf46add0a0d'}>,\n",
              "  <Document: {'content': '         \"id\": \"7654260e-79f9-45c5-9663-d72027cf88f3\",\\n                        \"name\": \"IEEE International Conference on Computer Vision\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ICCV\",\\n                            \"IEEE Int Conf Comput Vis\",\\n                            \"ICCV Workshops\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 557}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e2287c70e9a6560a413be6f137fd3a8f'}>,\n",
              "  <Document: {'content': '       \"ICCV Work\"\\n                        ],\\n                        \"url\": \"https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings\"\\n                    },\\n                    \"title\": \"ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules\",\\n                    \"abstract\": \"Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 558}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9aa136475c070a362335177a63b463eb'}>,\n",
              "  <Document: {'content': 'module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.1\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 559}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fce163c4ff903544aed3aa2de809f1c4'}>,\n",
              "  <Document: {'content': '     \"text\": \"The proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model and offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"8ccda6de0223bcd897d5dc0efc8f33222a899d0d\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 560}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1bd35dcd53e5e8be4fdb23ad815dc9e0'}>,\n",
              "  <Document: {'content': '        \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 561}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b0a081a3242b831cc294b5dfd58e9e86'}>,\n",
              "  <Document: {'content': '         \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"DocumentNet: Bridging the Data Gap in Document Pre-training\",\\n                    \"abstract\": \"Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 562}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '16abeab00f66feaf9a155726dcfed114'}>,\n",
              "  <Document: {'content': 'and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multi-modal capabilities for VDER.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.emnlp-industry.66.pdf\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 563}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c197ebb0404717fdaf1a0f3655cd8a1'}>,\n",
              "  <Document: {'content': '   \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper proposes a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models, and provides a large data source to extend their multi-modal capabilities for VDER.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 564}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e8d91c58941fee7447cf2dab1b8e58d9'}>,\n",
              "  <Document: {'content': '     \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"2107b867cb8f8afa30a9a940288d7c8b657f8aa5\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 565}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c545d045752c35ebf793505d68ef8f9e'}>,\n",
              "  <Document: {'content': '                         \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 566}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f949c14e155f1b6de7c6043ab97c802'}>,\n",
              "  <Document: {'content': '           \"title\": \"Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation\",\\n                    \"abstract\": \"Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 567}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '190a7d359f11cf0b2ddb43b5f1bc2e07'}>,\n",
              "  <Document: {'content': '\"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.acl-short.127.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper utilizes a conditional generation framework and formulates the zero-shot and few-shot stance detection problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts.\"\\n                    }\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 568}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6b2353f3524ceb9d752d6dd0a5f98ec'}>,\n",
              "  <Document: {'content': '          }\\n            ]\\n        },\\n        {\\n            \"total\": 7,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"72cce47fd053bf916314d89a8174726c58c05e02\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 569}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f55333d1e458ef8368154f1e77d96b3'}>,\n",
              "  <Document: {'content': 'of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 570}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b8d1dae5fcf0e63ad437af7de19b8f04'}>,\n",
              "  <Document: {'content': '    ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"Towards Open-Domain Twitter User Profile Inference\",\\n                    \"abstract\": \",\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.findings-acl.198.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 571}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b218b62c82701b724216930d6f0dcc82'}>,\n",
              "  <Document: {'content': '       \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": null\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 572}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b834d5f437b83238068ffc17df54e019'}>,\n",
              "  <Document: {'content': ' \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models\",\\n                    \"abstract\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 573}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a88a99425c872b4313688f6c10522ca'}>,\n",
              "  <Document: {'content': '\"Recommender systems play a vital role in various online services. However, the insulated nature of training and deploying separately within a specific domain limits their access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capability. Nevertheless, previous attempts to directly use LLMs as recommenders have not achieved satisfactory results. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs -- the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei\\'s news', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 574}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '47c2fd95db5e5b6c5a3482b5df832f4a'}>,\n",
              "  <Document: {'content': 'and music recommendation platforms and gain a 7\\\\\\\\% and 1.7\\\\\\\\% improvement in the online A/B test, respectively.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.10933\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 575}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '318176b6258028ac9ee553867adcdf6b'}>,\n",
              "  <Document: {'content': 'from LLMs -- the reasoning knowledge on user preferences and the factual knowledge on items.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"7e0d902dd70736b0da5eedab0b69392588e7fbd2\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"d068a5ed-de42-4f73-8312-51d07ecc19b7\",\\n                        \"name\": \"Electronic imaging\",\\n                        \"type\": \"journal\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 576}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '65b0373f48296ae6ddd4f9a96ffade60'}>,\n",
              "  <Document: {'content': '        \"alternate_names\": [\\n                            \"Electron imaging\",\\n                            \"electron imaging\",\\n                            \"electronic imaging\"\\n                        ],\\n                        \"issn\": \"0737-6553\"\\n                    },\\n                    \"title\": \"Practical OSINT investigation in Twitter utilizing AI-based aggressiveness analysis\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 577}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4c89a296de716b42ec28d1b4b1430cfe'}>,\n",
              "  <Document: {'content': '              \"abstract\": \"Open-source intelligence is gaining popularity these days due to the development of social networks. There is more and more information in the public domain. Twitter is one of the most popular social networks, so it\\\\u2019s worth analyzing its information. It was chosen to analyze the dependence of changes in the number of likes, reposts, quotes, and retweets on the aggressiveness of the post text for a separate profile since this information may be important not only for the owner of the channel on the social network but also for other studies that somehow affect user accounts and their behavior on the social network. also, the task of this work was a detailed analysis and evaluation of the capabilities of the tweety library and situations in which it can be effectively applied. also, the creation and description of a compiled neural network, the purpose of which is to predict changes in the number of likes, re-posts, quotes, and retweets from the aggressiveness of the post text for a separate profile.\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 578}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '79fcaea1a7ae9114e6d96d846b324638'}>,\n",
              "  <Document: {'content': '   \"openAccessPdf\": {\\n                        \"url\": \"https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/ei/35/3/MOBMU-355\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The task of this work was a detailed analysis and evaluation of the capabilities of the tweety library and situations in which it can be effectively applied and the creation and description of a compiled neural network.\"\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 579}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d891bd47e8385b678615c5fb7db9db4'}>,\n",
              "  <Document: {'content': ' }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 2722,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"6ea9bc55ee84db9d7370bcff98a46677f6f693e1\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"75f22e51-7d22-427e-aff6-cc35afbc508c\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 580}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3378d14ff0a7804313cdef123e1220f9'}>,\n",
              "  <Document: {'content': '    \"name\": \"IIAI International Conference on Advanced Applied Informatics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"IIAI-AAI\",\\n                            \"IIAI Int Conf Adv Appl Informatics\"\\n                        ]\\n                    },\\n                    \"title\": \"Movie Caption Generation with Vision Transformer and Transformer-based Language Model\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 581}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cf1b6ce3ef75a6084aa0d1c3187cc068'}>,\n",
              "  <Document: {'content': '                \"abstract\": \"The paper presents a video caption generation system using Vision Transformer (ViT) and a Transformer-based language model. A video is regarded as images with time information but it is difficult to analyze a video with ordinary image recognition techniques because of the difference between spacial information and time information. So, the system analyzes each image with ViT and integrates all image features into a video feature. ViT is a state-of-the-art object recognition system and consists of stacked Transformers. ViT is not trained with a training dataset but is employed with a pre-trained model. In a caption generation module, a caption is generated with Transformer decoders based on the video feature. The caption generation module is trained with a training dataset. In experiments, we use a large-scale video captioning dataset and train the proposed system. As experiment results, the proposed system achieves 0.27 F-measure in Rouge-L and we confirm that the trained system can generate appropriate captions according to input videos from the viewpoint of human judgment. The results show the proposed system is superior to the system with VGG16.\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 582}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e63ca25ef8d17cfd90e251a04411c5ac'}>,\n",
              "  <Document: {'content': '               \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A video caption generation system using Vision Transformer and a Transformer-based language model and it is confirmed that the trained system can generate appropriate captions according to input videos from the viewpoint of human judgment.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"38324aba4aed75c0d08408991c5f94be51ae670f\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 583}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '315a6f3cad88f648bde82c9b40545db1'}>,\n",
              "  <Document: {'content': '              \"publicationVenue\": {\\n                        \"id\": \"c89c0957-b21e-40c3-9e6c-0ab46adf1e53\",\\n                        \"name\": \"International Conference on Image Analysis and Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ICIAP\",\\n                            \"Int Conf Image Anal Process\"\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 584}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'df49533b57667cce6b0203c10ae1f8b2'}>,\n",
              "  <Document: {'content': '     ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=1380\"\\n                    },\\n                    \"title\": \"SynthCap: Augmenting Transformers with Synthetic Data for Image Captioning\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://iris.unimore.it/bitstream/11380/1309206/6/2023-iciap-captioning.pdf\",\\n                        \"status\": \"GREEN\"\\n                    },\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 585}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '484ad64532a77ac64a93d015867ac1be'}>,\n",
              "  <Document: {'content': '           \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes a simple yet effective synthetic data augmentation framework that is capable of significantly improving the quality of captions generated by a standard Transformer-based model, leading to competitive results on the COCO dataset.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"0a36008613d67fb3aac8345f847fc4787a0d69f3\",\\n                    \"publicationVenue\": {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 586}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd57a7d6262c37f9687f99964db29ddd7'}>,\n",
              "  <Document: {'content': '              \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 587}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '53088999a63991935b971bf8e61aa261'}>,\n",
              "  <Document: {'content': '       \"title\": \"PixLore: A Dataset-driven Approach to Rich Image Captioning\",\\n                    \"abstract\": \"In the domain of vision-language integration, generating detailed image captions poses a significant challenge due to the lack of a curated and rich dataset. This study introduces PixLore, a novel method that leverages Querying Transformers through the fine-tuning of the BLIP-2 model using the LoRa method on a standard commercial GPU. Our approach, which involves training on a carefully assembled dataset from state-of-the-art Computer Vision models combined and augmented by ChatGPT, addresses the question of whether intricate image understanding can be achieved with an ensemble of smaller-scale models. Comparative evaluations against major models such as GPT-4 and Google Bard demonstrate that PixLore-2.7B, despite having considerably fewer parameters, is rated higher than the existing State-of-the-Art models in over half of the assessments. This research not only presents a groundbreaking approach but also highlights the importance of well-curated datasets in enhancing the performance of smaller models.\",\\n                    \"openAccessPdf\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 588}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a31a1b6a352c29edf6895da1f0634b97'}>,\n",
              "  <Document: {'content': 'null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"PixLore is introduced, a novel method that leverages Querying Transformers through the fine-tuning of the BLIP-2 model using the LoRa method on a standard commercial GPU to address the question of whether intricate image understanding can be achieved with an ensemble of smaller-scale models.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 589}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c0ca75177a42995470921ceb8bcf8cc3'}>,\n",
              "  <Document: {'content': '   \"offset\": 0\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"0fe5c3133ce6eb5edeb13953a79a6b5b1375a1a7\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data\",\\n                    \"abstract\": \"Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 590}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f39feb90b80f2f2f89466628abacc195'}>,\n",
              "  <Document: {'content': 'However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 591}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c20d45077badb23547cb10b77ee1bc8'}>,\n",
              "  <Document: {'content': '      \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition and shows that this model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 6,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 592}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bbdeb45253a3ffb18c6477197947cb38'}>,\n",
              "  <Document: {'content': '        {\\n                    \"paperId\": \"233417aa02de1836a1f6cc3ee9f8235288213448\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Experimental Characterization of a Centrifugal Compressor in Second Quadrant Operation\",\\n                    \"abstract\": \"\\\\n The flow unsteadiness in turbomachines, presenting a characteristic behaviour, requires a deep understanding of the underlying flow fundamentals, a careful analysis through advanced experimental methods and a clear distinction on how the surrounding system might lead to instabilities.\\\\n A sustained deep surge condition might force a compressor into second quadrant operation, which can occur intermittently during surge cycles or continuously as a result of system malfunction or fast ESD with low inertia machines. In this unusual operating area, compressor system wise, the characteristic is often modelled with a simple approach, as a passive, valve-like component. Aerodynamically wise, corrections', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 593}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1387e60c8c5589f035336f8086f331cc'}>,\n",
              "  <Document: {'content': 'are possible accounting for specific phenomena, as incidence losses and blockage.\\\\n In similar researches, a booster is forcing a stable reverse flow, with the instrumentation rotated according to the expected direction. In this study, this is provided by a pressurized tank, representing a tuneable discharge volume as well as process equipment malfunction, i.e., check valve failure or leakage.\\\\n The test matrix covers a range of discharge pressures, flow rates and rotational speeds, both at constant-imposed speed and with no torque applied, at locked or free shaft, to investigate the effect of different parameters and phenomena.\\\\n The results document the characteristics under different operating conditions. The key contribution is the definition of a suitable approach for the collection of relevant and reliable data measurement, which lays an important foundation for further development of dynamic models for compressor system behaviour and allows to properly reduce complexity of surge protection devices, for operation in more favourable conditions.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 594}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36bff9d9802f23a73a384bc1899b8ad4'}>,\n",
              "  <Document: {'content': '           },\\n                {\\n                    \"paperId\": \"59bed17e1a2da93f236e77d2fc03f0709802c837\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Applying deep learning in brain computer interface to classify motor imagery\",\\n                    \"abstract\": \"\\\\u00a0Deep-learning (DL) is a new paradigm in the artificial intelligence field associated with learning structures able to connect directly numeric data with high-level patterns or categories. DL seems to be a suitable technique to deal with computationally challenging Brain Computer Interface (BCI) problems. Following DL strategy, a new modular and self-organized architecture to solve BCI problems is proposed. A pattern recognition system to translate the measured signals in order to establish categories representing thoughts, without previous pre-processing, is developed.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 595}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f269bd9f5af843778870d461153be5c'}>,\n",
              "  <Document: {'content': 'To achieve an easy interpretability of the system internal functioning, a neuro-fuzzy module and a learning methodology are carried out. The whole learning process is based on machine learning. The architecture and the learning method are tested on a representative BCI application to detect and classify motor imagery thoughts. Data is gathered with a low-cost device. Results prove the efficiency and adaptability of the proposed DL architecture where the used classification module (S-dFasArt) exhibits a better behaviour compared with the usual classifiers. Additionally, it employs neuro-fuzzy modules which allow to offer results in a rules format. This improves the interpretability with respect to the black-box description. A DL architecture, going from the raw data to the labels, is proposed. The proposed architecture, based on Adaptive Resonance Theory (ART) and Fuzzy ART modules, performs data processing in a self-organized way. It follows the DL paradigm, but at the same time, it allows an interpretation of the operation stages. Therefore this approach could be called Transparent Deep Learning.\",\\n                    \"openAccessPdf\": null,\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 596}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '699e6e750677fff66b80e4611f257cc1'}>,\n",
              "  <Document: {'content': '      \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A new modular and self- organized architecture, based on Adaptive Resonance Theory and Fuzzy ART modules, performs data processing in a self-organized way that follows the DL paradigm, but at the same time, it allows an interpretation of the operation stages.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"f8e1c77d77b4a5cf1c4f17ec38cc33503bb49aab\",\\n                    \"publicationVenue\": {\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 597}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '72c9f534c4e6e7ee8e65743bfc206ed9'}>,\n",
              "  <Document: {'content': '               \"id\": \"7e9db4a0-67c4-46c4-9b17-d48d100c2cb7\",\\n                        \"name\": \"Nature Physics\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Nat Phys\"\\n                        ],\\n                        \"issn\": \"1745-2473\",\\n                        \"url\": \"http://www.nature.com/nphys/\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 598}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b049c2281d0816f293a35c7e31b17870'}>,\n",
              "  <Document: {'content': '                   \"alternate_urls\": [\\n                            \"http://www.nature.com/nphys/archive/index.html\",\\n                            \"http://www.nature.com/nphys/index.html\"\\n                        ]\\n                    },\\n                    \"title\": \"Overcoming leakage in quantum error correction\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": {\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 599}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '50b59085ebcb18d34b3c8b78c5c7f430'}>,\n",
              "  <Document: {'content': '            \"url\": \"https://www.nature.com/articles/s41567-023-02226-w.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work demonstrates a distance-3 surface code and distance-21 bit-flip code on a quantum processor for which leakage is removed from all qubits in each cycle, and reports a tenfold reduction in the steady-state leakage population of the data qubits encoding the logical state.\"\\n                    }\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 600}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e37e4eb85ca0457ef19dc3eba135570'}>,\n",
              "  <Document: {'content': '        }\\n            ]\\n        }\\n    ],\\n    \"Daphne Ippolito\": [\\n        {\\n            \"total\": 80,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"48362b169a235ca650918c489c8cea4c597da645\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 601}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7aee6896e235e0ef99f61f58dbef8a1c'}>,\n",
              "  <Document: {'content': '     \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 602}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7d072e1da7ba9c885da856c34a069b52'}>,\n",
              "  <Document: {'content': '   \"abstract\": \"Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST$^{EM}$, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST$^{EM}$ scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 603}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2bc9a13d22700f229e7eef1bf7b1a0aa'}>,\n",
              "  <Document: {'content': '                \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, it is found that ReST$^{EM}$ scales favorably with model size and significantly surpasses fine-tuning only on human data.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"38fe8f324d2162e63a967a9ac6648974fc4c66f3\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fc0a208c-acb7-47dc-a0d4-af8190e21d29\",\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 604}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bc1b9ad117853eb4a2f9ab76eae50b5d'}>,\n",
              "  <Document: {'content': '              \"name\": \"International Conference on Machine Learning\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ICML\",\\n                            \"Int Conf Mach Learn\"\\n                        ],\\n                        \"url\": \"https://icml.cc/\"\\n                    },\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 605}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '41050032723392538f50c1b155170126'}>,\n",
              "  <Document: {'content': '                 \"title\": \"PaLM-E: An Embodied Multimodal Language Model\",\\n                    \"abstract\": \"Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 606}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '588c6da70fe68f2178bd754c99be2789'}>,\n",
              "  <Document: {'content': 'trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2303.03378\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 607}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '909b9d7d6a894e3ee908ff1926ffb169'}>,\n",
              "  <Document: {'content': 'establish the link between words and percepts to enable general inference in the real world.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"ff5f0c5b6905a8c4b361a625b450e9ab417fa854\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 608}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9acbfe0a8bc9162f58ce1a56b3b6d032'}>,\n",
              "  <Document: {'content': '           \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models\",\\n                    \"abstract\": \"Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs\\' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 609}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3d8f3938e686c76e102fa873de429505'}>,\n",
              "  <Document: {'content': 'abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia\\'s Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 610}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '266e674cdda1bd5d83d5ae825ab88372'}>,\n",
              "  <Document: {'content': '             \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"MEDITRON builds on Llama-2, and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines, to improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 46735,\\n            \"offset\": 0,\\n            \"next\": 3,\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 611}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '923a120c21bcbaf3bfc6c913ff8180af'}>,\n",
              "  <Document: {'content': '\"data\": [\\n                {\\n                    \"paperId\": \"2e965b5d97c2d6fb4af284307735be39283792ba\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"54649c1d-6bcc-4232-9cd1-aa446867b8d0\",\\n                        \"name\": \"USENIX Security Symposium\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"USENIX Secur Symp\"\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 612}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '508555d1f5fb99c187088a8937feb53a'}>,\n",
              "  <Document: {'content': '               ],\\n                        \"url\": \"http://www.usenix.org/events/bytopic/security.html\"\\n                    },\\n                    \"title\": \"Extracting Training Data from Diffusion Models\",\\n                    \"abstract\": \"Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 613}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '71ccebbe463c55d3c97d788a6d5e61e7'}>,\n",
              "  <Document: {'content': 'that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2301.13188\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The results show that diffusion models are much less private than prior generative', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 614}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '785b2a3707fd104c155b14e1bc006512'}>,\n",
              "  <Document: {'content': 'models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"616e2783f7e5f2b879a0d3171a73a31d99189259\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 615}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b1e455508eea5a29bb384089e0189077'}>,\n",
              "  <Document: {'content': '            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter\",\\n                    \"abstract\": \"The pre-trained text-image discriminative models, such as CLIP, has been explored for open-vocabulary semantic segmentation with unsatisfactory results due to the loss of crucial localization information and awareness of object shapes. Recently, there has been a growing interest in expanding the application of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 616}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '75a06c9a25c446c3a31ee97acfbca068'}>,\n",
              "  <Document: {'content': 'generative models from generation tasks to semantic segmentation. These approaches utilize generative models either for generating annotated data or extracting features to facilitate semantic segmentation. This typically involves generating a considerable amount of synthetic data or requiring additional mask annotations. To this end, we uncover the potential of generative text-to-image diffusion models (e.g., Stable Diffusion) as highly efficient open-vocabulary semantic segmenters, and introduce a novel training-free approach named DiffSegmenter. The insight is that to generate realistic objects that are semantically faithful to the input text, both the complete object shapes and the corresponding semantics are implicitly learned by diffusion models. We discover that the object shapes are characterized by the self-attention maps while the semantics are indicated through the cross-attention maps produced by the denoising U-Net, forming the basis of our segmentation results.Additionally, we carefully design effective textual prompts and a category filtering mechanism to further enhance the segmentation results. Extensive experiments on three benchmark datasets show that the proposed DiffSegmenter achieves impressive results for open-vocabulary semantic segmentation.\",\\n                    \"openAccessPdf\": {\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 617}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1c770dfdc0c5fe0912136c0ce2af42e0'}>,\n",
              "  <Document: {'content': '            \"url\": \"https://arxiv.org/pdf/2309.02773\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The potential of generative text-to-image diffusion models as highly efficient open-vocabulary semantic segmenters are uncovered, and a novel training-free approach named DiffSegmenter is introduced that achieves impressive results for open-vocabulary semantic segmentation.\"\\n                    }\\n                },\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 618}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6933e78cf65073ccb0b1c0d2facf25b'}>,\n",
              "  <Document: {'content': '            {\\n                    \"paperId\": \"9a768e60bd56c3d8ac2c4a76aac58561d056a786\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 619}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '674d6ba82118c6860f9702cb327488cf'}>,\n",
              "  <Document: {'content': '     \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation\",\\n                    \"abstract\": \"Volume-wise labeling in 3D medical images is a time-consuming task that requires expertise. As a result, there is growing interest in using semi-supervised learning (SSL) techniques to train models with limited labeled data. However, the challenges and practical applications extend beyond SSL to settings such as unsupervised domain adaptation (UDA) and semi-supervised domain generalization (SemiDG). This work aims to develop a generic SSL framework that can handle all three settings. We identify two main obstacles to achieving this goal in the existing SSL framework: 1) the weakness of capturing distribution-invariant features; and 2) the tendency for unlabeled data to', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 620}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'af75b21093615fa06491fadcaa27c52'}>,\n",
              "  <Document: {'content': 'be overwhelmed by labeled data, leading to over-fitting to the labeled data during training. To address these issues, we propose an Aggregating&Decoupling framework. The aggregating part consists of a Diffusion encoder that constructs a common knowledge set by extracting distribution-invariant features from aggregated information from multiple distributions/domains. The decoupling part consists of three decoders that decouple the training process with labeled and unlabeled data, thus avoiding over-fitting to labeled data, specific domains and classes. We evaluate our proposed framework on four benchmark datasets for SSL, Class-imbalanced SSL, UDA and SemiDG. The results showcase notable improvements compared to state-of-the-art methods across all four settings, indicating the potential of our framework to tackle more challenging SSL scenarios. Code and models are available at: https://github.com/xmed-lab/GenericSSL.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 621}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c9de2f35b6e4f355b9f2efdfbc7f8703'}>,\n",
              "  <Document: {'content': '            \"text\": \"This work aims to develop a generic SSL framework that can handle all three settings and demonstrates notable improvements compared to state-of-the-art methods across all four settings, indicating the potential of the framework to tackle more challenging SSL scenarios.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 59,\\n            \"offset\": 0,\\n            \"next\": 3,\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 622}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e2fb8a37ea2e2b39226a65859a11d60f'}>,\n",
              "  <Document: {'content': '        \"data\": [\\n                {\\n                    \"paperId\": \"fc7ee1828030a818f52518022a39f6a3ada60222\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 623}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6d60d9fcd5f305edaa5609008b1c7e64'}>,\n",
              "  <Document: {'content': '                  \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Scalable Extraction of Training Data from (Production) Language Models\",\\n                    \"abstract\": \"This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 624}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f19886b7c288cf8462427c99f59b7043'}>,\n",
              "  <Document: {'content': 'chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"In order to attack the aligned ChatGPT, a new divergence attack is developed that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly.\"\\n                    }\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 625}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e59200a670e4c1f7f3944da085def52d'}>,\n",
              "  <Document: {'content': '},\\n                {\\n                    \"paperId\": \"97010556749971d3e54039edb26fd47c713a735c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 626}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7ac684153cab384bf2bce812acdc0fc1'}>,\n",
              "  <Document: {'content': '                         \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 627}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4876f04fb4e649baa26979931cde59fc'}>,\n",
              "  <Document: {'content': '             \"abstract\": \"Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 628}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36fce62b45cb54a2c55b18f938631b49'}>,\n",
              "  <Document: {'content': 'length. Our code is availabel at https://github.com/thu-coai/Targeted-Data-Extraction.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2307.04401\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A method named Ethicist is proposed for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 629}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2492d53b72435b663b213d1d9ee4df1f'}>,\n",
              "  <Document: {'content': 'a prefix.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"dd2c5d5aa0166944ae0794e389760d5adbad7178\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 630}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c457203e9d7d651b77c97fa0bcd6b8c'}>,\n",
              "  <Document: {'content': ' \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 631}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bbc902f86a7e0f33c8a64f3c9ec40120'}>,\n",
              "  <Document: {'content': '                \"title\": \"S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction\",\\n                    \"abstract\": \"Current relation extraction methods suffer from the inadequacy of large-scale annotated data.While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data.We then propose an accompanied two-stage self-training algorithm that iteratively and alternately learns from synthetic and golden data together.We conduct comprehensive experiments and detailed ablations on popular relation extraction datasets to demonstrate the effectiveness of the proposed framework.\",\\n                    \"openAccessPdf\": {\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 632}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b298e9fbd6bc92d4eeb949b2c56a0358'}>,\n",
              "  <Document: {'content': '     \"url\": \"https://aclanthology.org/2023.acl-long.455.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction, which first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data.\"\\n                    }\\n                }\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 633}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1bcb557154aef7e04dac5693c803b8c2'}>,\n",
              "  <Document: {'content': '         ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"1567bcac0ab09269c9d0ff33c9a406132417fab9\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 634}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7851723dd107facf10a8d704fc03e6b0'}>,\n",
              "  <Document: {'content': '                         \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"A Pretrainer\\'s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity\",\\n                    \"abstract\": \"Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 635}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '197746b32d199831285b52a469a5deac'}>,\n",
              "  <Document: {'content': 'guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.\",\\n                    \"openAccessPdf\": {\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 636}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b823d0c13ffea9b28e29f276b9cd76b4'}>,\n",
              "  <Document: {'content': '                 \"url\": \"http://arxiv.org/pdf/2305.13169\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which are hoped to help support more informed data-centric decisions in LM development.\"\\n                    }\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 637}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '109681165240859c5cfe3faf7b67980c'}>,\n",
              "  <Document: {'content': ' }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"5698d1f43d2ee9fafbfab839706fd51585dd0fed\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 638}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '59200cd9e6b41aa90a7242e44e53e3ec'}>,\n",
              "  <Document: {'content': '         \"publicationVenue\": {\\n                        \"id\": \"75d7a8c1-d871-42db-a8e4-7cf5146fdb62\",\\n                        \"name\": \"Social Science Research Network\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"SSRN, Social Science Research Network (SSRN) home page\",\\n                            \"SSRN Electronic Journal\",\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 639}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e05947e64a39661878500099ada0190c'}>,\n",
              "  <Document: {'content': '      \"Soc Sci Res Netw\",\\n                            \"SSRN\",\\n                            \"SSRN Home Page\",\\n                            \"SSRN Electron J\",\\n                            \"Social Science Electronic Publishing presents Social Science Research Network\"\\n                        ],\\n                        \"issn\": \"1556-5068\",\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 640}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ae0b6aa3223b9028926b2e0c3669f67'}>,\n",
              "  <Document: {'content': '      \"url\": \"http://www.ssrn.com/\",\\n                        \"alternate_urls\": [\\n                            \"www.ssrn.com/\",\\n                            \"https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e\",\\n                            \"https://www.wikidata.org/wiki/Q53949192\",\\n                            \"www.ssrn.com/en\",\\n                            \"http://www.ssrn.com/en/\",\\n                           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 641}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7929e30145dea0d42487b560b4aceaae'}>,\n",
              "  <Document: {'content': '\"http://umlib.nl/ssrn\",\\n                            \"umlib.nl/ssrn\"\\n                        ]\\n                    },\\n                    \"title\": \"Report of the 1st Workshop on Generative AI and Law\",\\n                    \"abstract\": \"This report presents the takeaways of the inaugural Workshop on Generative AI and Law (GenLaw), held in July 2023. A cross-disciplinary group of practitioners and scholars from computer science and law convened to discuss the technical, doctrinal, and policy challenges presented by law for Generative AI, and by Generative AI for law, with an emphasis on U.S. law in particular. We begin the report with a high-level statement about why Generative AI is both immensely significant and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 642}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a5d00fffcbf8f31dc02ed0dd5797e85'}>,\n",
              "  <Document: {'content': 'immensely challenging for law. To meet these challenges, we conclude that there is an essential need for 1) a shared knowledge base that provides a common conceptual language for experts across disciplines; 2) clarification of the distinctive technical capabilities of generative-AI systems, as compared and contrasted to other computer and AI systems; 3) a logical taxonomy of the legal issues these systems raise; and, 4) a concrete research agenda to promote collaboration and knowledge-sharing on emerging issues at the intersection of Generative AI and law. In this report, we synthesize the key takeaways from the GenLaw workshop that begin to address these needs. All of the listed authors contributed to the workshop upon which this report is based, but they and their organizations do not necessarily endorse all of the specific claims in this report.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 643}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '26c4f1fb2ab5f97bec12011265d0cd9f'}>,\n",
              "  <Document: {'content': '\"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"There is an essential need for a shared knowledge base that provides a common conceptual language for experts across disciplines to meet the challenges presented by law for Generative AI, and by GenerativeAI for law, with an emphasis on U.S. law in particular.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 644}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a6abae2a0da210aa2304614e6b7fbe4a'}>,\n",
              "  <Document: {'content': '       \"paperId\": \"03fb535de5cfcf435705a079334ac60f501226ab\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"8648a277-d0ec-4691-9eed-399b31ff9860\",\\n                        \"name\": \"International Conference on Natural Language Generation\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Int Conf Nat Lang Gener\",\\n                            \"INLG\"\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 645}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6f62c206102dfaab12c6106cc97fa40d'}>,\n",
              "  <Document: {'content': '                  ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=1613\"\\n                    },\\n                    \"title\": \"Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\",\\n                    \"abstract\": \"Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 646}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '176585346ff3bfbafb5f59391ef18f43'}>,\n",
              "  <Document: {'content': 'truncate a model\\\\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2309.04858\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling)', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 647}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b5bb66ab883de4d70ee682cab39b39'}>,\n",
              "  <Document: {'content': 'are presented, which has implications for detecting generated text.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 8,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"c38d7f05cf71b9e98f959f4d727069143fbf8f02\",\\n                    \"publicationVenue\": {\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 648}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b23825f0c456d726d67b929e957619f'}>,\n",
              "  <Document: {'content': '      \"id\": \"75d7a8c1-d871-42db-a8e4-7cf5146fdb62\",\\n                        \"name\": \"Social Science Research Network\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"SSRN, Social Science Research Network (SSRN) home page\",\\n                            \"SSRN Electronic Journal\",\\n                            \"Soc Sci Res Netw\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 649}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8461a68a723b5df4d0db8dfe331ea66'}>,\n",
              "  <Document: {'content': '         \"SSRN\",\\n                            \"SSRN Home Page\",\\n                            \"SSRN Electron J\",\\n                            \"Social Science Electronic Publishing presents Social Science Research Network\"\\n                        ],\\n                        \"issn\": \"1556-5068\",\\n                        \"url\": \"http://www.ssrn.com/\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 650}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e47c2665b76d62f70a2b5d17b5fa529'}>,\n",
              "  <Document: {'content': '   \"alternate_urls\": [\\n                            \"www.ssrn.com/\",\\n                            \"https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e\",\\n                            \"https://www.wikidata.org/wiki/Q53949192\",\\n                            \"www.ssrn.com/en\",\\n                            \"http://www.ssrn.com/en/\",\\n                            \"http://umlib.nl/ssrn\",\\n                           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 651}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '16e582819a591aed553e98884e80e1f9'}>,\n",
              "  <Document: {'content': '\"umlib.nl/ssrn\"\\n                        ]\\n                    },\\n                    \"title\": \"AI and Law: The Next Generation\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                },\\n                {\\n                    \"paperId\": \"37113eb04f1d6011659ee53a5480c0db95b130b9\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 652}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c8632f1ac10e97efcf4fd439781e766d'}>,\n",
              "  <Document: {'content': '      \"publicationVenue\": {\\n                        \"id\": \"3dbf084c-ef47-4b74-9919-047b40704538\",\\n                        \"name\": \"Italian National Conference on Sensors\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"SENSORS\",\\n                            \"IEEE Sens\",\\n                            \"Ital National Conf Sens\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 653}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e56c14fab1c7762de73e8652540cbbf9'}>,\n",
              "  <Document: {'content': '                           \"IEEE Sensors\",\\n                            \"Sensors\"\\n                        ],\\n                        \"issn\": \"1424-8220\",\\n                        \"url\": \"http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001\",\\n                        \"alternate_urls\": [\\n                            \"http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 654}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd93a6ae01c223c7751d520545be733f'}>,\n",
              "  <Document: {'content': '           \"http://www.mdpi.com/journal/sensors\",\\n                            \"https://www.mdpi.com/journal/sensors\"\\n                        ]\\n                    },\\n                    \"title\": \"Novel Speech Recognition Systems Applied to Forensics within Child Exploitation: Wav2vec2.0 vs. Whisper\",\\n                    \"abstract\": \"The growth in online child exploitation material is a significant challenge for European Law Enforcement Agencies (LEAs). One of the most important sources of such online information corresponds to audio material that needs to be analyzed to find evidence in a timely and practical manner. That is why LEAs require a next-generation AI-powered platform to process audio data from online sources. We propose', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 655}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ee4fcf8bf1b9ef00ae433ed0d46c175'}>,\n",
              "  <Document: {'content': 'the use of speech recognition and keyword spotting to transcribe audiovisual data and to detect the presence of keywords related to child abuse. The considered models are based on two of the most accurate neural-based architectures to date: Wav2vec2.0 and Whisper. The systems were tested under an extensive set of scenarios in different languages. Additionally, keeping in mind that obtaining data from LEAs are very sensitive, we explore the use of federated learning to provide more robust systems for the addressed application, while maintaining the privacy of the data from LEAs. The considered models achieved a word error rate between 11% and 25%, depending on the language. In addition, the systems are able to recognize a set of spotted words with true-positive rates between 82% and 98%, depending on the language. Finally, federated learning strategies show that they can maintain and even improve the performance of the systems when compared to centralized trained models. The proposed systems set the basis for an AI-powered platform for automatic analysis of audio in the context of forensic applications of child abuse. The use of federated learning is also promising for the addressed scenario, where data privacy is an important issue to be managed.\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 656}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5feb13c11d84d0e3f7f6ffee39f94ac'}>,\n",
              "  <Document: {'content': '                   \"openAccessPdf\": {\\n                        \"url\": \"https://www.mdpi.com/1424-8220/23/4/1843/pdf?version=1675769633\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The use of speech recognition and keyword spotting to transcribe audiovisual data and to detect the presence of keywords related to child abuse and the use of federated learning to provide more robust systems for the addressed application, while', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 657}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e9029a73e319ad193e8d2a205b7a58e'}>,\n",
              "  <Document: {'content': 'maintaining the privacy of the data from LEAs is explored.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"85d17277c2f8f15ff820d83e39fad2a56bc6253e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"f1c5ab66-e090-4fac-abc2-5ae31b06c484\",\\n                        \"name\": \"International Symposium on Security in Computing and Communications\",\\n                        \"type\": \"conference\",\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 658}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a6db63f4e8ed614eab4c38e77236b3c8'}>,\n",
              "  <Document: {'content': '         \"alternate_names\": [\\n                            \"SSCC\",\\n                            \"A-SSCC\",\\n                            \"Asian Solid-State Circuits Conference\",\\n                            \"Int Symp Secur Comput Commun\",\\n                            \"Asian Solid-state Circuit Conf\"\\n                        ]\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 659}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34d5e1ee3617236c62220b078df4fd13'}>,\n",
              "  <Document: {'content': '    },\\n                    \"title\": \"Semiconductor Chip Design in a Legoland\",\\n                    \"abstract\": \"Moore\\'s Law and Dennard scaling have been ticking and propelling the semiconductor industry for decades. They have also pushed the digitization and pixelization of our everyday life. The industry has attained a great achievement in terms of Power consumption, Performance, silicon Area, chip and system Cost as well as Time to market (PPACT). Unfortunately, the pace and benefits of CMOS scaling have now slowed down, yet the explosive demand for chip PPACT is on a completely opposite trajectory due to the rise of Generative AI (GAI), next-generation wireline and wireless (5G-Advanced and 6G) communication, automotive electronics and so forth. On the energy supply side, the worldwide electricity generation increasing at ~6%/year is far less than the growth of the power consumption for just computing alone [1] making the supply vs demand unsustainable. Emerging technologies, such as new material, compound semiconductor devices, and novel circuits and systems both in the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 660}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'deac8d33e0309da58b39da0c25ebbdd6'}>,\n",
              "  <Document: {'content': \"Cloud and at the edge are coming to the rescue. But splitting functions then stitching diverse chiplets together is still at the dawn of the new homogeneous and heterogeneous integration (HI) era. It should also be noted that HI should not be limited to semiconductor chips in a package. Some promising progress will be illustrated ranging from electrical, optical, and passive component integration to support co-packaged optics (CPO), Antenna-in-Package/Module (AiP/AiM), socket waveguides, etc. While the advance in devices and manufacturing is essential, there are lots of innovations needed for architectures, systems, algorithms, and circuits to develop the huge variety of applications. Tools like Computer-Aided Design (CAD), Electronic Design Automation (EDA) and automatic software code generation are supposed to help human engineers to find optimal solutions in a timely manner. However, the pace of enhancement for those tools is around an order of magnitude slower than the Moore's Law. The emergence of machine learning and inference, commonly called ML or AI, is showing a great potential [2], [3] especially when the engineers need to deal with high-dimensional complex options. In fact, many designs assisted by ML have surpassed the PPACT achieved by human engineers using conventional software tools. Following the new\", 'content_type': 'text', 'score': None, 'meta': {'_split_id': 661}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'db62cd0a001ea7b55f4349043304619b'}>,\n",
              "  <Document: {'content': 'large AI foundation model framework, \\\\u201cChipGPT\\\\u201d could become an essential tool in the future. So far, we have only cracked the tip of the iceberg for all the aforementioned fronts. A lot more challenges and opportunities still lie ahead to be explored.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Following the new large AI foundation model framework, \\\\u201cChipGPT\\\\u201d could become an essential tool in the future for architectures, systems, algorithms, and circuits to develop the huge variety of applications.\"\\n                    }\\n                }\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 662}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2fe4465bcd0088505f14eb55301f56d2'}>,\n",
              "  <Document: {'content': '          ]\\n        }\\n    ],\\n    \"Lori Levin\": [\\n        {\\n            \"total\": 2,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"7a08051aac75a809737096e39820bf836908d4e1\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Construction Grammar Provides Unique Insight into Neural Language Models\",\\n                    \"abstract\": \"Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 663}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a1667212905712d7dc1f778b01ab925e'}>,\n",
              "  <Document: {'content': 'performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2302.02178\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 664}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e55772fed0f81428a42332c11a5189ca'}>,\n",
              "  <Document: {'content': '                 \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The view of the most important challenges and research questions that this promising new field of research faces is provided, and an analysis of selected previous work in detail is provided.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"c68afb94a0857681e4cbeb2745d445e1f0399b66\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"67128bf0-cf99-46bd-8915-1d9a05820d0a\",\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 665}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9732df94e524f2fb571869e33d3fe21'}>,\n",
              "  <Document: {'content': '                \"name\": \"International Conference on Information Technology\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Int Conf Inf Technol\",\\n                            \"International Conference on Information Technologies\",\\n                            \"IVUS\"\\n                        ]\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 666}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff792a27434f987de2eb8cc16971736c'}>,\n",
              "  <Document: {'content': '      },\\n                    \"title\": \"Fine-Tuning Language Models for Emotion Recognition in Lithuanian Texts Using Neural Machine Translation of Training datasets\",\\n                    \"abstract\": \"Lithuanian language is a complex and rich language with a unique grammar structure, making it an interesting choice for natural language processing (NLP) tasks such as emotion detection. This study provides helpful insights into the emotional nuances of Lithuanian texts by utilizing a translated and augmented emotion dataset. We present a methodology that leverages translated datasets for emotion recognition and augmentation approaches to improve the performance of emotion identification models in low-resource languages. We compared the outcomes of transformer-based language models, such as RoBERTa, LaBSE, and LitLat BERT on the translated and augmented data. Our results demonstrated that LitLat BERT, which is primarily trained on Lithuanian texts, showed the most significant improvement in performance when data augmentation was applied. We conclude that LitLat BERT could be the preferred choice for emotion recognition tasks for Lithuanian language due', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 667}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ba19f3bae184c39de1ff876805652c6'}>,\n",
              "  <Document: {'content': 'to its specialized training and enhanced adaptability when provided with diverse and augmented data. This study provides valuable insights into the challenges and potential solutions for emotion identification tasks in morphology-rich languages, like Lithuanian language.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is concluded that LitLat BERT could be the preferred choice for emotion recognition tasks for Lithuanian language due to its specialized training and enhanced adaptability when provided with diverse and augmented data.\"\\n                    }\\n                }\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 668}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'abd885d6f778d3be02280b5f37cb8f96'}>,\n",
              "  <Document: {'content': '      ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"659be1ff350634f50cc066d258ee6a45e697e552\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"56b19d19-ddf9-4e56-ba1e-37ef38ef5943\",\\n                        \"name\": \"Special Interest Group on Computational Morphology and Phonology Workshop\",\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 669}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4cb332c82d5bfffc95fde8c43e50a051'}>,\n",
              "  <Document: {'content': ' \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"SIGMORPHON\",\\n                            \"Sp\\\\u00e9c Interest Group Comput Morphol Phonol Workshop\"\\n                        ],\\n                        \"url\": \"http://sigmorphon.org/\"\\n                    },\\n                    \"title\": \"SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 670}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e3c194a2e4b1f59202ce51c789862c2'}>,\n",
              "  <Document: {'content': '        \"abstract\": \"In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.sigmorphon-1.22.pdf\",\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 671}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ef777a9bdeeb8a82799ae9d231791ed2'}>,\n",
              "  <Document: {'content': '                 \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.\"\\n                    }\\n                }\\n            ]\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 672}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7bb48b5f8826c90fef6757eceb06a6a'}>,\n",
              "  <Document: {'content': '},\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"bf42c0462d1415cdde877c90d58da11545407b8a\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"56b19d19-ddf9-4e56-ba1e-37ef38ef5943\",\\n                        \"name\": \"Special Interest Group on Computational Morphology and Phonology Workshop\",\\n                        \"type\": \"conference\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 673}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '88238f3709f1473e7acc98eaa78af9ec'}>,\n",
              "  <Document: {'content': '            \"alternate_names\": [\\n                            \"SIGMORPHON\",\\n                            \"Sp\\\\u00e9c Interest Group Comput Morphol Phonol Workshop\"\\n                        ],\\n                        \"url\": \"http://sigmorphon.org/\"\\n                    },\\n                    \"title\": \"Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\",\\n                    \"abstract\": \"Interlinear glossing', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 674}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6cc796e51e5cd6855ea2c0b9871f1b94'}>,\n",
              "  <Document: {'content': 'provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation convention\\\\u00e2\\\\u20ac\\\\u201dGeneralized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.sigmorphon-1.7.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 675}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ee22a1eb9d27b22a6ecb02d34a093506'}>,\n",
              "  <Document: {'content': '              \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"An annotation convention is proposed that combines all of these positive properties using an Item-and-Process (IP) framework, and its linguistic adequacy is demonstrated, and it is compared with two other interlinear glossed text annotation schemes.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 676}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe6390653e212aae5711bf18f73eff5b'}>,\n",
              "  <Document: {'content': ' \"data\": [\\n                {\\n                    \"paperId\": \"c5207241406586f4263b235667e004b71ea68953\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Syntax and Semantics Meet in the \\\\u201cMiddle\\\\u201d: Probing the Syntax-Semantics Interface of LMs Through Agentivity\",\\n                    \"abstract\": \"Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms\\\\u2014i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 677}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0ad8b5c15adb0a8958daf2eb95739c4'}>,\n",
              "  <Document: {'content': 'was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2305.18185\",\\n                        \"status\": \"GREEN\"\\n                    },\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 678}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9d3ac378eee4c72b3fb8382a1832eeab'}>,\n",
              "  <Document: {'content': '    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far and suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        }\\n    ],\\n    \"Lei', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 679}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bd2545cac663af736a0c3849a80c1421'}>,\n",
              "  <Document: {'content': 'Li\": [\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"8aa98fbfb6f1e979dead13ce24075503fe47658e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 680}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'aee1916366b3991926f04f5aadff66a4'}>,\n",
              "  <Document: {'content': '         \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"A Survey for In-context Learning\",\\n                    \"abstract\": \"With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples. It has been a new trend exploring ICL to evaluate and extrapolate the ability of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 681}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '868d77dbe8a5e0129df64d18f9f3bc73'}>,\n",
              "  <Document: {'content': 'LLMs. In this paper, we aim to survey and summarize the progress, challenges, and future work in ICL. We \\\\ufb01rst present a formal de\\\\ufb01nition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques of ICL, including training strategies, prompting strategies, and so on. Finally, we present the challenges of ICL and provide potential directions for further research. We hope our work can encourage more research on uncovering how ICL works and improving ICL in future work. 1\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2301.00234\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 682}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3d05cdbeb4fb73af3f471fcaed61e7b8'}>,\n",
              "  <Document: {'content': '                   \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The progress, challenges, and future work in ICL are summarized and a formal definition of ICL is presented and its correlation to related studies are clarified and potential directions for further research are provided.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 18,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 683}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8faf528656ed889623e24d3993b5902f'}>,\n",
              "  <Document: {'content': '          {\\n                    \"paperId\": \"dfd8944d39b378489b878d6e105d040fa0e524db\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 684}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff2ea5bc12b845fb1625e4ca2352ef9f'}>,\n",
              "  <Document: {'content': '   \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\",\\n                    \"abstract\": \"Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs\\' performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually improving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 685}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2f303d09ba446fe4382b0cbc2e9d8980'}>,\n",
              "  <Document: {'content': 'languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, instruction semantics can surprisingly be ignored when given in-context exemplars. Second, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Third, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2304.04675\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 686}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e3b0508efd5570003b837d07ed1da56c'}>,\n",
              "  <Document: {'content': '                       \"text\": \"It is discovered that LLMs exhibit new working patterns when used for MMT and cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"eda54452d8a8a412c2a985ef11572cb468906b1f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 687}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ca89ee650a16cdb725b27bd99f8d218f'}>,\n",
              "  <Document: {'content': 'Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 688}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b2f4eaa3f9e48ac099c315ac6d182bc8'}>,\n",
              "  <Document: {'content': '     ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"Multilingual Large Language Models Are Not (Yet) Code-Switchers\",\\n                    \"abstract\": \"Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 689}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b8c9a9cd76d702a97a2dbebb7623a1d'}>,\n",
              "  <Document: {'content': 'zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current\\\\\"multilingualism\\\\\"in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.14235\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 690}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '89ed7ae2f64404965b21f225244b4d60'}>,\n",
              "  <Document: {'content': 'argued that current\\\\\"multilingualism\\\\\" in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"e39e809fa8be75d0aa650351e230694ce3396ab5\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 691}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e7ecb408c16785a8f1ff3a7295b2948'}>,\n",
              "  <Document: {'content': '                    \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 692}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4c555f441b8f0f2ebd2823b32ffe4c05'}>,\n",
              "  <Document: {'content': '                },\\n                    \"title\": \"DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages\",\\n                    \"abstract\": \"Disfluency correction (DC) is the process of removing disfluent elements like fillers, repetitions and corrections from spoken utterances to create readable and interpretable text. DC is a vital post-processing step applied to Automatic Speech Recognition (ASR) outputs, before subsequent processing by downstream language understanding tasks. Existing DC research has primarily focused on English due to the unavailability of large-scale open-source datasets. Towards the goal of multilingual disfluency correction, we present a high-quality human-annotated DC corpus covering four important Indo-European languages: English, Hindi, German and French. We provide extensive analysis of results of state-of-the-art DC models across all four languages obtaining F1 scores of 97.55 (English), 94.29 (Hindi), 95.89 (German) and 92.97 (French). To demonstrate the benefits of DC on downstream tasks, we show that DC leads to 5.65 points increase', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 693}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3c8dc2ab86b7317a00deb2d54181a75f'}>,\n",
              "  <Document: {'content': 'in BLEU scores on average when used in conjunction with a state-of-the-art Machine Translation (MT) system. We release code to run our experiments along with our annotated dataset here.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work presents a high-quality human-annotated DC corpus covering four important Indo-European languages: English, Hindi, German and French, and provides extensive analysis of results of state-of-the-art DC models across all four languages.\"\\n                    }\\n                }\\n            ]\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 694}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cd770768a1fd6cc34df47c96d835d63a'}>,\n",
              "  <Document: {'content': '       },\\n        {\\n            \"total\": 22,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"c25d2a27f1abe169d7b68078071b6698f0980469\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fc0a208c-acb7-47dc-a0d4-af8190e21d29\",\\n                        \"name\": \"International Conference on Machine Learning\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 695}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '88f69a6554cc16930b31c43c695300aa'}>,\n",
              "  <Document: {'content': '   \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ICML\",\\n                            \"Int Conf Mach Learn\"\\n                        ],\\n                        \"url\": \"https://icml.cc/\"\\n                    },\\n                    \"title\": \"Protecting Language Generation Models via Invisible Watermarking\",\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 696}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '33d058a5c19e453981cc7fb76690c2fb'}>,\n",
              "  <Document: {'content': '    \"abstract\": \"Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as\\\\\"synonym randomization\\\\\". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 697}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90a76be32f0ccf75765ca41ae727e9b6'}>,\n",
              "  <Document: {'content': '      \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2302.03162\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"GINSEW, a novel method to protect text generation models from being stolen through distillation by injecting secret signals into the probability vector of the decoding steps for each target token, is proposed.\"\\n                    }\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 698}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '992f412261134dfd65f98347dac376bc'}>,\n",
              "  <Document: {'content': '               },\\n                {\\n                    \"paperId\": \"d4c3e3e3c01afed15926adf81527bf46aa491c6a\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 699}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '69ddbce3a0498950142648c9e7dd9005'}>,\n",
              "  <Document: {'content': '        \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"Are You Copying My Model?', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 700}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ae57281c2717e0fd4292b43a1c945fe'}>,\n",
              "  <Document: {'content': 'Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark\",\\n                    \"abstract\": \"Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro \\\\u2018METHOD\\\\u2019} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer\\\\u2019s model for copyright verification while minimizing the adverse impact', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 701}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f4485b304790cbe5f9a5fa8e0410e070'}>,\n",
              "  <Document: {'content': 'on the original embeddings\\\\u2019 utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.Our code is available at https://github.com/yjw1029/EmbMarker.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.10036\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"An Embedding Watermark method called {pasted macro', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 702}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62fd006194b4ec9c04a9885216f03d50'}>,\n",
              "  <Document: {'content': '\\\\u2018METHOD\\\\u2019} that implants backdoors on embeddings that can effectively protect the copyright of EaaS models without compromising service quality is proposed.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"3b913c24a50528bb724be59a797f4576ce17d0f9\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 703}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e1badf0f0ab72d191e88c297e1f37058'}>,\n",
              "  <Document: {'content': '                 \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring\",\\n                    \"abstract\": \"The strong general capabilities of Large Language Models (LLMs) bring potential ethical risks if they are unrestrictedly accessible to malicious users. Token-level watermarking inserts watermarks in the generated texts by altering the token probability distributions', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 704}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '32b96ce7d2ba0d03cccdb0ac10f47453'}>,\n",
              "  <Document: {'content': 'with a private random number generator seeded by its prefix tokens. However, this watermarking algorithm alters the logits during generation, which can lead to a downgraded text quality if it chooses to promote tokens that are less relevant given the input. In this work, we propose to improve the quality of texts generated by a watermarked language model by Watermarking with Importance Scoring (WIS). At each generation step, we estimate the importance of the token to generate, and prevent it from being impacted by watermarking if it is important for the semantic correctness of the output. We further propose three methods to predict importance scoring, including a perturbation-based method and two model-based methods. Empirical experiments show that our method can generate texts with better quality with comparable level of detection rate.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 705}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1c2f3d6b0414af886a653806245d128c'}>,\n",
              "  <Document: {'content': '                     \"text\": \"This work proposes to improve the quality of texts generated by a watermarked language model by Watermarking with Importance Scoring (WIS), and proposes three methods to predict importance scoring, including a perturbation-based method and two model-based methods.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 3,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc\",\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 706}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ab7af369a7620bcc7f85e379d4e9c31e'}>,\n",
              "  <Document: {'content': '                  \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 707}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff5e7ee4a0c3e9bc4d14344de4696c30'}>,\n",
              "  <Document: {'content': '                },\\n                    \"title\": \"Provable Robust Watermarking for AI-Generated Text\",\\n                    \"abstract\": \"We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs. Code is available at https://github.com/XuandongZhao/Unigram-Watermark.\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 708}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e6736b5cd4b5933b68ae2ed644b87351'}>,\n",
              "  <Document: {'content': '   \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2306.17439\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A robust and high-quality watermark method, Unigram-Watermark, is proposed by extending an existing approach with a simplified fixed grouping strategy that enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing.\"\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 709}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '55ca93dd06f1b73b2c6480a1b63cf8c3'}>,\n",
              "  <Document: {'content': ' }\\n                },\\n                {\\n                    \"paperId\": \"1c13af186d1e177b85ef1ec3fc7b8d33ec314cfd\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 710}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '49a922be1e5ab57cb7bd0cdebd795836'}>,\n",
              "  <Document: {'content': '  ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense\",\\n                    \"abstract\": \"The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 711}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f2521f27974a143c9535d3f3e56531d1'}>,\n",
              "  <Document: {'content': 'and content reordering. Using DIPPER to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI\\'s text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics. To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.\",\\n                    \"openAccessPdf\": {\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 712}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd55eeb249bcb90a16da920b7e4feb28a'}>,\n",
              "  <Document: {'content': '     \"url\": \"http://arxiv.org/pdf/2303.13408\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider is introduced that can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated.\"\\n                    }\\n                },\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 713}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1b2c1075302b73bde27c16761851d0ef'}>,\n",
              "  <Document: {'content': '               {\\n                    \"paperId\": \"f653569b131176dfed8842694b5ad2ead5e4b923\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 714}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '71df87e9405284bd38fb1a29586f8947'}>,\n",
              "  <Document: {'content': '        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust\",\\n                    \"abstract\": \"Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 715}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cf7bab124afab09151c1068fcfcd9f3d'}>,\n",
              "  <Document: {'content': 'dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at https://github.com/YuxinWenRick/tree-ring-watermark.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2305.20030\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 716}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6383e4dba9b7ba4c3d7521bac3c696fc'}>,\n",
              "  <Document: {'content': '                \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper introduces a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs and demonstrates that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 717}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a8533af5a04fceeedcc7db167cdafd7'}>,\n",
              "  <Document: {'content': '{\\n                    \"paperId\": \"7d722ec75cf4cde30156e71fffec6f8f08f91600\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 718}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5b16cd48b271709e23826651562c1e8'}>,\n",
              "  <Document: {'content': '         \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge\",\\n                    \"abstract\": \"Large language', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 719}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c1658934c29e4d49e141cbc8428ce5a0'}>,\n",
              "  <Document: {'content': 'models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \\\\u201clions don\\\\u2019t live in the ocean\\\\u201d, is also ubiquitous in the world but rarely mentioned explicitly in text.What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge.We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions.We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.05976\",\\n                        \"status\": \"GREEN\"\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 720}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b9042430ca08928d193e425284ea1ef'}>,\n",
              "  <Document: {'content': '         },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Results reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions, and analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 2,\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 721}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58b932c1f1e0cd4befdb55921dddb2f9'}>,\n",
              "  <Document: {'content': '        \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"460609e217fd59eaa34f5e11a820661f8ec8d7b6\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 722}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b83e4bfbd76263f06098947ea6e0fa16'}>,\n",
              "  <Document: {'content': '               \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 723}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ae8824853c293fec74a9331da221c47'}>,\n",
              "  <Document: {'content': '  \"title\": \"INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback\",\\n                    \"abstract\": \"The field of automatic evaluation of text generation made tremendous progress in the last few years. In particular, since the advent of neural metrics, like COMET, BLEURT and SEScore2, the newest generation of metrics show a high correlation with human judgment. Unfortunately, quality scores generated with neural metrics are not interpretable and it is unclear which part of the generation output is criticized by the metrics. To address this limitation, we present I NSTRUCT S CORE , an open-source, explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metric that can produce a diagnostic report aligned with human judgment. We evaluate I NSTRUCT S CORE on the WMT22 Zh-En translation task, where our 7B model surpasses other LLM-based baselines, including those based on 175B GPT3. Impressively, our I NSTRUCT S CORE , even without direct super-vision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 724}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5030f52d608b535c46a2823613acd9f7'}>,\n",
              "  <Document: {'content': 'was fine-tuned on human ratings. 1\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2305.14282\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work fine-tunes a LLAMA model to create an evaluative metric that can produce a diagnostic report aligned with human judgment, and achieves performance levels on par with state-of-the-art metrics like COMET22, which', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 725}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b3f7ff48c4c13efc78ed3d6308419310'}>,\n",
              "  <Document: {'content': 'was fine-tuned on human ratings.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"099f09ccbb7479b686be42b598a6efda751fb324\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback\",\\n                    \"abstract\": \"Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 726}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c424e9502c9b3d202581261fd04c6784'}>,\n",
              "  <Document: {'content': 'human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work fine-tunes a text evaluation metric based on LLaMA, producing both a score for generated text and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 727}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff7840dbeb37c784fe3046c8af12da41'}>,\n",
              "  <Document: {'content': 'a human readable diagnostic report, which achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"159b9c65260beea062cebdc023d9e802869768f3\",\\n                    \"publicationVenue\": {\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 728}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4c87feb96e83b38e88835103dba81ce9'}>,\n",
              "  <Document: {'content': '      \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 729}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f410781b985dc69f05df583d2d88be4'}>,\n",
              "  <Document: {'content': '               \"EMNLP\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"Learning from Mistakes via Cooperative Study Assistant for Large Language Models\",\\n                    \"abstract\": \"Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 730}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19b7c95be302437ed70faecf8296f293'}>,\n",
              "  <Document: {'content': 'phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLM-specific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.emnlp-main.659.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 731}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8fcf85b77c80ee88609c9c8c9b54dcef'}>,\n",
              "  <Document: {'content': '        \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper proposes Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation, and demonstrates that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 732}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ddeb8ed2eb27479e02a25cbed81c059'}>,\n",
              "  <Document: {'content': '     },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"b49ca3af2b038f0fa6bfa87c2296cb669ee5a68a\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fc0a208c-acb7-47dc-a0d4-af8190e21d29\",\\n                      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 733}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c731bcb63f6f8356f2ea50a7e563915'}>,\n",
              "  <Document: {'content': ' \"name\": \"International Conference on Machine Learning\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ICML\",\\n                            \"Int Conf Mach Learn\"\\n                        ],\\n                        \"url\": \"https://icml.cc/\"\\n                    },\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 734}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7d29ed6c5806eb258e0cd451a81a7bc1'}>,\n",
              "  <Document: {'content': '    \"title\": \"Importance Weighted Expectation-Maximization for Protein Sequence Design\",\\n                    \"abstract\": \"Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.\",\\n                    \"openAccessPdf\": {\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 735}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a2d57e1ce050149a1433b4defd146e4b'}>,\n",
              "  <Document: {'content': '                    \"url\": \"http://arxiv.org/pdf/2305.00386\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Experiments show that the proposed IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.\"\\n                    }\\n                }\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 736}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '244a00677987e9d6565f85bad8827bd0'}>,\n",
              "  <Document: {'content': '           ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"9fec5cf2f06e6fd8c5e6f6028226082d1ecec5b7\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 737}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c8c135cc032daf7deec0148a12a7476c'}>,\n",
              "  <Document: {'content': '                           \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Extrapolating Large Language Models to Non-English by Aligning Languages\",\\n                    \"abstract\": \"Existing large language models show disparate capability across different languages, due to the imbalance in the training data. Their performances on English tasks are often stronger than on tasks', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 738}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1f5e76ce212685fea8b802f574df304'}>,\n",
              "  <Document: {'content': 'of other languages. In this paper, we empower pre-trained LLMs on non-English languages by building semantic alignment across languages. We start from targeting individual languages by performing cross-lingual instruction-tuning (CoIT) on LLaMA, i.e. tuning it with translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMAs), and formulate underlying scaling laws to investigate the advantages of using scalable translation data. Then we perform multilingual instruction-tuning (MuIT) with mixed resources to build multilingual m-LLaMA. We also illustrate how we leverage the scaling laws to optimize data allocation in a resource-constrained setting. Experiment results on cross-lingual benchmarks XQUAD and MLQA show that x-LLaMAs surpass the English instruction-tuned counterpart (Alpaca) by an average of 27.83% across six non-English languages. Evaluation results on translation dataset Flores-101 show that x-LLaMAs outperform previous LLaMA-based models by an average of 18.89%. Encouragingly, m-LLaMA achieves comparable performance to x-LLaMAs on individual languages and demonstrates the ability to follow multilingual instructions. Further analysis on response content and representation space reveals the alignment of the multilingual semantic space within the middle layers of m-LLaMA.\",\\n                    \"openAccessPdf\": {\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 739}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c4d28148394ec49b81c1ef18f6559137'}>,\n",
              "  <Document: {'content': '                     \"url\": \"https://arxiv.org/pdf/2308.04948\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper empower pre-trained LLMs on non-English languages by building semantic alignment across languages by performing multilingual instruction-tuning with mixed resources to build multilingual m-LLaMA, and illustrates how the scaling laws are leveraged to optimize data allocation in a resource-constrained setting.\"\\n                    }\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 740}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '238c94268c5aca9a7546668727879467'}>,\n",
              "  <Document: {'content': '              }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"63ed9de4a895e3197180ef2258116a1dcc7fbf2c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 741}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd2e31ec11160f7ff5372e5e6ca1122c6'}>,\n",
              "  <Document: {'content': '             \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Generative Autoencoders as Watermark Attackers: Analyses of Vulnerabilities and Threats\",\\n                    \"abstract\": \"Invisible watermarks safeguard images\\\\u2019 copy-rights by embedding hidden messages detectable by owners. It', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 742}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8ae6406f4070171fd7e48ddf0ced75'}>,\n",
              "  <Document: {'content': 'also prevents people from misusing images, especially those generated by AI models. Malicious adversaries can violate these rights by removing the watermarks. In order to remove watermarks without damaging the visual quality, the adversary needs to erase them while retaining the essential information in the image. This is analogous to the encoding and decoding process of generative autoencoders, especially variational autoencoders (VAEs) and diffusion models. We propose a framework using generative autoen-coders to remove invisible watermarks and test it using VAEs and diffusions. Our results reveal that, even without specific training, off-the-shelf Stable Diffusion effectively removes most watermarks, surpassing all current attackers. The result underscores the vulnerabilities in existing watermarking schemes and calls for more robust methods for copyright protection.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2306.01953\",\\n                        \"status\": \"GREEN\"\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 743}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f2c4678d76a07da9731ab41915ad55bf'}>,\n",
              "  <Document: {'content': '          },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes a framework using generative autoencoders to remove invisible watermarks and test it using VAEs and diffusions, revealing that off-the-shelf Stable Diffusion effectively removes most watermarks, surpassing all current attackers.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1797,\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 744}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '57e4eaee7e44c6707c5cf81e74450f41'}>,\n",
              "  <Document: {'content': ' \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"5df238894c82b0ff8846290f6a660d2a9ee1963c\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 745}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ac80f2d930f6b9cd318ff012bbab62'}>,\n",
              "  <Document: {'content': '             ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Learning Personalized Story Evaluation\",\\n                    \"abstract\": \"While large language models (LLMs) have shown impressive results for more objective tasks such as QA and retrieval, it remains nontrivial to evaluate their performance on open-ended text generation for reasons including (1) data contamination; (2) multi-dimensional evaluation criteria; and (3) subjectiveness stemming from reviewers\\' personal preferences. To address such issues, we propose to model personalization in an uncontaminated open-ended generation assessment. We create two new datasets Per-MPST and Per-DOC for personalized', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 746}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '375cbd2a232c840b28c7aca858678433'}>,\n",
              "  <Document: {'content': 'story evaluation, by re-purposing existing datasets with proper anonymization and new personalized labels. We further develop a personalized story evaluation model PERSE to infer reviewer preferences and provide a personalized evaluation. Specifically, given a few exemplary reviews from a particular reviewer, PERSE predicts either a detailed review or fine-grained comparison in several aspects (such as interestingness and surprise) for that reviewer on a new text input. Experimental results show that PERSE outperforms GPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on pairwise preference prediction accuracy. Both datasets and code will be released.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2310.03304\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 747}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3fa7236c5ca104c5e6886a948055faa8'}>,\n",
              "  <Document: {'content': '      \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A personalized story evaluation model PERSE is developed to infer reviewer preferences and provide a personalized evaluation, and experimental results show that PERSe outperforms GPT-4 on Kendall correlation of story ratings, and by 13.7% on pairwise preference prediction accuracy.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"02c6addec7e934b3d2455c82933659d90f7157ad\",\\n                    \"publicationVenue\": {\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 748}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '71f42d582c757478f21745c73cebda01'}>,\n",
              "  <Document: {'content': '             \"id\": \"d406a3f4-dc05-43be-b1f6-812f29de9c0e\",\\n                        \"name\": \"IEEE Transactions on Information Forensics and Security\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"IEEE Trans Inf Forensics Secur\"\\n                        ],\\n                        \"issn\": \"1556-6013\",\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 749}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3a48131bd93c7fd2874c4463abbad8bb'}>,\n",
              "  <Document: {'content': '\"url\": \"http://www.ieee.org/organizations/society/sp/tifs.html\",\\n                        \"alternate_urls\": [\\n                            \"http://ieeexplore.ieee.org/servlet/opac?punumber=10206\",\\n                            \"http://www.signalprocessingsociety.org/publications/periodicals/forensics/\"\\n                        ]\\n                    },\\n                    \"title\": \"EEFED: Personalized Federated Learning of Execution&Evaluation Dual Network for CPS Intrusion Detection\",\\n                    \"abstract\": \"In the modern interconnected world, intelligent networks and computing technologies are increasingly being incorporated in industrial systems. However, this adoption of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 750}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '73ef1511190c13a99721b3d5786e9c9'}>,\n",
              "  <Document: {'content': 'advanced technology has resulted in increased cyber threats to cyber-physical systems. Existing intrusion detection systems are continually challenged by constantly evolving cyber threats. Machine learning algorithms have been applied for intrusion detection. In these techniques, a classification model is trained by learning cyber behavior patterns. However, these models typically require considerable high-quality datasets. Limited attack samples are available because of the unpredictability and constant evolution of cyber threats. To address these problems, we propose a novel federated Execution & Evaluation dual network framework (EEFED), which allows multiple federal participants to personalize their local detection models undermining the original purpose of Federated Learning. Thus, a general global detection model was developed for collaboratively improving the performance of a single local model against cyberattacks. The proposed personalized update algorithm and the optimizing backtracking parameters replacement policy effectively reduced the negative influence of federated learning in imbalanced and non-i.i.d distribution of data. The proposed method improved model stability. Furthermore, extensive experiments conducted on a network dataset in various cyber scenarios revealed that the proposed method outperformed single model and state-of-the-art methods.\",\\n                    \"openAccessPdf\": {\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 751}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9d39e929177f795275674bca1b7b1afd'}>,\n",
              "  <Document: {'content': '                      \"url\": \"https://ieeexplore.ieee.org/ielx7/10206/9970396/09919869.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A general global detection model was developed for collaboratively improving the performance of a single local model against cyberattacks, and the proposed personalized update algorithm and the optimizing backtracking parameters replacement policy effectively reduced the negative influence of federated learning in imbalanced and non-i.i.d distribution of data.\"\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 752}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '212610be89965585fd306c7d0b3ebca2'}>,\n",
              "  <Document: {'content': '     }\\n                },\\n                {\\n                    \"paperId\": \"048b352a0e90c035c518ca60870b8b3277831a2f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"6c8c2afc-7abf-4f41-a2ea-a673c67ab017\",\\n                        \"name\": \"Statistics in Medicine\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 753}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4afb5f573b9941ff1a9849a4db05224b'}>,\n",
              "  <Document: {'content': '         \"Stat Med\"\\n                        ],\\n                        \"issn\": \"0277-6715\",\\n                        \"url\": \"http://www.interscience.wiley.com/jpages/0277-6715/\",\\n                        \"alternate_urls\": [\\n                            \"http://onlinelibrary.wiley.com/journal/10.1002/%28ISSN%291097-0258\",\\n                            \"http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1097-0258\",\\n                            \"https://onlinelibrary.wiley.com/journal/10970258\"\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 754}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '448c60240c6615d06e8f322f977f6372'}>,\n",
              "  <Document: {'content': '                 ]\\n                    },\\n                    \"title\": \"Variable importance evaluation with personalized odds ratio for machine learning model interpretability with applications to electronic health records\\\\u2010based mortality prediction\",\\n                    \"abstract\": \"The interpretability of machine learning models, even though with an excellent prediction performance, remains a challenge in practical applications. The model interpretability and variable importance for well\\\\u2010performed supervised machine learning models are investigated in this study. With the commonly accepted concept of odds ratio (OR), we propose a novel and computationally efficient Variable Importance evaluation framework based on the Personalized Odds Ratio (VIPOR). It is a model\\\\u2010agnostic interpretation method that can be used to evaluate variable importance both locally and globally. Locally, the variable importance is quantified by the personalized odds ratio (POR), which can account for subject heterogeneity in machine learning.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 755}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cbf688189af481a780065af62d60967e'}>,\n",
              "  <Document: {'content': 'Globally, we utilize a hierarchical tree to group the predictors into five groups: completely positive, completely negative, positive dominated, negative dominated, and neutral groups. The relative importance of predictors within each group is ranked based on different statistics of PORs across subjects for different application purposes. For illustration, we apply the proposed VIPOR method to interpreting a multilayer perceptron (MLP) model, which aims to predict the mortality of subarachnoid hemorrhage (SAH) patients using real\\\\u2010world electronic health records (EHR) data. We compare the important variables derived from MLP with other machine learning models, including tree\\\\u2010based models and the L1\\\\u2010regularized logistic regression model. The top importance variables are consistently identified by VIPOR across different prediction models. Comparisons with existing interpretation methods are also conducted and discussed based on publicly available data sets.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 756}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22580441f6f152488f14b5b2da84eb86'}>,\n",
              "  <Document: {'content': '                    \"text\": \"A novel and computationally efficient Variable Importance evaluation framework based on the Personalized Odds Ratio (VIPOR), a model\\\\u2010agnostic interpretation method that can be used to evaluate variable importance both locally and globally.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 757}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ef12222df0a1ae814beebe5af3c79217'}>,\n",
              "  <Document: {'content': '{\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"be6ea285745955e5f4d095b61047137de2f9726b\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 758}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '53357488648264ed033474d060afaccd'}>,\n",
              "  <Document: {'content': '\"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback\",\\n                    \"abstract\": \"Recent improvements in text generation have leveraged human feedback to improve the quality of the generated output. However, human feedback is not always available, especially during inference. In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location and severity level', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 759}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3a2215ba38e855a0ae2e1cc6d0431a5'}>,\n",
              "  <Document: {'content': 'that are predicted by a learned error pinpoint model for iterative refinement. FITO starts with an initial output, then iteratively incorporates the feedback via a refinement model that generates an improved output conditioned on the feedback. Given the uncertainty of consistent refined samples at iterative steps, we formulate iterative refinement into a local search problem and develop a simulated annealing based algorithm that balances exploration of the search space and optimization for output quality. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at long form QA and topic summarization respectively, with a single iteration of refinement. With our simulated annealing algorithm, we see further quality improvements, including up to 1.7 MetricX improvements over the baseline approach.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n               ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 760}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6d362933b70eb4022d885cfe782b028f'}>,\n",
              "  <Document: {'content': '        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location and severity level that are predicted by a learned error pinpoint model for iterative refinement.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 34,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 761}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c516fd06e171961080352c75f808911'}>,\n",
              "  <Document: {'content': '  {\\n                    \"paperId\": \"0ed0e74d84f2523cdbf498844d00773217f6074f\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 762}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '934db162827a826ac0d31d25200ebbd1'}>,\n",
              "  <Document: {'content': '                    \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"How Multilingual is Multilingual LLM?\",\\n                    \"abstract\": \"Large Language Models (LLMs), trained predominantly on extensive English data, often exhibit limitations when applied to other languages. Current research is primarily focused on enhancing the multilingual capabilities of these models by employing various tuning strategies. Despite their effectiveness in certain languages, the understanding of the multilingual abilities of LLMs remains incomplete. This study endeavors to evaluate the multilingual capacity of LLMs by conducting an exhaustive analysis across 101 languages, and classifies languages with similar characteristics into four distinct quadrants. By delving into each quadrant, we shed light on the rationale behind their categorization and offer actionable guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 763}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '868f3b71a625e29727b507ec9022e160'}>,\n",
              "  <Document: {'content': 'our expectations, and we can significantly improve the multilingual performance of LLMs by focusing on these distinct attributes present in each quadrant.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This study evaluates the multilingual capacity of LLMs by conducting an exhaustive analysis across 101 languages, and classifies languages with similar characteristics into four distinct quadrants, shedding light on the rationale behind their categorization and offering actionable guidelines for tuning these languages.\"\\n                    }\\n                },\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 764}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9b37b91e35259c0e6a6309b77d076b27'}>,\n",
              "  <Document: {'content': '      {\\n                    \"paperId\": \"e6d1139f185acf6a08260190d4dba138f918e1df\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 765}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7450de40360f3eefc310b038551a3340'}>,\n",
              "  <Document: {'content': '               \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"How do languages influence each other? Studying cross-lingual data sharing during LLM fine-tuning\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 766}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a307defda9d8e052b2181000a297dd35'}>,\n",
              "  <Document: {'content': '   \"abstract\": \"Multilingual large language models (MLLMs) are jointly trained on data from many different languages such that representation of individual languages can benefit from other languages\\' data. Impressive performance on zero-shot cross-lingual transfer shows that these models are capable of exploiting data from other languages. Yet, it remains unclear to what extent, and under which conditions, languages rely on each other\\'s data. In this study, we use TracIn (Pruthi et al., 2020), a training data attribution (TDA) method, to retrieve the most influential training samples seen during multilingual fine-tuning for a particular test language. This allows us to analyse cross-lingual sharing mechanisms of MLLMs from a new perspective. While previous work studied cross-lingual sharing at the level of model parameters, we present the first approach to study cross-lingual sharing at the data level. We find that MLLMs rely on data from multiple languages from the early stages of fine-tuning and that this reliance gradually increases as fine-tuning progresses. We further study how different fine-tuning languages influence model performance on a given test language and find that they can both reinforce and complement the knowledge acquired from data of the test language itself.\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 767}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8d453dd9552c25a6b246cbba57fd6bf3'}>,\n",
              "  <Document: {'content': '              \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2305.13286\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This study uses TracIn (Pruthi et al., 2020), a training data attribution (TDA) method, to retrieve the most influential training samples seen during multilingual fine-tuning for a particular test language and presents the first approach to study cross-lingual sharing at the data level.\"\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 768}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56f6dfa3416d1948f7127ff647c2203'}>,\n",
              "  <Document: {'content': '                  }\\n                },\\n                {\\n                    \"paperId\": \"d7d5d3c76bd71042b976fffe85c37b0df641732a\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"0d6f7fba-7092-46b3-8039-93458dba736b\",\\n                        \"name\": \"IEEE International Conference on Acoustics, Speech, and Signal Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 769}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '668de4bac5a7c4865446c192d7f0e910'}>,\n",
              "  <Document: {'content': '[\\n                            \"Int Conf Acoust Speech Signal Process\",\\n                            \"IEEE Int Conf Acoust Speech Signal Process\",\\n                            \"ICASSP\",\\n                            \"International Conference on Acoustics, Speech, and Signal Processing\"\\n                        ],\\n                        \"url\": \"http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002\"\\n                    },\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 770}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6d50b20813ca126d67b0724ee22e70db'}>,\n",
              "  <Document: {'content': '                   \"title\": \"Massively Multilingual Shallow Fusion with Large Language Models\",\\n                    \"abstract\": \"While large language models (LLM) have made impressive progress in natural language processing, it remains unclear how to utilize them in improving automatic speech recognition (ASR). In this work, we propose to train a single multilingual language model (LM) for shallow fusion in multiple languages. We push the limits of the multilingual LM to cover up to 84 languages by scaling up using a mixture-of-experts LLM, i.e., generalist language model (GLaM). When the number of experts increases, GLaM dynamically selects only two at each decoding step to keep the inference computation roughly constant. We then apply GLaM to a multilingual shallow fusion task based on a state-of-the-art end-to-end model. Compared to a dense LM of similar computation during inference, GLaM reduces the WER of an English long-tail test set by 4.4% relative. In a multilingual shallow fusion task, GLaM improves 41 out of 50 languages with an average relative WER reduction of', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 771}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9db5918d131c373d0ee15ca86e60cd22'}>,\n",
              "  <Document: {'content': '3.85%, and a maximum reduction of 10%. Compared to the baseline model, GLaM achieves an average WER reduction of 5.53% over 43 languages.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2302.08917\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes to train a single multilingual language model (LM) for shallow fusion in multiple', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 772}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4cbd1d1cadf030c68aff2dd051658643'}>,\n",
              "  <Document: {'content': 'languages based on a state-of-the-art end-to-end model and pushes the limits of the multilingual LM to cover up to 84 languages by scaling up using a mixture of experts LLM.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"0947cbc83b72fefa536423114883ddb6627625f7\",\\n                    \"publicationVenue\": {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 773}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a230580dacbb4265f23be7318dff4d3'}>,\n",
              "  <Document: {'content': '              \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 774}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '53088999a63991935b971bf8e61aa261'}>,\n",
              "  <Document: {'content': '       \"title\": \"Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding\",\\n                    \"abstract\": \"Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs. Our experiments show that ToolDec reduces syntactic errors to zero, consequently achieving significantly better performance and as much as a 2x speedup. We also show that', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 775}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '42ad6a1c5a09e91af1a7e5b5535f1516'}>,\n",
              "  <Document: {'content': 'ToolDec achieves superior generalization performance on unseen tools, performing up to 8x better than the baselines.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2310.07075\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper proposes ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs that reduces syntactic errors to zero and enables LLM to effectively', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 776}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '281137eb2510437cc79e429540ffe987'}>,\n",
              "  <Document: {'content': 'select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"b8b76b856b171170b62663b4c0743738b42f0801\",\\n                    \"publicationVenue\": {\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 777}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e79cb339127874aac989b928a1985cef'}>,\n",
              "  <Document: {'content': '  \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Joint Design of Protein', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 778}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6f686595aecc6637a5579c6f5d4f8509'}>,\n",
              "  <Document: {'content': 'Sequence and Structure based on Motifs\",\\n                    \"abstract\": \"Designing novel proteins with desired functions is crucial in biology and chemistry. However, most existing work focus on protein sequence design, leaving protein sequence and structure co-design underexplored. In this paper, we propose GeoPro, a method to design protein backbone structure and sequence jointly. Our motivation is that protein sequence and its backbone structure constrain each other, and thus joint design of both can not only avoid nonfolding and misfolding but also produce more diverse candidates with desired functions. To this end, GeoPro is powered by an equivariant encoder for three-dimensional (3D) backbone structure and a protein sequence decoder guided by 3D geometry. Experimental results on two biologically significant metalloprotein datasets, including $\\\\\\\\beta$-lactamases and myoglobins, show that our proposed GeoPro outperforms several strong baselines on most metrics. Remarkably, our method discovers novel $\\\\\\\\beta$-lactamases and myoglobins which are not present in protein data bank (PDB) and UniProt. These proteins exhibit stable folding and active site environments reminiscent of those of natural proteins, demonstrating their excellent potential to be biologically functional.\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 779}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '296654390f6723ffb4ecff5185ee38bb'}>,\n",
              "  <Document: {'content': '              \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2310.02546\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The motivation is that protein sequence and its backbone structure constrain each other, and thus joint design of both can not only avoid nonfolding and misfolding but also produce more diverse candidates with desired functions.\"\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 780}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdb649e3507ebc3cd2b21e14bcb85dc'}>,\n",
              "  <Document: {'content': '          }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 2,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"c8b27d092e0f9e286bb354bb892984a30126c702\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"PlayGround Low Resource Machine Translation System for the 2023 AmericasNLP Shared Task\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 781}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '51df559b13e972b73d6337d97a5aba55'}>,\n",
              "  <Document: {'content': '   \"abstract\": \"This paper presents PlayGround\\\\u2019s submission to the AmericasNLP 2023 shared task on machine translation (MT) into indigenous languages. We finetuned NLLB-600M, a multilingual MT model pre-trained on Flores-200, on 10 low-resource language directions and examined the effectiveness of weight averaging and back translation. Our experiments showed that weight averaging, on average, led to a 0.0169 improvement in the ChrF++ score. Additionally, we found that back translation resulted in a 0.008 improvement in the ChrF++ score.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.americasnlp-1.19.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 782}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '583b247e7a59181d6b4633201f94265d'}>,\n",
              "  <Document: {'content': '              \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"PlayGround\\\\u2019s submission to the AmericasNLP 2023 shared task on machine translation (MT) into indigenous languages is presented and the effectiveness of weight averaging and back translation is examined.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"2f9c074f0b0a563be4dd15e8e340c7d813533329\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Four Approaches to Low-Resource Multilingual NMT: The Helsinki Submission to the AmericasNLP 2023 Shared Task\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 783}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd8c38a336c74adb196f05cb271fca396'}>,\n",
              "  <Document: {'content': '                \"abstract\": \"The Helsinki-NLP team participated in the AmericasNLP 2023 Shared Task with 6 submissions for all 11 language pairs arising from 4 different multilingual systems. We provide a detailed look at the work that went into collecting and preprocessing the data that led to our submissions. We explore various setups for multilingual Neural Machine Translation (NMT), namely knowledge distillation and transfer learning, multilingual NMT including a high-resource language (English), language-specific fine-tuning, and multilingual NMT exclusively using low-resource data. Our multilingual Model B ranks first in 4 out of the 11 language pairs.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.americasnlp-1.20.pdf\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 784}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8d4fcba12513ed5ad51c2349374dfa8d'}>,\n",
              "  <Document: {'content': '                  \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Various setups for multilingual Neural Machine Translation (NMT), namely knowledge distillation and transfer learning, multilingual NMT including a high-resource language (English), language-specific fine-tuning, and mult bilingual NMT exclusively using low-resource data are explored.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 2,\\n            \"offset\": 0,\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 785}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5fada8a13fbb229636241c131a2f19f'}>,\n",
              "  <Document: {'content': '   \"data\": [\\n                {\\n                    \"paperId\": \"83152f52af32a2a2ad4843680c386ac996ee5e39\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 786}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8e1e90628d1e72e403cac5e868d01b5d'}>,\n",
              "  <Document: {'content': 'Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"Extrapolating Multilingual Understanding Models as Multilingual Generators\",\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 787}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f14089e05818914e7f23e7549cb8e53b'}>,\n",
              "  <Document: {'content': '           \"abstract\": \"Multilingual understanding models (or encoder-based), pre-trained via masked language modeling, have achieved promising results on many language understanding tasks (e.g., mBERT). However, these non-autoregressive (NAR) models still struggle to generate high-quality texts compared with autoregressive (AR) models. Considering that encoder-based models have the advantage of efficient generation and self-correction abilities, this paper explores methods to empower multilingual understanding models the generation abilities to get a unified model. Specifically, we start from a multilingual encoder (XLM-R) and propose a \\\\\\\\textbf{S}emantic-\\\\\\\\textbf{G}uided \\\\\\\\textbf{A}lignment-then-Denoising (SGA) approach to adapt an encoder to a multilingual generator with a small number of new parameters. Experiments show that the proposed approach is an effective adaption method, outperforming widely-used initialization-based methods with gains of 9.4 BLEU on machine translation, 8.1 Rouge-L on question generation, and 5.5 METEOR on story generation on XLM-R$_{large}$. On the other hand, we observe that XLM-R is still inferior to mBART in supervised settings despite better results on zero-shot settings, indicating that more exploration is required to make understanding models strong generators.\",\\n                    \"openAccessPdf\": {\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 788}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '778c8d4c09b27f982c212da31f064dd0'}>,\n",
              "  <Document: {'content': '                       \"url\": \"http://arxiv.org/pdf/2305.13140\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The proposed approach is an effective adaption method, outperforming widely-used initialization-based methods with gains and observing that XLM-R is still inferior to mBART in supervised settings despite better results on zero-shot settings, indicating that more exploration is required to make understanding models strong generators.\"\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 789}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8872b8a3fc6e46c6815e1e5987ecefb8'}>,\n",
              "  <Document: {'content': '   }\\n                },\\n                {\\n                    \"paperId\": \"7340659eaffae8d11f39a052512f01d7b84fa166\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"28a83bf1-b6e5-416f-8c5a-fdc53e54f3a1\",\\n                        \"name\": \"International Conference on Automation and Computing\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 790}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c3266311b14975d1c4bdd46afbfa881e'}>,\n",
              "  <Document: {'content': '          \"IEEE International Conference on Autonomic Computing\",\\n                            \"IEEE Int Conf Auton Comput\",\\n                            \"Int Conf Autom Comput\",\\n                            \"ICAC\",\\n                            \"International Conference on Autonomic Computing\",\\n                            \"Int Conf Auton Comput\"\\n                        ],\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 791}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c90b9d09874071d23a58f49e8eba7898'}>,\n",
              "  <Document: {'content': '                 \"url\": \"http://www.wikicfp.com/cfp/program?id=1269\",\\n                        \"alternate_urls\": [\\n                            \"https://acsos.github.io/\"\\n                        ]\\n                    },\\n                    \"title\": \"SentiNet: A Robust and Multilingual Sentiment Analysis System with Transfer Learning and Adversarial Training Techniques\",\\n                    \"abstract\": \"Sentiment analysis, the task of automatically classifying the sentiment expressed in a text, plays a vital role in understanding public opinion and sentiment trends across various domains and languages.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 792}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b99a90d9e577ac406e751010aae9f1a'}>,\n",
              "  <Document: {'content': 'However, developing robust and multilingual sentiment analysis systems remains a challenging task due to the scarcity of labeled data and the linguistic complexities across different languages. This research paper introduces SentiNet, a novel sentiment analysis system that addresses these challenges through the integration of four-component framework for sentiment analysis. The Emotional Identifier accurately classifies emotions using advanced deep learning models. The Model Consistency Enhancer reduces biases and inconsistencies by combining multiple sentiment classifiers and subjecting them to adversarial attack testing. The SentSim Generator generates contextually similar sentences to enrich the analysis. cross-lingual sentiment transfer tool that can modify the sentiment of a text across multiple languages while preserving its meaning. Extensive evaluations on benchmark datasets demonstrate the superiority of SentiNet over state-of-the-art methods, with improved emotion detection, enhanced model consistency, and diverse sentence variations. SentiNet contributes to sentiment analysis advancements and establishes itself as a powerful tool for accurate sentiment analysis. The framework\\'s applicability extends to various domains requiring sentiment analysis for decision-making and understanding user sentiments.\",\\n                    \"openAccessPdf\": null,\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 793}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c7690979e37615b4da839f0b9435251'}>,\n",
              "  <Document: {'content': '       \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"SentiNet is introduced, a novel sentiment analysis system that addresses challenges through the integration of four-component framework for sentiment analysis, and its applicability extends to various domains requiring sentiment analysis for decision-making and understanding user sentiments.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 794}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '169027046219a2cec3bfbfd44ce6e42c'}>,\n",
              "  <Document: {'content': '           \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"e814deb54d154aad19ae2b72a2e4dd3376175bb5\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"41bf9ed3-85b3-4c90-b015-150e31690253\",\\n                        \"name\": \"Conference on Empirical Methods in Natural Language Processing\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 795}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d92730088030e02ac2c28b7bd0599e1'}>,\n",
              "  <Document: {'content': '                   \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Empir Method Nat Lang Process\",\\n                            \"Empirical Methods in Natural Language Processing\",\\n                            \"Conf Empir Method Nat Lang Process\",\\n                            \"EMNLP\"\\n                        ],\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 796}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '673e269704e19b8ae2aa2a2d14b051bd'}>,\n",
              "  <Document: {'content': '                   \"url\": \"https://www.aclweb.org/portal/emnlp\"\\n                    },\\n                    \"title\": \"AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models\",\\n                    \"abstract\": \"Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 797}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f2eb4fd306d1ff235e4b3374d18dc39'}>,\n",
              "  <Document: {'content': 'even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2305.15064\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection and achieves success rates on par with the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 798}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '179d8fd8bd1fbd1b0beb8a7d0c89e983'}>,\n",
              "  <Document: {'content': 'baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"0947cbc83b72fefa536423114883ddb6627625f7\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 799}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '39103162b041e5238a0fa0796f9f1873'}>,\n",
              "  <Document: {'content': '               \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 800}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '70bbcb7204f571d11fc951b91b3f73b8'}>,\n",
              "  <Document: {'content': '             },\\n                    \"title\": \"Syntax Error-Free and Generalizable Tool Use for LLMs via Finite-State Decoding\",\\n                    \"abstract\": \"Large language models (LLMs) have shown promising capabilities in using external tools to solve complex problems. However, existing approaches either involve fine-tuning on tool demonstrations, which do not generalize to new tools without additional training, or providing tool documentation in context, limiting the number of tools. Both approaches often generate syntactically invalid tool calls. In this paper, we propose ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs. ToolDec eliminates tool-related errors for any tool-augmented LLMs by ensuring valid tool names and type-conforming arguments. Furthermore, ToolDec enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation. We evaluated multiple prior methods and their ToolDec-enhanced versions on a variety of tasks involving tools like math functions, knowledge graph relations, and complex real-world RESTful APIs.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 801}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8e76815bea6fd242fe866c3555bc6825'}>,\n",
              "  <Document: {'content': 'Our experiments show that ToolDec reduces syntactic errors to zero, consequently achieving significantly better performance and as much as a 2x speedup. We also show that ToolDec achieves superior generalization performance on unseen tools, performing up to 8x better than the baselines.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2310.07075\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 802}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd8b641b15ca04e73823568f9e3dc79e7'}>,\n",
              "  <Document: {'content': '  \"text\": \"This paper proposes ToolDec, a finite-state machine-guided decoding algorithm for tool-augmented LLMs that reduces syntactic errors to zero and enables LLM to effectively select tools using only the information contained in their names, with no need for fine-tuning or in-context documentation.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 3,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"24113f06520130c8a42f8920aba9cc9f0f5a5777\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 803}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2cc6929659da9169004c6c280b3e675b'}>,\n",
              "  <Document: {'content': '   \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 804}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b18a287c271874764bebb7f344cf0ed'}>,\n",
              "  <Document: {'content': ' },\\n                    \"title\": \"Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design\",\\n                    \"abstract\": \"Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein\\'s sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\\\\\\\\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capability of our model to design protein sequences', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 805}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e15d44ace9547b5d85c02da47b32cf8d'}>,\n",
              "  <Document: {'content': 'and structures that closely resemble their natural counterparts. Furthermore, in-depth analysis further confirms our model\\'s ability to generate highly effective proteins capable of binding to their target metallocofactors. We provide code, data and models in Github.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2310.04343\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"NAEPro is a', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 806}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd7c0bfd3eeeede2de4fadfc9582797d4'}>,\n",
              "  <Document: {'content': 'model to jointly design Protein sequence and structure based on automatically detected functional sites powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"b8b76b856b171170b62663b4c0743738b42f0801\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 807}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'db57b29f7697b62722a05ebbb1ebb85d'}>,\n",
              "  <Document: {'content': '            \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Joint Design of Protein Sequence and Structure based on Motifs\",\\n                    \"abstract\": \"Designing novel proteins with desired functions is crucial in biology and chemistry. However, most', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 808}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c9c2a09bf630d15df1ad34badfd0ef2a'}>,\n",
              "  <Document: {'content': 'existing work focus on protein sequence design, leaving protein sequence and structure co-design underexplored. In this paper, we propose GeoPro, a method to design protein backbone structure and sequence jointly. Our motivation is that protein sequence and its backbone structure constrain each other, and thus joint design of both can not only avoid nonfolding and misfolding but also produce more diverse candidates with desired functions. To this end, GeoPro is powered by an equivariant encoder for three-dimensional (3D) backbone structure and a protein sequence decoder guided by 3D geometry. Experimental results on two biologically significant metalloprotein datasets, including $\\\\\\\\beta$-lactamases and myoglobins, show that our proposed GeoPro outperforms several strong baselines on most metrics. Remarkably, our method discovers novel $\\\\\\\\beta$-lactamases and myoglobins which are not present in protein data bank (PDB) and UniProt. These proteins exhibit stable folding and active site environments reminiscent of those of natural proteins, demonstrating their excellent potential to be biologically functional.\",\\n                    \"openAccessPdf\": {\\n                        \"url\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 809}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '367b4d0bd7641b1468ecae55c963bae4'}>,\n",
              "  <Document: {'content': '\"https://arxiv.org/pdf/2310.02546\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The motivation is that protein sequence and its backbone structure constrain each other, and thus joint design of both can not only avoid nonfolding and misfolding but also produce more diverse candidates with desired functions.\"\\n                    }\\n                },\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 810}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dcd1cd036c0f99a2e9da08852250ab5'}>,\n",
              "  <Document: {'content': '  {\\n                    \"paperId\": \"920207ba3f6b2293524c4fa71b6084454347b1a8\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"458166b3-de17-4bf3-bbbb-e53782de2f0f\",\\n                        \"name\": \"Nature Biotechnology\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Nat Biotechnol\"\\n                        ],\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 811}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '38cbf7d4efd737fa6db46f78fded8c92'}>,\n",
              "  <Document: {'content': '                      \"issn\": \"1087-0156\",\\n                        \"url\": \"http://www.nature.com/nbt/\",\\n                        \"alternate_urls\": [\\n                            \"http://www.nature.com/nbt\"\\n                        ]\\n                    },\\n                    \"title\": \"Biotech commission accused of conflicts of interest\",\\n                    \"abstract\": null,\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 812}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a00432df535a969a5145d6a91a1ef058'}>,\n",
              "  <Document: {'content': '             \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A new generative modeling-based design algorithm called Chroma, which includes several features that improve its performance and success rate, and a language modeling algorithm that can yield novel computer-designed proteins that can be successfully produced in the lab with catalytic activities comparable to those of natural enzymes.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 813}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6984f12fff58ae546eb7042045b6b060'}>,\n",
              "  <Document: {'content': '       \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 165,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"bb553de910c20a31c951c6211d9af19ad202e260\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 814}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9bc58f46c95bb3e07baaa05eec00e883'}>,\n",
              "  <Document: {'content': '                     \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"A Systematic Analysis of Vocabulary and BPE Settings for Optimal Fine-tuning of NMT: A Case Study of In-domain Translation\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 815}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a7018f881bc78a5858da58022f6d82c'}>,\n",
              "  <Document: {'content': '   \"abstract\": \"The effectiveness of Neural Machine Translation (NMT) models largely depends on the vocabulary used at training; small vocabularies can lead to out-of-vocabulary problems -- large ones, to memory issues. Subword (SW) tokenization has been successfully employed to mitigate these issues. The choice of vocabulary and SW tokenization has a significant impact on both training and fine-tuning an NMT model. Fine-tuning is a common practice in optimizing an MT model with respect to new data. However, new data potentially introduces new words (or tokens), which, if not taken into consideration, may lead to suboptimal performance. In addition, the distribution of tokens in the new data can differ from the distribution of the original data. As such, the original SW tokenization model could be less suitable for the new data. Through a systematic empirical evaluation, in this work we compare different strategies for SW tokenization and vocabulary generation with the ultimate goal to uncover an optimal setting for fine-tuning a domain-specific model. Furthermore, we developed several (in-domain) models, the best of which achieves 6 BLEU points improvement over the baseline.\",\\n                  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 816}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b276e2569dd8908cfea064d94fb57c7'}>,\n",
              "  <Document: {'content': ' \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2303.00722\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Through a systematic empirical evaluation, this work compares different strategies for SW tokenization and vocabulary generation with the ultimate goal to uncover an optimal setting for fine-tuning a domain-specific model.\"\\n                    }\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 817}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c4a9fb2e58251b3cac0f5be44fa23d9b'}>,\n",
              "  <Document: {'content': '        },\\n                {\\n                    \"paperId\": \"0dcd3678f7ed3295d92fb3ff093e9f8023f57b71\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"47c47088-7640-441b-85ba-375fe8f5c3a4\",\\n                        \"name\": \"International Conference on Computing Communication and Networking Technologies\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 818}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e9f1540e53dd5e62ee7a95e137db4b4'}>,\n",
              "  <Document: {'content': ' \"Int Conf Comput Commun Netw Technol\",\\n                            \"International Conference on Computing, Communication and Networking Technologies\",\\n                            \"ICCCNT\"\\n                        ]\\n                    },\\n                    \"title\": \"Performance Evaluation of English to Bodo Neural Machine Translation System with Varying Model Architecture and Vocabulary Size\",\\n                    \"abstract\": \"This paper is about a work done on Neural Machine Translation of English-Bodo language pair using deep learning technique. Bodo is a language of northeastern part of India particularly', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 819}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a061a0dce18d84745b67ef977b21a7d'}>,\n",
              "  <Document: {'content': 'in the state of Assam. The experiments are performed using English-Bodo parallel data collected from open sources, as well as created inhouse using expert linguists. The dataset was subjected to data filtering and cleaning. IndicNLP library and Mosesdecoder are used for tokenization for Bodo and English respectively. After tokenization, Byte Pair Encoding (BPE) technique was used for subword tokenization. Two transformer encoder and decoder models are built using different architecture. OpenNMT-py framework is used for building our models. Experiments have been performed on the two models using two different vocabularies of 8000 and 16000 sizes. Highest BLEU score of 10.43 was achieved on the testset from training Model 2 with 8000 vocabulary. The details of the results are analysed against different models.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 820}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f6f5729a540fdcddc52970f0ed4d4fd3'}>,\n",
              "  <Document: {'content': '            \"text\": \"This paper is about a work done on Neural Machine Translation of English-Bodo language pair using deep learning technique and two transformer encoder and decoder models are built using different architecture.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"9dd7e3c0d5bdb1cf3f0dd732a7faad610175a0c5\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"23f24704-1359-4644-be68-ab8e1f36ad92\",\\n                        \"name\": \"Computer Assisted Language Learning\",\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 821}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ea17801f95c78897457068472b978653'}>,\n",
              "  <Document: {'content': '              \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Comput Assist Lang Learn\"\\n                        ],\\n                        \"issn\": \"0958-8221\",\\n                        \"url\": \"http://www.tandfonline.com/loi/ncal20\"\\n                    },\\n                    \"title\": \"Neural machine translation in EFL classrooms: learners\\\\u2019 vocabulary improvement, immediate vocabulary retention and delayed', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 822}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '71ffbd3abc1131eaa708f05f75022bc4'}>,\n",
              "  <Document: {'content': 'vocabulary retention\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 9,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"3704b76de736eba421a4cb47b0a0e6cb9930b126\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 823}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '35ef93f40518e5192f9e208e5f810079'}>,\n",
              "  <Document: {'content': '                 \"publicationVenue\": {\\n                        \"id\": \"5361f7b6-ede4-4f32-8694-9d5472e7a41b\",\\n                        \"name\": \"Lab on a Chip\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Lab Chip\"\\n                        ],\\n                        \"issn\": \"1473-0189\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 824}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'df8669f04b16dfaeff184e55c37a9d01'}>,\n",
              "  <Document: {'content': '                       \"url\": \"https://www.rsc.org/journals-books-databases/about-journals/lab-on-a-chip/\",\\n                        \"alternate_urls\": [\\n                            \"http://xlink.rsc.org/jumptojournal.cfm?journal_code=LC\",\\n                            \"http://www.rsc.org/Publishing/Journals/lc/index.asp\",\\n                            \"https://pubs.rsc.org/en/journals/journalissues/lc#!recentarticles&adv\"\\n                        ]\\n                    },\\n                    \"title\": \"Acrylic-based culture', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 825}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2e81a2831789cff956f23cee4f136c31'}>,\n",
              "  <Document: {'content': 'plate format perfusion device to establish liver endothelial-epithelial interface.\",\\n                    \"abstract\": \"Microphysiological Systems (MPSs) or organs-on-chips, are microfluidic devices used to model human physiology in vitro. Polydimethylsiloxane (PDMS) is the most widely used material for organs-on-chips due to its established fabrication methods and biocompatibility properties. However, non-specific binding of small molecules limits PDMS for drug screening applications. Here, we designed a novel acrylic-based MPS to capture the physiological architecture that is observed universally in tissues across the body: the endothelial-epithelial interface (EEI). To reconstruct the EEI biology, we designed a membrane-based chip that features endothelial cells on the underside of the membrane exposed to mechanical shear from the path of media flow, and epithelial cells on the opposite side of the membrane protected from flow, as they are in vivo. We used a liver model with a hepatic progenitor cell line and human umbilical vein endothelial cells to assess the biological efficacy of the MPS. We computationally modeled the physics that govern the function of perfusion through the MPS. Empirically, efficacy was measured by comparing differentiation of the hepatic progenitor cells between', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 826}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '21da8d858b194a96b0d678bc405fcac5'}>,\n",
              "  <Document: {'content': 'the MPS and 2D culture conditions. We demonstrated that the MPS significantly improved hepatocyte differentiation, increased extracellular protein transport, and raised hepatocyte sensitivity to drug treatment. Our results strongly suggest that physiological perfusion has a profound effect on proper hepatocyte function, and the modular chip design motivates opportunities for future study of multi-organ interactions.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is demonstrated that the MPS significantly improved hepatocyte differentiation, increased extracellular protein transport, and raised hepatocyte sensitivity to drug treatment, and the modular chip design motivates opportunities for future study of multi-organ interactions.\"\\n                    }\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 827}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '41a0d0f76d6397689d168a62084bce00'}>,\n",
              "  <Document: {'content': '              },\\n                {\\n                    \"paperId\": \"e7c3fb49b961cd722ab7fd0c7148a369f38ff728\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Javanese Maxims as Politeness Guidance in Social Media Phatic Communication: A Cyberpragmatics Perspective\",\\n                    \"abstract\": \"Communication on social media raises at a significant rate during the Covid-19 Pandemic. Social media communication (SMC) in specific culture requires particular strategies related to cultural norms. Due to the limitation of cyber communication compares to direct communication, users of social media utilize available features in social media platforms to perform politeness. The strategies of Javanese netizens to perform Javanese politeness maxims in SMC are the main discussion in this study. The data were', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 828}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5b40ca968fa94287ebc311866dd46a08'}>,\n",
              "  <Document: {'content': 'collected from five WhatsApp Groups (WAG) of Javanese adults of 20s-40s of age comprising 246 respondents. The conversation texts containing phatic talks in WAG were selected by observation method. Then, the conversational texts and communication icons were transcribed for contextual analysis. The result shows that the Javanese maxims of Kurmat (Respect), Tepa Selira (Tolerance), Andhap Asor (Humility), and Empan Papan (Self-Awareness) were utilized by Javanese netizens in performing politeness in virtual phatic communication. During the pandemic, tolerance maxim is the most frequently used to support each other. The maxim of humility is not obvious in virtual phatic communication. To this extent, Javanese politeness maxims are mitigating devices to establish social rapport in phatic communication. The Javanese ability in performing Javanese maxims as politeness devices is essential to avoid conflict and mitigate Face Threatening Act (FTA) since the Javanese community tends to create harmony in life known as Guyub Rukun.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://jurnal.uns.ac.id/javanologi/article/download/67942/37943\",\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 829}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd655f5da80c160f7f0721cb7c8826c44'}>,\n",
              "  <Document: {'content': '                  \"status\": \"BRONZE\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The result shows that the Javanese maxims of Kurmat (Respect), Tepa Selira (Tolerance), Andhap Asor (Humility), and Empan Papan (Self-Awareness) were utilized byJavanese netizens in performing politeness in virtual phatic communication.\"\\n                    }\\n                },\\n                {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 830}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd05a0a912a209a991bd07e87ecfa8eaf'}>,\n",
              "  <Document: {'content': '            \"paperId\": \"779919ed3a3669c446b2315f850a5de17ee1da19\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"2633f5b2-c15c-49fe-80f5-07523e770c26\",\\n                        \"name\": \"IEEE Access\",\\n                        \"type\": \"journal\",\\n                        \"issn\": \"2169-3536\",\\n                        \"url\": \"http://www.ieee.org/publications_standards/publications/ieee_access.html\",\\n                        \"alternate_urls\": [\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 831}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1401ea7c7c36e4146e9c8cde760dbe26'}>,\n",
              "  <Document: {'content': '             \"http://ieeexplore.ieee.org/servlet/opac?punumber=6287639\"\\n                        ]\\n                    },\\n                    \"title\": \"Safeguarding Online Spaces: A Powerful Fusion of Federated Learning, Word Embeddings, and Emotional Features for Cyberbullying Detection\",\\n                    \"abstract\": \"Cyberbullying has emerged as a pervasive issue in the digital age, necessitating advanced techniques for effective detection and mitigation. This research explores the integration of word embeddings, emotional features, and federated learning to address the challenges of centralized data processing and user privacy concerns prevalent in previous methods. Word embeddings capture semantic relationships and contextual information, enabling a more nuanced understanding of text data, while emotional features derived from text extend the analysis to encompass the affective dimension, enhancing cyberbullying identification. Federated learning, a decentralized', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 832}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f613428e3b50ffeef2fe61da20285839'}>,\n",
              "  <Document: {'content': 'learning paradigm, offers a compelling solution to centralizing sensitive user data by enabling collaborative model training across distributed devices, preserving privacy while harnessing collective intelligence. In this study, we conduct an in-depth investigation into the fusion of word embeddings, emotional features, and federated learning, complemented by the utilization of BERT, Convolutional Neural Networks (CNN), Deep Neural Networks (DNN), and Long Short-Term Memory (LSTM) models. Hyperparameters and neural architecture are explored to find optimal configurations, leading to the generation of superior results. These techniques are applied in the context of cyberbullying detection, using publicly available multi-platform (social media) cyberbullying datasets. Through extensive experiments and evaluations, our proposed framework demonstrates superior performance and robustness compared to traditional methods. The results illustrate the enhanced ability to identify and combat cyberbullying incidents effectively, contributing to the creation of safer online environments. Particularly, the BERT model consistently outperforms other deep learning models (CNN, DNN, LSTM) in cyberbullying detection while preserving the privacy of local datasets for each social platform through our improved federated learning setup. We have provided Differential Privacy based security analysis for the proposed method to further strengthen the privacy and robustness of the system. By leveraging word embeddings, emotional features, and federated', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 833}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a15c5948959821a260a5ef1cb5007c22'}>,\n",
              "  <Document: {'content': 'learning, this research opens new avenues in cyberbullying research, paving the way for proactive intervention and support mechanisms. The comprehensive approach presented herein highlights the substantial strengths and advantages of this integrated methodology, setting a foundation for future advancements in cyberbullying detection and mitigation.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://ieeexplore.ieee.org/ielx7/6287639/6514899/10305070.pdf\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 834}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9d9df0ac53cff7067aa40195aa050435'}>,\n",
              "  <Document: {'content': '    \"text\": \"An in-depth investigation into the fusion of word embeddings, emotional features, and federated learning, complemented by the utilization of BERT, Convolutional Neural Networks (CNN), Deep Neural networks (DNN), and Long Short-Term Memory (LSTM) models.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 835}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd07a22d272bca00df44b2d4a67594a6'}>,\n",
              "  <Document: {'content': '                   \"paperId\": \"d1a6ca4d7d84d2b664076d03a47138cd34393c77\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fc0a208c-acb7-47dc-a0d4-af8190e21d29\",\\n                        \"name\": \"International Conference on Machine Learning\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"ICML\",\\n                          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 836}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5495aa2177988b9f557d2ba0823e9bd7'}>,\n",
              "  <Document: {'content': ' \"Int Conf Mach Learn\"\\n                        ],\\n                        \"url\": \"https://icml.cc/\"\\n                    },\\n                    \"title\": \"ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval\",\\n                    \"abstract\": \"Diffusion models show promising generation capability for a variety of data. Despite their high generation quality, the inference for diffusion models is still time-consuming due to the numerous sampling iterations required. To accelerate the inference, we propose ReDi, a simple yet learning-free Retrieval-based Diffusion sampling framework. From a precomputed knowledge base, ReDi retrieves a trajectory similar to the partially generated trajectory at an early stage of generation, skips a large portion of intermediate steps, and continues sampling from', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 837}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e15c675d025b67c116bc13d07533a3e3'}>,\n",
              "  <Document: {'content': 'a later step in the retrieved trajectory. We theoretically prove that the generation performance of ReDi is guaranteed. Our experiments demonstrate that ReDi improves the model inference efficiency by 2x speedup. Furthermore, ReDi is able to generalize well in zero-shot cross-domain image generation such as image stylization.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2302.02285\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 838}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c1e13ec17521c010acb73f441199207'}>,\n",
              "  <Document: {'content': '       \"text\": \"ReDi, a simple yet learning-free Retrieval-based Diffusion sampling framework that retrieves a trajectory similar to the partially generated trajectory at an early stage of generation, skips a large portion of intermediate steps, and continues sampling from a later step in the retrieved trajectory.\"\\n                    }\\n                }\\n            ]\\n        }\\n    ],\\n    \"Teruko Mitamura\": [\\n        {\\n            \"total\": 1411,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 839}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9d5d2e3e7f1d31e606277b03d806a569'}>,\n",
              "  <Document: {'content': '            \"paperId\": \"ff77105b2c345f54e1a87f4fbb3a701201f0c1a8\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"bdc2e585-4e48-4e36-8af1-6d859763d405\",\\n                        \"name\": \"AAAI Conference on Artificial Intelligence\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"National Conference on Artificial Intelligence\",\\n                            \"National Conf', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 840}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '500814d9f4232e4363310eeb14bdcfd9'}>,\n",
              "  <Document: {'content': 'Artif Intell\",\\n                            \"AAAI Conf Artif Intell\",\\n                            \"AAAI\"\\n                        ],\\n                        \"url\": \"http://www.aaai.org/\"\\n                    },\\n                    \"title\": \"Hierarchical Event Grounding\",\\n                    \"abstract\": \"Event grounding aims at linking mention references in text corpora to events from a knowledge base (KB). Previous work on this task focused primarily on linking to', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 841}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4de9aa5700a10780df62026f02612d5b'}>,\n",
              "  <Document: {'content': 'a single KB event, thereby overlooking the hierarchical aspects of events. Events in documents are typically described at various levels of spatio-temporal granularity. These hierarchical relations are utilized in downstream tasks of narrative understanding and schema construction. In this work, we present an extension to the event grounding task that requires tackling hierarchical event structures from the KB. Our proposed task involves linking a mention reference to a set of event labels from a subevent hierarchy in the KB. We propose a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss. On an automatically created multilingual dataset from Wikipedia and Wikidata, our experiments demonstrate the effectiveness of the hierarchical loss against retrieve and re-rank baselines. Furthermore, we demonstrate the systems\\' ability to aid hierarchical discovery among unseen events. Code is available at https://github.com/JefferyO/Hierarchical-Event-Grounding\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2302.04197\",\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 842}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8382003a27bd6adec586dd1e8c37970e'}>,\n",
              "  <Document: {'content': '    \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work presents an extension to the event grounding task that requires tackling hierarchical event structures from the KB, and proposes a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss.\"\\n                    }\\n                },\\n                {\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 843}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6f56939fa9d7790af092f528aae56624'}>,\n",
              "  <Document: {'content': '\"paperId\": \"d908dbdecadb766b4e993e0cba02f18a1fba2788\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 844}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c485cf8608f4da87e00e58500f5872ef'}>,\n",
              "  <Document: {'content': '                       \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n                    },\\n                    \"title\": \"Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification\",\\n                    \"abstract\": \"Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 845}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5854eb20ee849d04a46baede89d5ce55'}>,\n",
              "  <Document: {'content': 'a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.\",\\n                    \"openAccessPdf\": {\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 846}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '16ababd27a77cd88047a9baf9b430e3f'}>,\n",
              "  <Document: {'content': '                       \"url\": \"https://arxiv.org/pdf/2307.01972\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs) and greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way.\"\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 847}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3258f0afd7a0827b9efeb7df7646546'}>,\n",
              "  <Document: {'content': '     }\\n                },\\n                {\\n                    \"paperId\": \"ac9d007ee7b594ca854553eedbe6e70a56da91a9\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"768b87bb-8a18-4d9c-a161-4d483c776bcf\",\\n                        \"name\": \"Computer Vision and Pattern Recognition\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 848}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4fe83b3101c56321d97faa3ad6ba96a1'}>,\n",
              "  <Document: {'content': '           \"CVPR\",\\n                            \"Comput Vis Pattern Recognit\"\\n                        ],\\n                        \"issn\": \"1063-6919\",\\n                        \"url\": \"https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147\",\\n                        \"alternate_urls\": [\\n                            \"https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition\"\\n                        ]\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 849}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9cc5bf6806696f3c748a291247662513'}>,\n",
              "  <Document: {'content': '             },\\n                    \"title\": \"WINNER: Weakly-supervised hIerarchical decompositioN and aligNment for spatio-tEmporal video gRounding\",\\n                    \"abstract\": \"Spatio-temporal video grounding aims to localize the aligned visual tube corresponding to a language query. Existing techniques achieve such alignment by exploiting dense boundary and bounding box annotations, which can be prohibitively expensive. To bridge the gap, we investigate the weakly-supervised setting, where models learn from easily accessible video-language data without annotations. We identify that intra-sample spurious correlations among video-language components can be alleviated if the model captures the decomposed structures of video and language data. In this light, we propose a novel framework, namely WINNER, for hierarchical video-text understanding. WINNER first builds the language decomposition tree in a bottom-up manner, upon which the structural attention mechanism and top-down feature backtracking jointly build a multi-modal decomposition tree, permitting a hierarchical understanding of unstructured videos. The multi-modal decomposition tree serves as the basis for multi-hierarchy language-tube matching.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 850}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'af9aa51fd81b400de267f04ec864d7e2'}>,\n",
              "  <Document: {'content': 'A hierarchical contrastive learning objective is proposed to learn the multi-hierarchy correspondence and distinguishment with intra-sample and inter-sample video-text decomposition structures, achieving video-language decomposition structure alignment. Extensive experiments demonstrate the rationality of our design and its effectiveness beyond state-of-the-art weakly supervised methods, even some supervised methods.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A novel framework, namely WINNER, for hierarchical video-text understanding is proposed, upon which the structural attention mechanism and top-down feature backtracking jointly build a multi-modal decomposition tree, permitting a hierarchical understanding of unstructured videos.\"\\n                    }\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 851}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'aed9ee68623453a6cf48967b74c2084'}>,\n",
              "  <Document: {'content': '      }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"444737639aeea4e1e616509e368afb0bae8f89d6\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"fc51fcd0-8420-4934-b76d-e46d8a084906\",\\n                        \"name\": \"Workshop on Document-grounded Dialogue and Conversational Question Answering\",\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 852}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36a8a865d931aa1ffd1097c54e7e88fb'}>,\n",
              "  <Document: {'content': '            \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"DialDoc\",\\n                            \"Workshop Doc Dialogue Conversational Quest Answering\"\\n                        ],\\n                        \"url\": \"https://aclanthology.org/venues/dialdoc/\"\\n                    },\\n                    \"title\": \"Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA\",\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 853}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd6c1079aab30b0adbb2aeb997cda57ef'}>,\n",
              "  <Document: {'content': '                   \"abstract\": \"The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://aclanthology.org/2023.dialdoc-1.11.pdf\",\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 854}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '191783b698a6723cae30cf4bcd71423b'}>,\n",
              "  <Document: {'content': '\"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 855}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd6a38fe1537b39883cc04c1f523d5789'}>,\n",
              "  <Document: {'content': '          \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"7fb98b6c5bb07c2b010966c05f29d9db7f783d27\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"7654260e-79f9-45c5-9663-d72027cf88f3\",\\n                        \"name\": \"IEEE International Conference on Computer Vision\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 856}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7552e3b156db1ffedd8f14c031cba9e9'}>,\n",
              "  <Document: {'content': '               \"ICCV\",\\n                            \"IEEE Int Conf Comput Vis\",\\n                            \"ICCV Workshops\",\\n                            \"ICCV Work\"\\n                        ],\\n                        \"url\": \"https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings\"\\n                    },\\n                    \"title\": \"ChartReader: A Unified Framework for', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 857}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dcb0fcfb446751f404b072b8aad153e1'}>,\n",
              "  <Document: {'content': 'Chart Derendering and Comprehension without Heuristic Rules\",\\n                    \"abstract\": \"Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 858}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cb634f19c923dbedce2951458a32450b'}>,\n",
              "  <Document: {'content': 'such as T5 and TaPas, extending their capability to chart comprehension tasks.1\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"The proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model and offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.\"\\n                    }\\n                }\\n            ]\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 859}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4153275d3fb0eb4d8ed75d4cf4576d87'}>,\n",
              "  <Document: {'content': ' },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 32,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"b1c4999f20d374ad78d60e5081d09221741a17ce\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"ab453bce-d4ec-48ec-ad78-ef19dc9333ab\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 860}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6da66261530e8a4da4f4af6b551f3a54'}>,\n",
              "  <Document: {'content': '          \"name\": \"Conference and Labs of the Evaluation Forum\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"CLEF\",\\n                            \"Conf Lab Evaluation Forum\",\\n                            \"Cross-language Evaluation Forum\",\\n                            \"Cross-Language Evaluation Forum\"\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 861}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6809c249380cdc0388050670424bc110'}>,\n",
              "  <Document: {'content': '          ],\\n                        \"url\": \"http://www.clef-initiative.eu/\"\\n                    },\\n                    \"title\": \"Biomedical Question Answering with Transformer Ensembles\",\\n                    \"abstract\": \"Recent advancements in natural language processing, specifically transformers, have shown great promise in improving the performance of question-answering systems. However, we observe that a single transformer model may not achieve sufficient accuracy and reliability to meet the stringent requirements of biomedical question answering. Based on our participation in the BioASQ Challenge, we present a comprehensive approach for biomedical question answering using transformers, integrating an end-to-end data processing pipeline with the UMLS Metamap and different ensembling techniques. Our findings suggest that transformer ensembles achieve significant performance improvements when compared to individual models.\",\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 862}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7bf9ca328a62a88d77e39a5a85dcf094'}>,\n",
              "  <Document: {'content': '            \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work presents a comprehensive approach for biomedical question answering using transformers, integrating an end-to-end data processing pipeline with the UMLS Metamap and different ensembling techniques, and suggests that transformer ensembles achieve significant performance improvements when compared to individual models.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"4a07c92996411dcdc94d42a3b1bef7b5093f3158\",\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 863}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7cd750236d070449dfee2b3cc7c1f117'}>,\n",
              "  <Document: {'content': '                \"publicationVenue\": {\\n                        \"id\": \"8f30bc0d-1a1b-40f0-84bc-59e35b8bbcce\",\\n                        \"name\": \"International Conference on Developments in eSystems Engineering\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"DSE\",\\n                            \"DeSE\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 864}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34116ca8dd8edb678c72ab316150f52'}>,\n",
              "  <Document: {'content': '       \"Int Conf Dev esystems Eng\"\\n                        ]\\n                    },\\n                    \"title\": \"COVID QA Network: A Specific Case of Biomedical Question Answering\",\\n                    \"abstract\": \"COVID-19 crisis has led to an outburst of information that needs to be organized, validated, and made available to the seekers. Despite the rapid growth and success of BERT models in the last 3 years, COVID QA is a difficult task due to the lack of applicable datasets and a relevant language representation. Therefore, this study proposes a transformer-based Question Answering (QA) model for COVID-19 questions from the biomedical domain. Further, explored several datasets, and models required for question type prediction, no-answer prediction, and answer extraction and transfer learning strategies. It has been demonstrated', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 865}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1aafb3ceac384f356786a2bdf76bbbf3'}>,\n",
              "  <Document: {'content': 'that the exact match score can be significantly improved with limited amounts of training data from the biomedical domain. Finally, the findings of the study have been summarized as Factoid QA Finetuning Framework (FQFF), which can provide initial direction for domain-specific QA tasks with a limited amount of data.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It has been demonstrated that the exact match score can be significantly improved with limited amounts of training data from the biomedical domain and the findings of the study have been summarized as Factoid QA Finetuning Framework (FQFF), which can provide initial direction for domain-specific QA tasks with a limited amount of data.\"\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 866}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0a286f48fd4f78646205ed5acad5bef'}>,\n",
              "  <Document: {'content': '            }\\n                },\\n                {\\n                    \"paperId\": \"1cc28b3e7278c50c214aefa11aafdac6a243d3e8\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"8c7fd839-36e4-4261-a5c2-846c243dbbb5\",\\n                        \"name\": \"International Conference on Pattern Analysis and Intelligent Systems\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 867}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd0e3cc5c5c7112bd658038302de47c9'}>,\n",
              "  <Document: {'content': '                     \"PAIS\",\\n                            \"Int Conf Pattern Anal Intell Syst\"\\n                        ]\\n                    },\\n                    \"title\": \"Transformer-Based Question Answering Model for the Biomedical Domain\",\\n                    \"abstract\": \"Motivation: Question Answering (QA) is a highly focused topic in the field of Natural Language Processing (NLP). Recent progress in neural network models and the availability of large datasets like SQuAD have played a significant role in improving performance in open domains. However, there remains a need to further effectively implement these systems', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 868}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7eb8f02e1490910c65d05a83a817639e'}>,\n",
              "  <Document: {'content': 'in more specific domains, especially in the biomedical field, to help medical practitioners provide accurate solutions for inquiries related to medicine and healthcare, including specific subjects such as the COVID-19 disease. Fortunately, recent models, such as transformers, have opened up avenues and modern techniques for developing accurate systems.Aims: In this work, we aim to leverage transformer models and Transfer Learning to effectively train models in the biomedical domain. By taking a pre-trained model for Question Answering tasks and further fine-tuning it on specific domains, we enhance the system\\\\u2019s performance in the biomedical domain. Our ultimate goal is to develop a QA model specifically tailored for COVID-19 QA.Results: We have trained BERT and RoBERTa models on the COVID-QA dataset and achieved competitive results on COVID-19 QA. Our RoBERTa model achieved an Exact Match (EM)/F1 score of 0.38/0.64, respectively, on COVID-QA, indicating successful performance in COVID-19 QA.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 869}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'adfbf7886fbaa518ea46c86ccf04ccd8'}>,\n",
              "  <Document: {'content': '          \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work aims to leverage transformer models and Transfer Learning to effectively train models in the biomedical domain by taking a pre-trained model for Question Answering tasks and further fine-tuning it on specific domains, to enhance the system\\\\u2019s performance in theomedical domain.\"\\n                    }\\n                }\\n            ]\\n        }\\n    ],\\n    \"Louis-Philippe Morency\": [\\n        {\\n            \"total\": 17,\\n            \"offset\": 0,\\n            \"next\": 3,\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 870}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3560a145f10f51ceb23c4b6b41b20904'}>,\n",
              "  <Document: {'content': '  \"data\": [\\n                {\\n                    \"paperId\": \"0a425c0d87c674b142104a07e17c5084b3ad28ca\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 871}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8f2781b0c2b02567ae305ff398492a19'}>,\n",
              "  <Document: {'content': '            \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Quantifying & Modeling Feature Interactions: An Information Decomposition Framework\",\\n                    \"abstract\": \"The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and inte-grating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy , uniqueness , and synergy across input features, which we term the PID statistics of a multimodal distribution.', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 872}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bef44745fdfa3f51d39435dbc7abad90'}>,\n",
              "  <Document: {'content': 'Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2302.12247\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 873}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd20863fcb6b54cd6251de4d8095d7c68'}>,\n",
              "  <Document: {'content': '  \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy across input features, which is term the PID statistics of a multimodal distribution.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"0126f8d40bd682b1c4659f3f7d2a64fb6a515354\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Quantifying&Modeling Multimodal Interactions: An Information', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 874}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '938c27a963d1598ed6bc90401f8024d8'}>,\n",
              "  <Document: {'content': 'Decomposition Framework\",\\n                    \"abstract\": \"The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 875}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74f99aca646cbc63410e8b1800a02dc7'}>,\n",
              "  <Document: {'content': 'engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task, and introduces two new estimators for these PID statistics that scale to high-dimensional distributions.\"\\n                    }\\n                },\\n              ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 876}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7d39aeecc6a860d12878d9cc310ab36d'}>,\n",
              "  <Document: {'content': ' {\\n                    \"paperId\": \"697aa4d7181d919aa9c07df8e1a4bab5a1feeb41\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Hierarchical Framework for Predicting Entropies in Bottom-Up Coarse-Grained Models\",\\n                    \"abstract\": \"The thermodynamic entropy of coarse-grained (CG) models stands as one of the most important properties for quantifying the missing information during the CG process and for establishing transferable (or extendible) CG interactions. However, performing additional CG simulations on top of model construction often leads to significant additional computational overhead. In this work, we propose a simple hierarchical framework for predicting the thermodynamic entropies of various molecular CG systems. Our approach employs a decomposition of the CG interactions, enabling the estimation of the CG partition function and thermodynamic properties a priori. Starting from the ideal gas description, we leverage classical perturbation theory to systematically incorporate simple yet essential', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 877}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '39d80da9846229687fda62b498bf5a61'}>,\n",
              "  <Document: {'content': 'interactions, ranging from the hard sphere model to the generalized van der Waals model. Additionally, we propose an alternative approach based on multiparticle correlation functions, allowing for systematic improvements through higher-order correlations. Numerical applications to molecular liquids validate the high fidelity of our approach, and our computational protocols demonstrate that a reduced model with simple energetics can reasonably estimate the thermodynamic entropy of CG models without performing any CG simulations. Overall, our findings present a systematic framework for estimating not only the entropy but also other thermodynamic properties of CG models, relying solely on information from the reference system.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes a simple hierarchical framework for predicting the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 878}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cf42eb58f68d55a7e9489f3f1ab4633f'}>,\n",
              "  <Document: {'content': 'thermodynamic entropies of various molecular CG systems, and presents a systematic framework for estimating not only the entropy but also other thermodynamic properties of CG models, relying solely on information from the reference system.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969\",\\n                    \"publicationVenue\": {\\n     ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 879}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'defc7c80627f087b730b6103aec5c392'}>,\n",
              "  <Document: {'content': '                  \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 880}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '51f71afc64347ecd522a7665c2090cd2'}>,\n",
              "  <Document: {'content': '           \"title\": \"SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents\",\\n                    \"abstract\": \"Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems\\' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 881}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd74fd6036a792fcc9cf1b7fba5710a07'}>,\n",
              "  <Document: {'content': 'findings demonstrate SOTOPIA\\'s promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence, is presented and it is found that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills.\"\\n                    }\\n                }\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 882}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be759ec594dae4744f8e32c395e30bd3'}>,\n",
              "  <Document: {'content': '     ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"f891e9eeedbf20cdc54429ffcc0402a10f48494e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44\",\\n                        \"name\": \"Annual Meeting of the Association for Computational Linguistics\",\\n                        \"type\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 883}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4013c487cba0f2adda18bd4725d28ef1'}>,\n",
              "  <Document: {'content': '\"conference\",\\n                        \"alternate_names\": [\\n                            \"Annu Meet Assoc Comput Linguistics\",\\n                            \"Meeting of the Association for Computational Linguistics\",\\n                            \"ACL\",\\n                            \"Meet Assoc Comput Linguistics\"\\n                        ],\\n                        \"url\": \"https://www.aclweb.org/anthology/venues/acl/\"\\n', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 884}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5e27bd9ef06cbfbfd9c49eea008b27a4'}>,\n",
              "  <Document: {'content': '                   },\\n                    \"title\": \"Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions\",\\n                    \"abstract\": \"Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 885}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e0b10fde462b9c2245fe980dd71978ec'}>,\n",
              "  <Document: {'content': 'a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"http://arxiv.org/pdf/2306.04597\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 886}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '151f5003994696691651bedc1c8df78d'}>,\n",
              "  <Document: {'content': 'paper empirically shows that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced, and argues that the few-shot de-biasing approach is highly feasible and practical.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 6,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"dcb4f2b9b0e6da0d629878d1ad0469aee3df2020\",\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 887}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ef1a201e0b916455983a34c7adbfceb9'}>,\n",
              "  <Document: {'content': '         \"publicationVenue\": {\\n                        \"id\": \"768b87bb-8a18-4d9c-a161-4d483c776bcf\",\\n                        \"name\": \"Computer Vision and Pattern Recognition\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                            \"CVPR\",\\n                            \"Comput Vis Pattern Recognit\"\\n                        ],\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 888}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ecc9f37e9232b50cf2439a6e630a57d'}>,\n",
              "  <Document: {'content': '                     \"issn\": \"1063-6919\",\\n                        \"url\": \"https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147\",\\n                        \"alternate_urls\": [\\n                            \"https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition\"\\n                        ]\\n                    },\\n                    \"title\": \"Understanding Masked Autoencoders via Hierarchical Latent Variable Models\",\\n                    \"abstract\": \"Masked autoencoder (MAE), a simple and effective', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 889}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7ad7b91072f2c6537a7b183764decb6b'}>,\n",
              "  <Document: {'content': 'self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.\",\\n                    \"openAccessPdf\": {\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 890}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5abb7c9707e2411b197d478f9eb170cc'}>,\n",
              "  <Document: {'content': '               \"url\": \"https://arxiv.org/pdf/2306.04898\",\\n                        \"status\": \"GREEN\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE, and formulate the underlying data-generating process as a hierarchical latent variable model, and shows that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model.\"\\n                    }\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 891}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3fe862741f6cbafe03857d3f3cce0e93'}>,\n",
              "  <Document: {'content': '            },\\n                {\\n                    \"paperId\": \"c0d7f8c69ced98eda8c2dba9b2ceb30c6f7702b0\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                        \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 892}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c66a73a74e164ea071c4483eb4a00faa'}>,\n",
              "  <Document: {'content': '                     \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Learning variational autoencoders via MCMC speed measures\",\\n                    \"abstract\": \"Variational autoencoders (VAEs) are popular likelihood-based generative models which can be efficiently trained by maximizing an Evidence Lower Bound (ELBO). There has been much progress in improving the expressiveness of the variational distribution to obtain tighter variational bounds and increased generative performance. Whilst previous work has leveraged Markov chain Monte Carlo (MCMC) methods for the construction of variational densities, gradient-based methods for adapting the proposal distributions for deep latent variable models have received less attention. This work suggests an entropy-based adaptation for a short-run Metropolis-adjusted', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 893}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ff3190945f98760d212b109c9c7eb5e'}>,\n",
              "  <Document: {'content': 'Langevin (MALA) or Hamiltonian Monte Carlo (HMC) chain while optimising a tighter variational bound to the log-evidence. Experiments show that this approach yields higher held-out log-likelihoods as well as improved generative metrics. Our implicit variational density can adapt to complicated posterior geometries of latent hierarchical representations arising in hierarchical VAEs.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://arxiv.org/pdf/2308.13731\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n             ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 894}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '71a45fd46e0b6a48709b6aed9238655a'}>,\n",
              "  <Document: {'content': '          \"text\": \"This work suggests an entropy-based adaptation for a short-run Metropolis-adjusted Langevin (MALA) or Hamiltonian Monte Carlo (HMC) chain while optimising a tighter variational bound to the log-evidence.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"b10c4b219befd102f2f4b3ca97a986b6a550681d\",\\n                    \"publicationVenue\": null,\\n                    \"title\": \"Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders\",\\n                    \"abstract\": \"The posterior collapse phenomenon in variational autoencoder (VAE), where the variational posterior distribution closely matches', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 895}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '63b13c3e72049b3af05e015a799fece5'}>,\n",
              "  <Document: {'content': 'the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAE preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAE performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAE. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAE: conditional VAE and hierarchical VAE. Specifically, via a non-trivial theoretical analysis of linear conditional VAE and hierarchical VAE with two levels of latent, we prove that the cause of posterior collapses in these models includes the correlation between the input and output of the conditional VAE and the effect of learnable encoder variance in the hierarchical VAE. We empirically validate our theoretical findings for linear conditional and hierarchical VAE and demonstrate that these results are also predictive for non-linear cases with extensive experiments.\",\\n                    \"openAccessPdf\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 896}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10f0fc5a4ca4450072c999ea359b2083'}>,\n",
              "  <Document: {'content': 'null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"It is proved that the cause of posterior collapses in these models includes the correlation between the input and output of the conditional VAE and the effect of learnable encoder variance in the hierarchical VAE.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 1,\\n            \"offset\": 0,\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 897}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd96bf7f2697b800696db9d1e6abd06e4'}>,\n",
              "  <Document: {'content': '       \"data\": [\\n                {\\n                    \"paperId\": \"89a448e914ad5e1a64f262660df971c8fc0a691e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"b0c05768-6345-45d7-b541-235edf6ead54\",\\n                        \"name\": \"IEEE International Conference on Automatic Face & Gesture Recognition\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 898}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '38219d8dc8c74a4e4408a3fb871443f1'}>,\n",
              "  <Document: {'content': '  \"IEEE Int Conf Autom Face Gesture Recognit\",\\n                            \"FG\",\\n                            \"IEEE International Conference on Automatic Face and Gesture Recognition\",\\n                            \"IEEE Int Conf Autom Face  Gesture Recognit\",\\n                            \"FGR\",\\n                            \"Form Gramm\",\\n                            \"Formal Grammar\"\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 899}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a5e4e3e6356cbc6ba97b4035691359bc'}>,\n",
              "  <Document: {'content': '                 ],\\n                        \"url\": \"http://www.wikicfp.com/cfp/program?id=1029\"\\n                    },\\n                    \"title\": \"Multimodal Feature Selection for Detecting Mothers\\' Depression in Dyadic Interactions with their Adolescent Offspring\",\\n                    \"abstract\": \"Depression is the most common psychological disorder, a leading cause of disability world-wide, and a major contributor to inter-generational transmission of psychopathol-ogy within families. To contribute to our understanding of depression within families and to inform modality selection and feature reduction, it is critical to identify interpretable features in developmentally appropriate contexts. Mothers with and without depression were studied. Depression was defined as history of treatment for depression and elevations in current or recent symptoms. We explored two multimodal feature selection strategies in', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 900}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a7eb268386f3dc970287607d052bb635'}>,\n",
              "  <Document: {'content': 'dyadic interaction tasks of mothers with their adolescent children for depression detection. Modalities included face and head dynamics, facial action units, speech-related behavior, and verbal features. The initial feature space was vast and inter-correlated (collinear). To reduce dimension-ality and gain insight into the relative contribution of each modality and feature, we explored feature selection strategies using Variance Inflation Factor (VIF) and Shapley values. On an average collinearity correction through VIF resulted in about 4 times feature reduction across unimodal and multimodal features. Collinearity correction was also found to be an optimal intermediate step prior to Shapley analysis. Shapley feature selection following VIF yielded best performance. The top 15 features obtained through Shapley achieved 78 % accuracy. The most informative features came from all four modalities sampled, which supports the importance of multimodal feature selection.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": null\\n                }\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 901}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '32a8e3189799990f91386554e131919e'}>,\n",
              "  <Document: {'content': '   ]\\n        },\\n        {\\n            \"total\": 12,\\n            \"offset\": 0,\\n            \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"a988c09b7e76e86a93edcbf3f284dd028b0fb406\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1901e811-ee72-4b20-8f7e-de08cd395a10\",\\n                        \"name\": \"arXiv.org\",\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 902}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '93f2af2dad76de1addf7eb34b458330'}>,\n",
              "  <Document: {'content': '   \"alternate_names\": [\\n                            \"ArXiv\"\\n                        ],\\n                        \"issn\": \"2331-8422\",\\n                        \"url\": \"https://arxiv.org\"\\n                    },\\n                    \"title\": \"Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications\",\\n                    \"abstract\": \"In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 903}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '55aa08accfbc967aa6fceb94eb711f8b'}>,\n",
              "  <Document: {'content': 'emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.\",\\n                    \"openAccessPdf\": {\\n            ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 904}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '45e62d94d0788bc01cca078395945fef'}>,\n",
              "  <Document: {'content': '           \"url\": \"http://arxiv.org/pdf/2306.04539\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"This work proposes two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings and validate these estimated bounds and show how they accurately track true interactions.\"\\n                    }\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 905}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7fee0a9b3380da82a93cc8276d7c241e'}>,\n",
              "  <Document: {'content': '            },\\n                {\\n                    \"paperId\": \"620cbaa991635f47bf32c8f6a9cc710480fad371\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"2ca4279c-8ed7-4280-8022-09e577923a09\",\\n                        \"name\": \"Frontiers in Neuroscience\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                           ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 906}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6382e978ca4dd12067bb797b3d131f35'}>,\n",
              "  <Document: {'content': '\"Front Neurosci\"\\n                        ],\\n                        \"issn\": \"1662-453X\",\\n                        \"url\": \"https://www.frontiersin.org/journals/neuroscience\",\\n                        \"alternate_urls\": [\\n                            \"http://journal.frontiersin.org/journal/neuroscience\",\\n                            \"http://www.frontiersin.org/neuroscience/\"\\n                        ]\\n                   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 907}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad3166197f81fc0dad8969d21e9f5eab'}>,\n",
              "  <Document: {'content': '},\\n                    \"title\": \"Contrastive self-supervised representation learning without negative samples for multimodal human action recognition\",\\n                    \"abstract\": \"Action recognition is an important component of human-computer interaction, and multimodal feature representation and learning methods can be used to improve recognition performance due to the interrelation and complementarity between different modalities. However, due to the lack of large-scale labeled samples, the performance of existing ConvNets-based methods are severely constrained. In this paper, a novel and effective multi-modal feature representation and contrastive self-supervised learning framework is proposed to improve the action recognition performance of models and the generalization ability of application scenarios. The proposed recognition framework employs weight sharing between two branches and does not require negative samples, which could effectively learn useful feature representations by using multimodal unlabeled data, e.g., skeleton sequence and inertial measurement unit signal (IMU). The extensive experiments are conducted on two benchmarks: UTD-MHAD and MMAct, and the results show that our proposed recognition framework outperforms both unimodal and multimodal baselines in action', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 908}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2b6b7677250f8473ca1bae14b5c080bc'}>,\n",
              "  <Document: {'content': 'retrieval, semi-supervised learning, and zero-shot learning scenarios.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.frontiersin.org/articles/10.3389/fnins.2023.1225312/pdf\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A novel and effective multi-modal feature representation and contrastive self-supervised learning framework is proposed to improve the action recognition performance of models and the generalization ability of application scenarios.\"\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 909}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5a3af8ed7d85d7b093beb95513c1ae0'}>,\n",
              "  <Document: {'content': '                }\\n                },\\n                {\\n                    \"paperId\": \"1866331f9f06e5ffc0496edbac5fe00f07094135\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"f3b9cfef-ea93-4f7f-a14f-95fbf796875e\",\\n                        \"name\": \"IEEE Geoscience and Remote Sensing Magazine\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 910}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f47c23b55b21bbb3beb217ab45bc2e65'}>,\n",
              "  <Document: {'content': '                       \"IEEE Geosci Remote Sens Mag\"\\n                        ],\\n                        \"issn\": \"2168-6831\",\\n                        \"url\": \"http://ieeexplore.ieee.org/servlet/opac?punumber=6245518\",\\n                        \"alternate_urls\": [\\n                            \"https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6245518\",\\n                            \"https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6245518\"\\n                 ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 911}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '425b53e31680f69d0e44eca3108a579d'}>,\n",
              "  <Document: {'content': '      ]\\n                    },\\n                    \"title\": \"SSL4EO-S12: A large-scale multimodal, multitemporal dataset for self-supervised learning in Earth observation [Software and Data Sets]\",\\n                    \"abstract\": \"Self-supervised pretraining bears the potential to generate expressive representations from large-scale Earth observation (EO) data without human annotation. However, most existing pretraining in the field is based on ImageNet or medium-sized, labeled remote sensing (RS) datasets. In this article, we share an unlabeled dataset Self-Supervised Learning for Earth Observation-Sentinel-1/2 (SSL4EO-S12) to assemble a large-scale, global, multimodal, and multiseasonal corpus of satellite imagery. We demonstrate SSL4EO-S12 to succeed in self-supervised pretraining for a set of representative methods: momentum contrast (MoCo), self-distillation with no labels (DINO), masked autoencoders (MAE), and data2vec, and multiple downstream applications, including scene classification, semantic segmentation, and change detection. Our benchmark results prove the effectiveness of SSL4EO-S12 compared to existing datasets. The dataset, related source', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 912}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3e306086fadbbc28350b923b09870619'}>,\n",
              "  <Document: {'content': 'code, and pretrained models are available at https://github.com/zhu-xlab/SSL4EO-S12.\",\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"An unlabeled dataset Self-Supervised Learning for Earth Observation-Sentinel-1/2 (SSL4EO-S12) is shared to assemble a large-scale, global, multimodal, and multiseasonal corpus of satellite imagery and proves the effectiveness of SSL4EO-S12 compared to existing datasets.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 913}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd262d9c229a4402e06b39afc9256b598'}>,\n",
              "  <Document: {'content': '       \"total\": 2,\\n            \"offset\": 0,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"96cc4a8f42b415c5c285f417cd052d85c77aea9e\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"1736bbcd-647a-4fa5-9478-e22de96eec12\",\\n                        \"name\": \"Journal of the American Academy of Child and Adolescent Psychiatry\",\\n                        \"type\": \"journal\",\\n                       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 914}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6066999edc3372c1c81d2f5f9af12d8'}>,\n",
              "  <Document: {'content': '\"alternate_names\": [\\n                            \"J Am Acad Child Adolesc Psychiatry\"\\n                        ],\\n                        \"issn\": \"0890-8567\",\\n                        \"url\": \"http://www.jaacap.com/\",\\n                        \"alternate_urls\": [\\n                            \"http://gateway.ovid.com/ovidweb.cgi?AN=00004583-000000000-00000&D=ovft&MODE=ovid&PAGE=toc&T=JS\",\\n                            \"http://www.sciencedirect.com/science/journal/08908567\"\\n          ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 915}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2ed617fddb888fbb71ca95b1ce33b855'}>,\n",
              "  <Document: {'content': '             ]\\n                    },\\n                    \"title\": \"Intensive Longitudinal Assessment of Adolescents to Predict Suicidal Thoughts and Behaviors.\",\\n                    \"abstract\": null,\\n                    \"openAccessPdf\": null,\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Intensive longitudinal assessment through personal smartphones offers a feasible method to assess variability in adolescents\\' emotional experiences and suicide risk.\"\\n   ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 916}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '33f0387cc4a73774df5856b0afb4ca1a'}>,\n",
              "  <Document: {'content': '                }\\n                },\\n                {\\n                    \"paperId\": \"6f27302c76f954e79b48a2abe1f8ee3f552ada18\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"278131df-030d-4e6c-b083-d57f3b740dc4\",\\n                        \"name\": \"JMIR Research Protocols\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n       ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 917}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a88fd7491ce26fac9655f15f660a2cea'}>,\n",
              "  <Document: {'content': '                    \"JMIR Res Protoc\"\\n                        ],\\n                        \"issn\": \"1929-0748\",\\n                        \"url\": \"https://www.researchprotocols.org/\",\\n                        \"alternate_urls\": [\\n                            \"http://www.researchprotocols.org/index\"\\n                        ]\\n                    },\\n      ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 918}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'daed447a2718f71ea0e9072cc91744d5'}>,\n",
              "  <Document: {'content': '             \"title\": \"Real-Time Real-World Digital Monitoring of Adolescent Suicide Risk During the Six Months Following Emergency Department Discharge: Protocol for an Intensive Longitudinal Study\",\\n                    \"abstract\": \"Background Suicide is the second leading cause of death in adolescents, and self-harm is one of the strongest predictors of death by suicide. The rates of adolescents presenting to emergency departments (EDs) for suicidal thoughts and behaviors (STBs) have increased. Still, existing follow-up after ED discharge is inadequate, leaving a high-risk period for reattempts and suicide. There is a need for innovative evaluation of imminent suicide risk factors in these patients, focusing on continuous real-time evaluations with low assessment burden and minimal reliance on patient disclosure of suicidal intent. Objective This study examines prospective longitudinal associations between observed real-time mobile passive sensing, including communication and activity patterns, and clinical and self-reported assessments of STB over 6 months. Methods This study will include 90 adolescents recruited on their first outpatient clinic visit following their discharge from the ED due to a recent STB. Participants will', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 919}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '99875d7653f5f27bc3fe16b40123f6c5'}>,\n",
              "  <Document: {'content': 'complete brief weekly assessments and be monitored continuously for their mobile app usage, including mobility, activity, and communication patterns, over 6 months using the iFeel research app. Participants will complete 4 in-person visits for clinical assessment at baseline and at the 1-, 3-, and 6-month follow-ups. The digital data will be processed, involving feature extraction, scaling, selection, and dimensionality reduction. Passive monitoring data will be analyzed using both classical machine learning models and deep learning models to identify proximal associations between real-time observed communication, activity patterns, and STB. The data will be split into a training and validation data set, and predictions will be matched against the clinical evaluations and self-reported STB events (ie, labels). To use both labeled and unlabeled digital data (ie, passively collected), we will use semisupervised methods in conjunction with a novel method that is based on anomaly detection notions. Results Participant recruitment and follow-up started in February 2021 and are expected to be completed by 2024. We expect to find prospective proximal associations between mobile sensor communication, activity data, and STB outcomes. We will test predictive models for suicidal behaviors among high-risk adolescents. Conclusions Developing digital markers of STB in a real-world sample of high-risk', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 920}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '78159cdddde246a1a2d702a01c702c5f'}>,\n",
              "  <Document: {'content': 'adolescents presenting to ED can inform different interventions and provide an objective means to assess the risk of suicidal behaviors. The results of this study will be the first step toward large-scale validation that may lead to suicide risk measures that aid psychiatric follow-up, decision-making, and targeted treatments. This novel assessment could facilitate timely identification and intervention to save young people\\\\u2019s lives. International Registered Report Identifier (IRRID) DERR1-10.2196/46464\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://www.researchprotocols.org/2023/1/e46464/PDF\",\\n                        \"status\": \"GOLD\"\\n                    },\\n                    \"tldr\": {\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 921}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '76c7d785e7a8323483256a1d0b46e15'}>,\n",
              "  <Document: {'content': '   \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"Developing digital markers of STB in a real-world sample of high-risk adolescents presenting to ED can inform different interventions and provide an objective means to assess the risk of suicidal behaviors.\"\\n                    }\\n                }\\n            ]\\n        },\\n        {\\n            \"total\": 0,\\n            \"offset\": 0\\n        },\\n        {\\n            \"total\": 5,\\n            \"offset\": 0,\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 922}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2aa37f76250fe2e9a40c6c1a5452ab50'}>,\n",
              "  <Document: {'content': '   \"next\": 3,\\n            \"data\": [\\n                {\\n                    \"paperId\": \"64703e760f662b1c0f647931bb63fe57e5ba91e4\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"d11025b6-9660-45df-b13a-555e3ff4ceca\",\\n                        \"name\": \"International Conference on Multimodal Interaction\",\\n                        \"type\": \"conference\",\\n                        \"alternate_names\": [\\n                    ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 923}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '789944e990e25082ff70d4127228bb14'}>,\n",
              "  <Document: {'content': '       \"Int Conf Multimodal Interact\",\\n                            \"International Conference on Multimodal Interfaces\",\\n                            \"Int Conf Multimodal Interface\",\\n                            \"ICMI\"\\n                        ],\\n                        \"url\": \"https://en.wikipedia.org/wiki/ACM/IEEE_Virtual_Reality_International_Conference\"\\n                    },\\n                    \"title\": \"Neural Mixed Effects for Nonlinear Personalized Predictions\",\\n  ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 924}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e0787aaff14837fc68561f906eb8f1f'}>,\n",
              "  <Document: {'content': '                 \"abstract\": \"Personalized prediction is a machine learning approach that predicts a person\\\\u2019s future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a neural network in a scalable manner1. NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling. Empirically, we observe that NME improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 925}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9dca56df5f0524f4f8283a2c0177ebab'}>,\n",
              "  <Document: {'content': 'mother-adolescent dataset to predict affective state sequences where half the mothers experience symptoms of depression. Furthermore, we evaluate NME for two model architectures, including for neural conditional random fields (CRF) to predict affective state sequences where the CRF learns nonlinear person-specific temporal transitions between affective states. Analysis of these person-specific transitions on the mother-adolescent dataset shows interpretable trends related to the mother\\\\u2019s depression symptoms.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://dl.acm.org/doi/pdf/10.1145/3577190.3614115\",\\n                        \"status\": \"HYBRID\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 926}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8a925fa9937d162e5cc79cf4be22a7a2'}>,\n",
              "  <Document: {'content': '\"tldr@v2.0.0\",\\n                        \"text\": \"NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling and improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent datasets to predict affective state sequences where half the mothers experience symptoms of depression.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"afe61a1766243b5cd249d72eef468c85b5faf944\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"30931fa0-800b-4d7c-8ad5-f67035ce8fb7\",\\n         ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 927}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f879655da0df78708a7f02a81a35b277'}>,\n",
              "  <Document: {'content': '              \"name\": \"Turkish Journal of Agriculture and Forestry Sciences\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\": [\\n                            \"Turkish Journal of Agriculture and Forestry\",\\n                            \"Turk J Agric For Sci\",\\n                            \"Turk J Agric For\"\\n                        ],\\n        ', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 928}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '756b67ebce9ba11746f24bde45c4eed'}>,\n",
              "  <Document: {'content': '               \"issn\": \"1300-011X\",\\n                        \"url\": \"http://journals.tubitak.gov.tr/agriculture/index.php\"\\n                    },\\n                    \"title\": \"Artificial intelligence as an alternative modelling strategy for reliable height-diameter predictions of mixed-oaks species\",\\n                    \"abstract\": \": Forest management and sustainable timber production rely on forest measurements that include tree height. However, the fieldwork needed for tree height measurements is time-consuming, and several times, hard to obtain. The solution to this problem is the construction of reliable height-diameter ( h-d ) models that can provide accurate tree height predictions. To this direction, a modified Gompertz model that included dominant height and diameter at breast height was used to predict tree height. In order to investigate the most promising modelling method,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 929}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b71cde3155bdcb25f533865437369602'}>,\n",
              "  <Document: {'content': 'six alternative approaches were evaluated; these involved both regression and artificial intelligence techniques and produced the (i) fixed-effects (FE), (ii) mixed-effects (ME), (iii) three-quantile regression (3QR), (iv) five-quantile regression (5QR), (v) general regression neural network (GRNN), and (vi) support vector regression (SVR) models. The accuracy of the developed h-d models was studied in this work for mixed-oak stands ( Quercus cerris L., Quercus petraea (Matt.) Liebl. and Quercus frainetto Ten.) in T\\\\u00fcrkiye. For this purpose, 1735 trees were measured in 52 sample plots in total. The tree variability among sampling plots was incorporated in the constructed models through the dominant heights and the tree height variance in each sample plot, while the models were localized using one to five oak trees, as calibration samples, per plot. The study showed that the ME constructed model with root mean square value equal to 1.8331 was constantly superior to nonlinear regression methods used, while the GRNN and the SVR models with root mean square values equal to 1.8330 and 1.8279, respectively, showed similar predictive ability with the later to prevail, as compared to the rest of the models tested. Finally, five trees per plot were found to be an acceptable trade-off between sampling', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 930}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '92ab6ccce7d72c929899985604588fe0'}>,\n",
              "  <Document: {'content': 'cost and predictive capacity and reliability.\",\\n                    \"openAccessPdf\": {\\n                        \"url\": \"https://journals.tubitak.gov.tr/cgi/viewcontent.cgi?article=3080&context=agriculture\",\\n                        \"status\": \"CLOSED\"\\n                    },\\n                    \"tldr\": {\\n                        \"model\": \"tldr@v2.0.0\",\\n                        \"text\": \"A modified Gompertz model that included dominant height and diameter at breast height was used to predict tree height and five trees per plot were found to be an acceptable trade-off between sampling', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 931}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '15a500b5e178f179f345dd5e06740953'}>,\n",
              "  <Document: {'content': 'cost and predictive capacity and reliability.\"\\n                    }\\n                },\\n                {\\n                    \"paperId\": \"32058dc02319acd21d5ac591335eda74a98a99df\",\\n                    \"publicationVenue\": {\\n                        \"id\": \"cd9a76b6-d411-43c3-8fdc-6d7912251f82\",\\n                        \"name\": \"Forest Systems\",\\n                        \"type\": \"journal\",\\n                        \"alternate_names\":', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 932}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd31563f7ae1f25f676cd9b14811d30e0'}>,\n",
              "  <Document: {'content': '[\\n                            \"For Syst\"\\n                        ],\\n                        \"issn\": \"2171-5068\",\\n                        \"url\": \"http://recyt.fecyt.es/index.php/IA/index\"\\n                    },\\n                    \"title\": \"A comparison of artificial neural networks and regression modeling techniques for predicting dominant heights of Oriental spruce in a mixed stand\",\\n                    \"abstract\": \"Aim of study: This paper introduces comparative evaluations of artificial neural network models and regression', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 933}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e94fcf860c471d7bc66c63a6dc5528c9'}>,\n",
              "  <Document: {'content': 'modeling techniques based on some fitting statistics and desirable characteristics for predicting dominant height. \\\\nArea of study: The data of this study were obtained from Oriental spruce (Picea orientalis L.) felled trees in even-aged and mixed Oriental spruce and Scotch pine (Pinus sylvestris L.) stands in the northeast of T\\\\u00fcrkiye. \\\\nMaterial and methods: A total of 873 height-age pairs were obtained from Oriental spruce trees in a mixed forest stand. Nonlinear mixed-effects models (NLMEs), autoregressive models (ARM), dummy variable method (DVM), and artificial neural networks (ANNs) were compared to predict dominant height growth. \\\\nMain results: The best predictive model was NLME with a single random parameter (root mean square error, RMSE: 0.68 m). The results showed that NLMEs outperformed ARM (RMSE: 1.09 m), DVM in conjunction with ARM (RMSE: 1.09 m), and ANNs (RMSE: from 1.11 to 2.40 m) in the majority of the cases. Whereas considering variations among observations by random parameter(s) significantly improved predictions of dominant height, considering correlated error terms by autoregressive correlation parameter(s) enhanced slightly the predictions. ANNs generally underperformed compared to NLMEs, ARM, and DVM with ARM. \\\\nResearch highlights: All regression techniques fulfilled the desirable characteristics such as sigmoidal pattern, polymorphism, multiple asymptotes, base-age invariance,', 'content_type': 'text', 'score': None, 'meta': {'_split_id': 934}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd533807eb916fe5cdf8e4293c098f9'}>,\n",
              "  ...],\n",
              " 'root_node': 'File',\n",
              " 'params': {},\n",
              " 'file_paths': ['/content/drive/My Drive/Colab Notebooks/NLP2/reference/scs.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/history.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/commencement.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/buggy.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/LTI_faculty_directory_traning_80.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/LTI_faculty_directory_test_20.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/lti_program2_test_2.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/lti_program2_train_8.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_details.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/merged_courses_offered_test_25.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/merged_courses_offered_train_75.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/program_details_test_1:3.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/program_details_train_2:3.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/Spring Carnival Schedule_test_2.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/2324-pgh-class-meeting-days.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_475.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_307.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_313.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_111.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_461.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_449.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_139.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_29.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_105.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_267.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_515.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_501.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_298.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_299.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_528.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_529.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_272.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_500.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_514.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_138.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_104.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_28.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_306.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_110.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_448.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_460.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_474.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_312.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_462.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_304.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_310.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_476.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_338.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_106.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_489.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_16.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_502.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_516.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_112.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_264.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_503.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_258.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_265.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_259.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_271.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/Handbook-MSAII-2022-2023.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/2324-academic-calendar-list-view.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_517.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_17.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_113.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_488.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_107.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_339.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_311.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_477.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_463.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_329.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_301.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_305.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_473.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_467.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_498.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_249.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_103.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_117.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_507.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_513.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_261.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_275.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_260.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_506.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_274.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_512.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_248.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_116.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_102.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_499.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_472.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_12.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_300.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_328.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_466.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_314.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_458.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_316.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_128.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_464.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_470.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_302.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_10.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_38.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_100.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_262.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_504.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_114.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_289.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_276.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_288.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_510.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_263.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_277.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_505.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_101.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_39.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_115.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_11.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_465.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_129.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_317.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_303.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_370.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_459.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_471.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/MIIS Handbook_2023 - 2024.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_416.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_402.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_364.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_358.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_172.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_166.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_76.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_62.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_89.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_199.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_210.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_204.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_238.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_239.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_211.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_198.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_167.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_205.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_173.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_359.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_77.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_403.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_365.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_401.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_367.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_415.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_417.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_398.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_373.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_429.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_61.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_165.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_171.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_75.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_213.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_159.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_206.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_49.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_48.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_212.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_158.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_170.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_164.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_60.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_399.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_428.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_400.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_366.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_404.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_438.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_414.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_389.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_362.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_410.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_376.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_58.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_148.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_64.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_70.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_160.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_174.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_8.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_217.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_9.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_203.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_202.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_216.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_175.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_161.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_65.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_149.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_377.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_388.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_59.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_411.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_405.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_439.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_363.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_413.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_349.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_375.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_73.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_177.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_407.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_361.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_163.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_67.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_188.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_98.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/2425-academic-calendar-list-view.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_229.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_214.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_201.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_215.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_228.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_200.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_99.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_189.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_66.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_162.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/2324-doctoral-academic-calendar-list-view.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_72.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_176.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_360.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_412.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_406.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_374.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_351.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_348.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_345.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_379.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_423.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_437.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_386.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_392.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_57.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_153.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_147.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_94.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_43.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_190.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_80.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_184.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_225.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_219.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_231.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_7.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_224.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_230.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_218.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_6.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_81.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_185.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_191.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_95.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_146.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_56.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_152.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_42.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_387.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_436.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_422.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_344.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_393.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_346.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_420.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_434.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_352.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_385.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_408.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_391.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_144.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_40.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_150.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_178.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_68.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_83.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_193.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_187.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_97.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_226.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_227.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_4.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_232.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_5.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_192.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_233.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_96.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_82.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_69.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_186.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_41.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_55.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_179.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_145.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_435.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_390.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_384.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_409.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_419.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_353.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_347.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_421.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_425.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_343.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_431.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_357.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_79.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_394.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_380.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_45.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_141.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_169.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_51.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_86.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_182.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_92.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_196.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_237.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_1.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_223.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_222.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_236.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_0.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_93.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_197.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_50.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_87.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_183.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_140.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_44.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_154.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_78.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_395.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_168.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_430.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_424.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_356.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_342.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_418.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_432.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_368.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_340.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_426.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_397.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_383.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_52.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_46.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_195.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_142.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_91.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_85.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_181.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_220.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_208.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_2.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_234.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/MLT Student Handbook 2023 - 2024.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_235.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_221.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_209.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_3.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_84.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_194.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_90.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_180.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_157.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_143.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_47.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_382.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_427.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_341.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_396.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_454.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_433.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_369.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_355.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_332.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_326.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_468.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_440.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_483.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_497.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_34.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_130.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_20.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_124.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_118.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_246.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_520.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_285.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_508.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_252.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_291.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_284.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_290.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_521.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_509.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_253.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_247.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_131.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_21.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_125.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_119.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_496.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_482.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_35.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_441.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_455.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_469.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_333.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_319.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_331.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_457.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_443.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_480.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_494.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_127.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_23.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_37.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_133.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/MCDS Handbook 23-24 AY.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_245.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_251.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_292.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_279.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_523.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_244.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_293.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_278.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_132.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_522.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_250.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_22.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_126.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_495.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_481.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_324.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_318.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_442.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_456.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_330.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_308.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_446.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_320.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_452.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_334.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_491.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_485.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_122.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_26.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_268.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/PhD_Student_Handbook_2023-2024.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_254.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_526.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_532.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_297.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_240.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_282.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_283.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_296.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_527.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_241.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_533.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_255.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_269.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_33.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_27.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_490.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_335.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_453.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_484.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_309.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_447.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_445.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_337.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_451.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_323.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_479.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_492.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_486.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_19.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_109.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_31.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_121.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_135.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_243.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_519.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_25.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_531.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_525.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_294.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_295.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_257.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_281.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_530.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_524.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_242.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_256.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_518.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_120.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_24.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_30.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_134.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_18.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_108.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_487.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_322.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_336.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_493.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_444.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_450.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/paper_478.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/cmu_fact_sheet_01.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/cmu_fact_sheet_02.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/cmu_important_date_test_1:3.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/Spring Carnival Schedule_train_8.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/cmu_important_date_traning_2:3.txt',\n",
              "  '/content/drive/My Drive/Colab Notebooks/NLP2/reference/athletics.txt'],\n",
              " 'node_id': 'DocumentStore'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "files_to_index = [os.path.join(doc_dir, f) for f in os.listdir(doc_dir) if f.endswith('.txt')]\n",
        "indexing_pipeline.run_batch(file_paths=files_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlvh2FUFmXvf"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import BM25Retriever\n",
        "retriever = BM25Retriever(document_store=document_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4TGusc5mXvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "48c2f0fa5e304757993b0c9f3cc89dc1",
            "b18952c2ebc74e279d75016a54e32094",
            "b31be110f01a4184b01ba9291a34ad83",
            "8378aa12de4348969f6cc56554d13132",
            "f58042af6c0b4d189c458f9859c1a42e",
            "c164764eef3643448b21bf9fdb44a464",
            "9f562f7e12b5487fb30c5b4a1acaf9b1",
            "58cd7d979ab04c49b9a48159d188050d",
            "490c285fb079445bb37a8df2a3e669ac",
            "6106158fe93042e9af78e525154b4d65",
            "37869b7cd7b841699080126abb9b7db1",
            "ed1d92c77840443a9ffcbede0c3ff97b",
            "8cb8ca59fbd649febe6effabbbfb5bd9",
            "3d57d822da444ea584cabc0a2981aeb7",
            "4ad0ff5a3e554a4c8064954b7f62af2f",
            "c3f7e9ce639040628428d60acb2bf500",
            "373493e93fbe48b59a265586d645d4eb",
            "ad4bd2550fe84fea83692180db054376",
            "c1f3a80eb73b44e1a4ef4746878f865d",
            "8cce4bf58fac4288b8170131b0204623",
            "11349692d0614f52bb2115d41a96dc8a",
            "b2641f14af294dd28fbec018755ee1e2",
            "8e14a925eb9747c7b161b402b55a5ba5",
            "344a5d26bbcf469ab1d1b30ca9804dde",
            "6548bc06655c476a8ce091a3ef17f28a",
            "c88bac54e3ba4ca5bf8e03a6cb3ac76e",
            "95a2cd05397343609079b83bb672c605",
            "4f702037f0084d78bd39be03c1560e55",
            "4b891f679e7d4d53bc3a553da0a74ec7",
            "41871c4df2d94efd9d048a907ad22774",
            "e4026e2d73fa4807b40ff040c375c139",
            "4ce97c1186924ed5bd765630e059abe4",
            "d23dd3a8b1d54e0d8540512f67fa9c13",
            "34cc58a8af944f30aab104aafe9b6b7d",
            "481a33dab98c4879ae894f0c657e445c",
            "9947bc83666b4cdda12f60f09c113f6c",
            "abb7228db67a4ac893126dedb3bd19a4",
            "de6f2dd2dd7845a9976f80a8bc944228",
            "3482cfa627d44301910c2d9a261bb584",
            "63a9b97133dd4dc188fef005c1ffd60a",
            "7d00d2d1da9c4ba99f6399bb3feee61a",
            "698a95f05e82476aa10c9fe59522f641",
            "dfc594c2bbb649008ee151332b7c5fc4",
            "59a9bd975f004fb8acac1eee81e4769c",
            "ccfb510a094f440d9ae01ce68f3f4cc7",
            "c60b81c14aee487084cb85e9f2bc8b2b",
            "328f9ddc822e48318acf77ecef396c0e",
            "9d61a2235a88432cacb6d4e6d4382d1e",
            "da772006ad37494293fcdc5c46987a4e",
            "cd0b198324c74c23b89ab04772024412",
            "0710cee2c3ce4a7b99af6650005a1235",
            "a964770def024335a1647f90175d2f9a",
            "e5e514d605a24dbb956c5c934f186ed9",
            "9be58183ccb84482a5a6ab5975b4d474",
            "9fe3429c59a144e7b31a6cdb95f8c3a6",
            "7e51b99518184a8bb7fcc86aa6209c4d",
            "86adba64214f4d5f943882396c66d859",
            "aa20383988574695869a29119c484504",
            "dc20378c34cb400aac44249a65ca8288",
            "98fd54f25b7341149eb78d55c76bb584",
            "ca22d257d57c4c6696c785459e4cab6f",
            "96b16a3802dd4bc4a268f8130aad8de4",
            "7436da62e7cd424288556836a0b6b348",
            "ab58efc1a0234b76b6158674d70d10c4",
            "09a638372424498ba04b7c9558c8a138",
            "71eb4e1c2d254ab49accf9037d015332"
          ]
        },
        "outputId": "d5b17311-7b6c-4f5b-be2f-d64e08b817d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48c2f0fa5e304757993b0c9f3cc89dc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed1d92c77840443a9ffcbede0c3ff97b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e14a925eb9747c7b161b402b55a5ba5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34cc58a8af944f30aab104aafe9b6b7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccfb510a094f440d9ae01ce68f3f4cc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e51b99518184a8bb7fcc86aa6209c4d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from haystack.nodes import FARMReader\n",
        "\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBlvkH3amXvm"
      },
      "outputs": [],
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "querying_pipeline = Pipeline()\n",
        "querying_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
        "querying_pipeline.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Path to the question file, adjust the path according to your setup\n",
        "questions_file_path = '/content/drive/My Drive/Colab Notebooks/NLP2/FAQ_style_QA/Q_Train/NLP2_Q_traning_all.txt'\n"
      ],
      "metadata": {
        "id": "600KquVMTRJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a20cdc-8687-46c7-99d9-508630292dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "answers_file_path = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer.txt'\n",
        "\n",
        "# Open and read the questions file\n",
        "with open(questions_file_path, 'r') as file:\n",
        "    questions = file.readlines()\n",
        "\n",
        "# Process each question\n",
        "with open(answers_file_path, 'w') as answers_file:\n",
        "    # Process each question\n",
        "    for question in questions:\n",
        "        question = question.strip()  # Remove any leading/trailing whitespace\n",
        "        if question:  # Check if the question is not empty\n",
        "            prediction = querying_pipeline.run(\n",
        "                query=question,\n",
        "                params={\"Retriever\": {\"top_k\": 5}, \"Reader\": {\"top_k\": 1}}\n",
        "            )\n",
        "\n",
        "            # Check if there are any answers returned\n",
        "            if prediction['answers']:\n",
        "                # Directly access the 'answer' attribute of the first Answer object\n",
        "                answer_text = prediction['answers'][0].answer\n",
        "                answer_text = answer_text.replace('\\n', ' ')\n",
        "                con_text = prediction['answers'][0].context\n",
        "                print(f\"Answer: {answer_text}\")\n",
        "                # Write the answer to the answers file, one answer per line\n",
        "                answers_file.write(f\"{answer_text}\\n\")\n",
        "            else:\n",
        "                # If no answer was found, write a placeholder or a notification\n",
        "                answers_file.write(\" \\n\")\n"
      ],
      "metadata": {
        "id": "6GZlPMGgQ6qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb3dcb4-3a07-448a-ff10-cb53dfa76cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.05s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: August 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.87s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: September 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.87s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: September 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: September 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 28 July 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.37s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.02s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: February 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.87s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Oct 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Oct 16-20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.91s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: October 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.50s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: October 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.57s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Evening classes after\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 15 -April 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 18 August 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.89s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: October 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.79s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: October 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.10s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: May 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 28 July 2023 Page 3 of 3August 1 Th Semester & Mini-6 voucher deadline (4) August 2 F Semester & Mini-6 Final Exams*** August 2 F Semester & Mini-6 Faculty Course Evaluations close August 6 T Semester & Mini-6 Final Grades Due by 4 pm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.98s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: January 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.79s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 15 -April 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.08s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: February 5 W\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.02s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: January 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: February 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: November 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March 4 -March 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.73s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 4 pm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Mar 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.38s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.72s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:43<00:00, 43.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.94s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.54s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Independence Day\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.41s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.00s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: #1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.29s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.13s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: #1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.20s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3.11.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.55s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: more than 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.38s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Nearly triple the   national average\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.58s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Andrew Carnegie\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.60s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1,000+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.81s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Java\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.43s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: the University Center, 412 -268-2064 .  The Office of the Assistant Vice Provost for Graduate Education (AVPGE) offers a robust schedule of professional development opportunities. Some are geared towards a specific  population (master’s students, Ph.D.  students at the beginning of their program, graduate  students seeking tenure track positions, etc.) and others are open to all graduate students\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.07s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Richard Cyert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.12s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.49s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Kevlar Fiber\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.08s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.55s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: invented\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: David Coulter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.50s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: all graduate students\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.48s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: graded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.12s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: support to more than 110  centers around the world\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.09s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.37s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Andy Warhol\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.70s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Global University\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.98s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 56 countries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.42s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: three\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.50s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 412- 268-6298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.23s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Systems, Analytics, and Human-Centered Data Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.44s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Students must also complete a capstone project in which they work on a research project at CMU or on an industry-sponsored project\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.57s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3.3.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.08s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 10-601 Introduction to Machine  Learning; 05-839 Interactive Data Science; 15-619 Cloud Computing; 11-631 Data  Science Seminar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Completion and Certification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Language  Technologies Institute\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.08s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: seven years\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.60s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.23s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: eight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.86s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: minimum of 36 units\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.67s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 11-637 Foundations of Computational Data  Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: entirely\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Internship Conversational Interfaces\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.91s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: several\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: may be taken with prior permission\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.49s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: zero tolerance policy for multiple Academic Integrity Violations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.30s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: dismissal from the graduate program\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.91s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: petition for approval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.41s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: None required\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: summer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: online through The HUB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.49s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The university has a very clear and specific protocol for responding to alleged violations of  academic integrity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: not permitted for courses used to satisfy a degree  requirement\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: may be possible on a case-by-case basis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 4.1.5 External Internships and Job Interviewing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: using SIO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: I -9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Students who have no project funding may be provided with partial funding,  with a larger amount available for travel to presen t a refereed conference paper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.32s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Jennifer M Lucas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carolyn Penstein Rosé\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.92s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Robert Frederking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Gates-Hillman Center 6515\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.05s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.14s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The M.S. in Artificial Intelli gence and Innovation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.43s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Michael I. Shamos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: B or better\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 87 units\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.10s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: summer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: four\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Elective courses in other Schools at Carnegie Mellon may be taken with prior permission\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.20s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: after accepting admission and  receiving an Andrew ID\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.94s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: terminated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: suspension or expulsion under University policies\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:21<00:00, 21.14s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Satisfactory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: placed on academ ic probation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.10s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Amber Vivis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:20<00:00, 20.88s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LTI policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:26<00:00, 26.14s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: must follow the university’s leave of  absence process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.54s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 87 units\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ch eck with OIE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.38s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ZERO TOLERANCE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.43s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: replaces the previous grade\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:20<00:00, 20.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: if their college's and department's policies allow this.).  The MSAII program does not accept transfer credits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.55s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Mellon University offers students the o pportunity to take courses for credit through a  cross-registration program\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.44s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Form I-9 must be completed within 3  business days of beginning work for any type of compensation (stipend or employment).  Additional details are highlighted below. To ensure compliance with federal law,  Carnegie Mellon University maintains the Employment Eligibility Verification (I-9) Policy  covering the university’s I-9 and E-Verify requirements:   Every individual receiving a stipend from CMU or employed by CMU must comply  with the I-9 Policy by completing the Form I-9 within three business days following  the first day of stipend start date/employment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: one eight -hour day per seven -day week, only with the advisor's consent\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: permission may be  granted by the Director allowing a student to participate in short portions of the program  remotely\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.32s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 11.5.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.48s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Every MSAII student will be assigned an advisor,  normally the Program Director\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.52s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: five consecutive semesters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.94s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: online\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Amber Vivis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.70s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Amber Vivis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: David Garlan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.96s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 6 units\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.99s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: $80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.58s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: MIIS Program D irector\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.12s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Human Language for Language Technologies\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.47s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: opportunities  to apply and hone new skills while building state -of-the-art systems\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three academic semesters (fall, spring, fall) and a summer internship\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Analytics, Systems, or Human- Centered Data Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.19s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: under the supervision of their advisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: an opportunity to apply new skills in a  professional setting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.92s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.27s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.86s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: acquires data, software, and other resources required for successful  completion of the project\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: B - or better grade\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: not permitted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.82s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 6.1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: online\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.48s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 6.2.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.55s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: strongly discouraged\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.08s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: leaving the universit y temporarily with a co mmitment to return\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Leave of Absence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.75s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 6.2.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.91s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Athletics, Physical Fitness & Recreation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 7.2 Sexual Misconduct Policy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 7.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: inappropriate and prohibited\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.57s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: I -9 and E -Verify\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.66s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: apply for TA positions only with the permission of the program director\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: software development supervised by their advisor (24 units equivalent to two courses); a summer internship\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.52s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 14.41%.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 78.24%.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.20s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: https://webarena.dev/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.73s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: e-commerce, social forum discussions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.69s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Robert Lo†Abishek Sridhar†Xianyi Cheng Tianyue Ou Yonatan Bisk Daniel Fried Uri Alon Graham Neubig Carnegie Mellon University {shuyanzh, fangzhex, gneubig}@cs.cmu.edu ABSTRACT With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic andreproducible\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 14.41%,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:27<00:00, 27.75s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Deng et al., 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.43s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: research advisor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.45s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.84s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: higher\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Mellon University\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.70s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: natural language input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: five\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: reliably evaluating their generated output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.19s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: natural language input\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: encodes the natural lan- guage input preceding the generated code\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.75s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: four\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.14s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: higher correlation with human preference and with functional correctness\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.95s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: met- rics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: higher\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: multiple datasets and programming languages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: modeling the consistency between the generated code and its given natural language context as well\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.84s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: more likely to be preferred by humans\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.13s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ex- ponentiation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: the context for the generated code\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: encodes the natural lan- guage input preceding the generated code\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: reliably evaluating their generated output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.86s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: unsupervised and does not depend on any specific dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.83s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: trivially match variable names according to their semantic similarity and their functional role in the code\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.03s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: balanced and robust measure of their performance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.67s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: encodes the natural lan- guage input preceding the generated code\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.29s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: agents that perform tasks on the web\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: e-commerce, social\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.05s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 78.24%.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: efficiently tackling downstream applications\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.99s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: simplify the hard problem of counting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: the problem of picking any object in anyunseen environment, and placing it in a commanded location\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.97s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: deficiencies in perception, navigation, and manipulation skills\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: to extract object-identifying information from the language command and image\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.00s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Humans are social beings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.54s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: perform both understanding and generation tasks involving non- linguistic modalities such as images or videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.60s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: it is confirmed that the trained system can generate appropriate captions according to input videos from the viewpoint of human judgment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: through the fine-tuning of the BLIP-2 model using the LoRa method on a standard commercial GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: title\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.87s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: appearance, dialogue mechanism, emotional model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.75s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: young children actively acquire language through interactions with their surrounding environment and caretakers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.84s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: PLMs model con- structions as gestalts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.60s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: investigating whether false-belief reasoning (FBR) and source-monitoring ability (SMA), abilities within the theory of mind (ToM) framework, trigger the comprehension of semantic and pragmatic knowledge\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: key challenges and promising future directions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: OUTDOOR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: learn language-conditioned skills\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:29<00:00, 29.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The study takes advantage of the latest data (i.e., online housing advertisement data) and point of interests (POIs) to infer fine-grained block-group-level SES in Brooklyn through machine learning techniques\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.54s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: higher-order effects\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.97s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Conclusion LED or BDI may be sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Demand for low\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.88s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ViTs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: FEEDBACK and REFINE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: density declined by 11% and richness increased by 12.2%,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.55s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: biases\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: evidence of hydroperoxy aldehydes and epoxy products from NO3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.58s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: heritagelanguages.org\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Balance Due\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.68s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Aug 14-16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Balance Due\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.87s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: First-Year Student Residence Hall Check-In\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.41s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 01 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.79s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 11 -711, 11- 791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Aug 23-27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Semester & Mini-1 Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: rapid decrease in joint goal accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.50s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Oct 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: October 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Thursday: 3-6 p.m. Friday: 1-4 p.m.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Oct 16-20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.67s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Mini-2 Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.53s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Tu Democracy Day\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.66s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: November 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Nov 13-17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: November 22 -November 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.79s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: December 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.38s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: June 21 F Mini-5 Final Exams June\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.78s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: May 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.67s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Dec 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Jan 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.65s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Balance Due\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.82s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Jan 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March 4 -March 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.87s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.57s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 11 -April 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.19s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a TA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.82s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 5711 Gates & Hillman Centers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Director\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.67s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Justine Cassell\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.52s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Mona Diab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.14s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Information Retrieval: Recommender Systems, Retrieval and Ranking Models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: professor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.15s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Associate Dean of Doctoral Programs/MLT Program Director\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.03s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Anatole Gershman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.52s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 5519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.07s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Natural Language Generation, Privacy and Security, Language Technology Application Areas/Issues, Creativity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.82s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: the Center for Machine Translation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.92s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: mσ2+P\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.75s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Louis-Philippe Morency\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:20<00:00, 20.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 5707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.66s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Eric Nyberg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Mona Diab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:25<00:00, 25.54s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: AL for structured prediction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.03s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Mellon University\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.05s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sumit Srivastava\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Joint prediction and denoising for large- scale multilingual self-supervised learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.20s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sean Welleck\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.32s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Justine Cassell\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Irina M. Bogdanovskaya\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: researchers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Moza Bint Nasser University Professor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Roni Rosenfeld\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.81s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carolyn Rosé\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: B24 Baker-Porter Hall\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.07s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: industry or government\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.70s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 24 -month\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.50s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: September 6, 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: December 13, 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: A GRE subject test in science, engineering, computer science, math, etc. is not required\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two years\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.93s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: three\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.68s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: MIIS-21 students have to take at least two more courses from the selected concentration area to satisfy their degree requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.69s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Computer Science and Machine Learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.89s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.48s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Standard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.29s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Artificial Intelligence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: intrapreneurship and entrepreneurship\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: critical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: an average grade of B (3.0) or better\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.75s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: prerequisite courses\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.49s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: not a required part of the application process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: an average grade of B (3.0) or better\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.72s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: they vary widely from school to school, are often very limited in applicability, and can require onerous documentation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: A short (1-3 minute) video of yourself\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Kate Schaich\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: emergency loans and conference travel grants\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.68s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: capstone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: M.S. in Artificial Intelligence  and Innovation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: MIIS: Advanced Study degree\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:37<00:00, 37.77s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 48731\tSustainable Design Synthesis Prep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: industry or government\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.88s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 24 -month\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.49s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.75s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The MLT program is a 24 -month program  consisting of courses, directed research, and an optional Masters ' Thesis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Students may also choose to complete an optional MLT thesis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.03s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: September 6, 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: December 13, 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.95s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: A GRE subject test in science, engineering, computer science, math, etc. is not required\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.31s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two years\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: three\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: MIIS-21 students have to take at least two more courses from the selected concentration area to satisfy their degree requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Computer Science and Machine Learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Standard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Artificial Intelligence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: intrapreneurship and entrepreneurship\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: critical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.88s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: an average grade of B (3.0) or better\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: prerequisite courses\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: not a required part of the application process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.68s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: an average grade of B (3.0) or better\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.37s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: they vary widely from school to school, are often very limited in applicability, and can require onerous documentation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.37s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: A short (1-3 minute) video of yourself\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.28s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Kate Schaich\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.94s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: emergency loans and conference travel grants\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: capstone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: M.S. in Artificial Intelligence  and Innovation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: MIIS: Advanced Study degree\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:36<00:00, 36.31s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 48731\tSustainable Design Synthesis Prep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:39<00:00, 39.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 48519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:25<00:00, 25.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: VAR\tA\tTBA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:44<00:00, 44.71s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Levin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sections 3. 2.5 – 3.2.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.57s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 48731\tSustainable Design Synthesis Prep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:29<00:00, 29.99s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Ph.D Thesis Defense\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.84s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 12  2 The Language Technologies Institute  ......................................................................................... 12  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sections 3. 2.5 – 3.2.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:22<00:00, 22.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Lec\tMW\t12:30PM\t01:50PM\tDH 2315\tPittsburgh, Pennsylvania\tMusuraca, Eslami  \tUser-Centered Research and Evaluation:\t \t \t \t \t \t \t   \tUser-Centered Research and Evaluation\t \tA\tMW\t12:30PM\t01:50PM\tGHC 4101\tPittsburgh, Pennsylvania\tMusuraca, Eslami  \tUser-Centered Research and Evaluation:\t \t \t \t \t \t \t   \tUser-Centered Research and Evaluation\t \tB\tMW\t12:30PM\t01:50PM\tWEH 8427\tPittsburgh, Pennsylvania\tMusuraca, Eslami  \tUser-Centered Research and Evaluation:\t \t \t \t \t \t \t   \tUser\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:30<00:00, 30.77s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 18999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:36<00:00, 36.15s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: W\tMW\t11:30AM\t12:45PM\tCMB 3069\tDoha, Qatar\tIndaco 73103\tPrinciples of Macroeconomics\t9.0\tLec\tMW\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:29<00:00, 29.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: faculty members of CMU who  will be supervising the study. These individuals are referred to as “independent study  supervisors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:25<00:00, 25.47s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Independent Study\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:38<00:00, 38.82s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: C\tW\t12:00PM\t01:50PM\tANS 106\tPittsburgh, Pennsylvania\tJayan 24491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.84s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 95752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:47<00:00, 47.77s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 12.0\tW\tU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:27<00:00, 27.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: A\tMW\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:35<00:00, 35.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: B\tTBA\t\t\tDNM DNM\tPittsburgh, Pennsylvania\tBain  \t \t \tRW\tTBA\t\t\tDNM DNM\tKigali, Rwanda\tBain  \t \t \tSV\tTBA\t\t\tDNM DNM\tSan Jose, California\tBain 18983\tM.S. Graduate Project - AIE Enablers\t0-24\tA\tTBA\t\t\tDNM DNM\tPittsburgh, Pennsylvania\tBain 18984\tM.S. Graduate Project - AIE Producers\t0-48\tA\tTBA\t\t\tDNM DNM\tPittsburgh, Pennsylvania\tBain 18985\tM.S. Graduate Project - AIE Consumers\tVAR\tA\tTBA\t\t\tDNM DNM\tPittsburgh, Pennsylvania\tBain 18989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 67316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:26<00:00, 26.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.45s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 48731\tSustainable Design Synthesis Prep\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:27<00:00, 27.23s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: A\tM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:35<00:00, 35.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: N\tTBA\t\t\tDNM DNM\tPittsburgh, Pennsylvania\tHellendoorn 17882\tSoftware Architectures\t12.0\tLec\tMW\t09:30AM\t10:50AM\t3SC 265\tPittsburgh, Pennsylvania\tGarlan, Schmerl  \t \t \tA\tF\t09:30AM\t10:50AM\t3SC 265\tPittsburgh, Pennsylvania\tGarlan, Schmerl 17950\tCrafting Software\t12.0\tLec\tMW\t03:30PM\t04:50PM\tWEH 4707\tPittsburgh, Pennsylvania\tSunshine, Hilton  \t \t \tA\tF\t04:00PM\t04:50PM\tWEH 4707\tPittsburgh, Pennsylvania\tSunshine, Hilton 17993\tSocietal Computing Graduate Reading and Research\tVAR\tA\tTBA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:40<00:00, 40.14s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: A3\tF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:37<00:00, 37.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 95752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.42s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Di Lisio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.00s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Salvador Carnegie Mellon University-Wide Studies\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.68s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: C\tTR\t11:00AM\t11:50AM\tMM 119\tPittsburgh, Pennsylvania\tNeely 57171\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.91s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3-7 p.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 7-9 p.m. and 11 p.m.-1 a.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.10s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 4/12/24 – 4/13/24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: VR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.28s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 11 a.m.-11 p.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.23s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: wide diversity of materials from preliminary sketches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.31s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Reunion celebrants have two opportunities to join fellow Tartan alumni\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.58s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 2:00 PM-11:00 PM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Let the Games Begin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.58s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3:00 PM-3:30 PM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.32s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3:30 PM-4:30 PM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.93s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ferris wheel and swings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.41s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 12:00 PM-5:00 PM ET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 12:00 PM-2:00 PM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 8:00 AM-8:15 AM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 11:00 AM-2:30 PM ET Open to entire CMU community Enjoy some of Pittsburgh's local food trucks on campus. Full list of trucks available in March\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.91s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Class of 1999 Reunion celebrants\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.60s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: NROTC Flag Raising\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: an evolution of gaming\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 2008 Spring Carnival\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.93s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a performance under the sea\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.92s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: open to CMU faculty and staff and their guests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.30s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: wide diversity of materials from preliminary sketches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3:00 PM-6:00 PM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.96s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Class of 1974, Half Century Tartans and their guests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.20s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3:00 PM-5:00 PM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.19s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: alumni and families of the The Tartan community and their guests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnival\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \"to die rich is to die disgraced\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.31s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: conservatory degree programs in music and drama\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.50s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: School of Urban\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.64s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: revolutionary, initiating a university culture where information technology pervaded virtually all areas of study\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.31s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: School of Computer Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The Graduate School of Industrial Administration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: $1 million\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.13s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: more than a dozen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.44s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: My heart is in the work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Institute of Technology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.78s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: steel producing company\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 50th anniversary of the Carnegie Tech-Mellon Institute merger\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: conservatory degree programs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.02s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Qatar and Silicon Valley, Calif.,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.29s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Plan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Mellon University\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.30s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: beaux arts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.73s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: bachelor's degrees\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Twenty Plus Two\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.93s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: recruiting leading scientists, offering sponsored fellowships with government and industry leaders\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 10.5 Employment Eligibility Verification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.13s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Student-Athlete Handbook\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.98s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Margaret Morrison Carnegie College\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.89s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: outlining the principles of a sound professional education\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.03s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Mellon College of Science and the College of Humanities and Social Sciences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Portugal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: practical\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Scotland\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.87s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: quantitative analysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.15s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.94s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: China\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.47s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: commonsense and logical coherence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Tech\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: technology and humanity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.97s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Pittsburgh, Pa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: the arts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.57s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: nontraditional interdisciplinary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.99s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sunday, May 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.29s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: tickets are not needed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 10 a.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.86s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1.5 hours\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.99s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 9:15 a.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.05s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sunday, May 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: presentation of diplomas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.98s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: on and off campus\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: soon\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1.5 hours\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.43s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Thursday, May 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.67s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3:30–4:30 p.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.12s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Location TBDRegistration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.02s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sunday, May 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: –Sunday, May 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 4–5:30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.05s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Undergraduate students and their guests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Cohon University Center\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:31<00:00, 31.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Auditorium, Soldiers & Sailors Memorial Hall & Museum\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.86s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 4–6 p.m.Tepper Building Atrium\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.31s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 8 a.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.43s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 9–10 a.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 9:15 a.m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Access to guest seating will be restricted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.37s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Pittsburgh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.19s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Parmigiani Fleurier , Manor , Heineken , Vaudoise and UBS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.12s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: president, keynote speaker, student speaker and academic deans\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.37s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: M. Shernell Smith\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.72s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: M. Shernell Smith\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.30s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1956 and 1957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.60s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Logic Theorist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: harder or easier problems\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Hearsay I\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Scott Fahlman\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.70s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.32s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Robotic Reconnaissance Vehicle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.54s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: human-computer interaction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.92s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Machine translation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Mach kernel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.49s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Hitech\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Java\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.00s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: OpenAI\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Lycos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: model checking\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.29s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Completely Automated Public Turing Test to tell Computers and Humans Apart,”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.16s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Eyevision\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.76s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: BOSS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: reading people’s thoughts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: An algorithm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.38s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: EteRNA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.96s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Duolingo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Watson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.74s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Shafi Goldwasser\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Smart, adaptable traffic signals\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.60s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Jordan and Reddy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.68s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Andrew Project\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Running Scared\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.42s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.55s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: ↑0.3860.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.14s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: six\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Fringe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: body, pushbar, wheels and driving and braking mechanisms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Ital Inf Retr Workshop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.58s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Each team keeps its actual processes a secret\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 12:00 PM-2:00 PM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.58s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: nerves of steel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: nerves of steel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.12s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: to teach everyone how to do everything\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: rain barrels with bicycle wheels to three-wheeled ash cans\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: help reduce drag, make the vehicle quieter and just looks cool\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.36s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: six people\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.03s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: the way they brake, steer and what types of wheels are used\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Each team keeps its actual processes a secret\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: culture -specific while others are shared between cultures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.96s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 10, 2019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.52s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Team member\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: nerves of steel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.23s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: nights and weekends\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: to propel that buggy forward\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.42s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Each team keeps its actual processes a secret\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Person who propels a buggy via a pushbar along one of the five hills of the buggy course\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.49s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: flaggers man the course\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Sweepstakes and its traditions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Mellon University\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 2128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: six\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.98s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.32s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Institute of Technology\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.53s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.88s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The Carnegie\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.27s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Andrew Carnegie's Scottish heritage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.52s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.92s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a fierce warrior from either the Asian tundra or Scottish highlands\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.53s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Andrew Carnegie's Scottish heritage. A tartan is often misrepresented as a fierce warrior\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 2006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.94s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 78 percent\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.30s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Susan Bassett\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.43s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: SME Branding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.08s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 2006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.99s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Director\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.61s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.48s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: traditional marches and oldies to current pop tunes and jazz standards\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: any member of the campus community with music experience\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Band without Pants\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.08s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3,500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.30s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: FieldTurf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Dr. Farnam Jahanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.82s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Cardinal and Gray\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Mondays and Thursdays\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.07s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 7,062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.23s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: excite the fans at athletic events\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.37s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: twilled woolen\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Pittsburgh, PA 15213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.30s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 2006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.82s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.86s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: SME Branding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.48s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Carnegie Mellon's Scottish heritage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.83s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: seven\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.33s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: musicians\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.41s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Scottie Dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.53s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Andrew Carnegie\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.35s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Scottie Dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.20s/ Batches]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: students and alumni\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Path to the question file, adjust the path according to your setup\n",
        "sample_file_path = '/content/drive/My Drive/Colab Notebooks/NLP2/sample_questions.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojjN5oVlNInu",
        "outputId": "574d9eb6-7f94-4d59-9355-1f8a8ee88b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "answers_file_path = '/content/drive/My Drive/Colab Notebooks/NLP2/roberta_sample_model_answer.txt'\n",
        "\n",
        "# Open and read the questions file\n",
        "with open(sample_file_path, 'r') as file:\n",
        "    questions = file.readlines()\n",
        "\n",
        "# Process each question\n",
        "with open(answers_file_path, 'w') as answers_file:\n",
        "    # Process each question\n",
        "    for question in questions:\n",
        "        question = question.strip()  # Remove any leading/trailing whitespace\n",
        "        if question:  # Check if the question is not empty\n",
        "            prediction = querying_pipeline.run(\n",
        "                query=question,\n",
        "                params={\"Retriever\": {\"top_k\": 5}, \"Reader\": {\"top_k\": 1}}\n",
        "            )\n",
        "\n",
        "            # Check if there are any answers returned\n",
        "            if prediction['answers']:\n",
        "                # Directly access the 'answer' attribute of the first Answer object\n",
        "                answer_text = prediction['answers'][0].answer\n",
        "                # con_text = prediction['answers'][0].context\n",
        "                answer_text = answer_text.replace('\\n', ' ')\n",
        "                print(f\"Answer: {answer_text}\")\n",
        "                # Write the answer to the answers file, one answer per line\n",
        "                answers_file.write(f\"{answer_text}\\n\")\n",
        "            else:\n",
        "                # If no answer was found, write a placeholder or a notification\n",
        "                answers_file.write(\" \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQqEIgg6NcEt",
        "outputId": "fb671f68-c376-40be-c336-01d02c249559"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.51s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Buggy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.45s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: four semesters\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 144\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.26s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Temporal Acoustic Parameter Loss\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.42s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: to evaluate this chal- lenge task\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.70s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: August 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.80s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: nine\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.84s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.51s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3.0\t \t \t \t \t \t   \tMaster of Architecture\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.30s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Junwei Huang\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: International Conference on Advanced Cloud and Big Data\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.14s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: International Society for Music Information Retrieval\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  8.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Scott Fahlman\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:26<00:00, 26.05s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: pi k\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.33s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: less\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: PI\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.83s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.74s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 6.7B\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.46s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: father\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.82s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3.3.27\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.64s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Unlimiformer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2.75\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: McConomy Auditorium, Cohon University Center\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.55s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: September 18\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.79s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 28.2%30.7%17.6%17.8%16.4%30.1%29.6%27.9%19.2%16.3%15.4%16.0%31.9%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.50s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 11 -929 – Masters ’ Thesis II\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.62s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Summer Fall 2 Spring 2  Search Engines Algorithms for NLP Intro to ML (MLD) MIIS Directed Study   Question Answering Intro to Deep Learning\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.64s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a large-scale dataset of 218k values, rights, and duties\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.87s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Nearly triple the   national average\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.58s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Pairwise Similarity Learning is SimPLE\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: off-the-shelf language models\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.68s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 35\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.52s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Nicholas Carlini\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: $80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.49s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 84\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.36s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1914\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: course assignments, directed study projects, and/or the capstone project\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fall   Spring   Summer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.39s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Latin\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Computational Biology\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: L1-L6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conference on Robot Learning\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2008\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.84s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: deep learning frameworks\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.52s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: October 27\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2021\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.06s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: does not discriminate\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.83s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: outperform the results of a strong LLM, gpt-3.5-turbo , by an average of 20%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.64s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fall   Spring   Summer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Stephanie Kwolek\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.93s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Monday\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.54s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Luís Borges\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.03s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Amber Vivis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.76s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Paul Pu Liang Yun Cheng\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:05<00:00,  5.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: International Conference on Machine Learning\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: SYNTHIA $\\\\rightarrow$ Cityscapes and GTA5 $\\\\rightarrow$ Cityscapes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.39s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Scientific Reports\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.52s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.61s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: All users must present a valid  CMU ID\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Oct 16-20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.46s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 12 -unit course with a course number indicating a unit of the School of  Computer Science (including LTI); a 6 -unit\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 96\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.85s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.38s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: June 24\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Luís Borges\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 16\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.73s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: International Conference on the Theory of Information Retrieval\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.53s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: multilingual translation of ACL 2022 technical presentations into 10 target languages\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.74s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Eval\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.55s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: August 28\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.51s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.05s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 13.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 412-268-7125\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 0.3% and 1.4%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.42s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Mellon Institute of Industrial Research\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.66s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 462\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.47s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 412- 268-6591\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.80s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: separation performance\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.60s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: September 18\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.84s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Unless otherwise specified\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:29<00:00, 29.46s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Carnegie Mellon University\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.09s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Eric Xing\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: two\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.36s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Balance Due\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 812\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.78s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Summer Semester 2024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: LM's internal sequence-level value estimate\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 15 -April 19\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.47s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 14.41%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.59s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: There is no limit\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.43s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.02s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 312\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.15s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ML-SUPERB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.57s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: YourTTS\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.45s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 6 units\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.15s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3.0\t \t \t \t \t \t   \tMaster of Architecture\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: research advisor\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.56s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: translates a task description into a list of high-level sub-tasks\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.62s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: framework tax\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.47s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester (S24\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Young Min Kim∗andKalvin Chang∗andChenxuan Cui andDavid Mortensen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.01s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conf Mach Transl\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.24s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 4] and TREC DL20 [ 5] judged sets of queries\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.87s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 8 a.m.-Noon\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: http://arxiv.org\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.18s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1.PET\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.79s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 11-12 Th-F\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 6 units\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: kNN distances\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: it is  possible to change advisors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.38s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: “Detecting gender differences in perception of emo- tion in crowdsourced data\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.07s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Michael I. Shamos\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.76s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: perform better than standard LMs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 0.317 0.288\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.20s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: February 26\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.07s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: MiniLM-L6-30M\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.71s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Brianna Eriksen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: disparity\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: February 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.64s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Facutual Augmentation\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.31s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.63s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 15%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Alf- World\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Office Manager\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: White and educated young people from English-speaking countries\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.50s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3.3.27\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.74s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: lower word error rates\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.42s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Liu and Liu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: visit our YouTube channel\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Seattle\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Lab on a Chip\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 21\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.91s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: September 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.52s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: successful Teaching Assistantships\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.46s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 11-797\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 5 top- ranked features. This confirms the findings men- tioned in related work for the characterization of the different hedge classes ( justwith Propositional Hedges ,sorry with Apologizer ,Iwith Subjectiviz- ers). The presence of Ohalso has high importance for the characterization of Apologizer (n=2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.31s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: National Conf Artif Intell\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:31<00:00, 31.58s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: A\tF\t10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.15s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3 points\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.50s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 77%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.38s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: vocabulary expansion of surface form and semantic representation of term meaning\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.59s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: FREDOM\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: isca-speech.org\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 144\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.52s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: $100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: TAPLoss\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.88s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Schenley Park's Flagstaff Hill\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.54s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: David Gray Widder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.80s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: consistent summaries\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.88s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: document text information, retrieval features, and global document information\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.69s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 144\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.73s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 10pm-2am\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.20s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fall   Spring   Summer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Imprecise label learning\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.11s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: LRLs and African languages\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: watermarks\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.29s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 15-213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ICLR 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fall   Spring   Summer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2:30 PM-6:30 PM ET Open to entire CMU community Join us for a delightful celebration of Holi at CMU! Embrace the start of spring as we celebrate the festival of colors during Carnival Weekend\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.42s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Carnegie Mellon University, USA\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Search Search CMU\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.53s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fall 2 Spring 2  Natural Language Processing Algorithms for NLP Intro to ML (MLD) MIIS Directed Study   Question Answering Intro to Deep Learning\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.06s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Kumar\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.39s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fall   Spring   Summer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Alan Perlis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Vol. 7, No. CSCW2, Article 316. Publication date: October 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.25s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Santa models\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.57s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2021\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.07s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: $80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.87s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.52s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 61st Annual Meeting of the Association for Computational Linguistics\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.56s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Carnegie Tech\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.62s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: EmpathicStories\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 812\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Buggy Showcase\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.77s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: finer grained phonetic distinctions\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.79s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 11-636\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Jiatong Shi\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.71s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Andrew Project\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Summer 2024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.24s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Amanda Bertsch\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.36s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: GREs are now optional\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: $80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Sweepstakes Finals\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.68s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1958\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.22s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3 x 45\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: February 17\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.20s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: September 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.85s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Shi Yu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.69s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: vanilla\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.22s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Ankit Shah\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.71s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.59s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Translatotron models, and direct discrete unit models\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: August 22\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3.1.2 Early Completion\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:16<00:00, 16.73s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: M. Shernell Smith\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.60s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 15-213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:23<00:00, 23.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Pursuing a PhD could allow you to become highly specialized in your field and develop cutting-edge research skills, which could be very valuable in industry. On the other hand, gaining hands-on experience through internships or entry-level positions can also be incredibly beneficial, especially if you’re looking to start your career in a particular area. Ultimately, the decision is up to you - consider all of your options carefully before making a choice. 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.16s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ACM Conference\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: July 29\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 29\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Monday after break\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Margaret Morrison\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.76s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: query rewriting does not enhance per- formance compared to the original queries\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.31s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2128\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.02s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2023-2024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:35<00:00, 35.64s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3.0\tA\tW\t07:00PM\t09:50PM\tHH B103\tPittsburgh, Pennsylvania\tShah, Hayes 98230\tStudent Taught Courses (StuCo): Avatar: The Last Airbender & The Legend of Korra\t3.0\tA\tW\t07:00PM\t07:50PM\tPH A18A\tPittsburgh, Pennsylvania\tAceti, Crawford, Rodas Leal 98242\tStudent Taught Courses (StuCo): Intro to Esoteric Programming Languages\t3.0\tA\tM\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.81s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: without additional training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.01s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 268-4525\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.63s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: B\tTBA\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: MLT program\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.51s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: speech quality\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.87s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Weigand Gymnasium in the Cohon University Center\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 84%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: catastrophic forgetting and background shift challenges\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.18s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Outdoor Underspecified TaskDescriptions OfObjects and Regions\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.05s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: inference\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 8:30 PM-10:00 PM\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.42s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Block- Wise\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: SPAE\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.71s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Amber Vivis\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:23<00:00, 23.26s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Master of Science\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.24s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Sweepstakes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.77s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ride\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.71s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Ximing Lu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.55s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: preferential treatment\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Akhila Yerukola\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.42s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: November 7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.55s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: August 28\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: September 6, 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2.75x\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Computational Data Science Seminar\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:23<00:00, 23.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: responsible development of large language models for code\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.52s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Association for Compu- tational Linguistics\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  8.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: PaintSeg\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The LTI does not allow direct transfers from its master’s programs into its Ph.D.  program\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.03s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: F1%)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.60s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: COBRACORPUS\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The  HUB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:22<00:00, 22.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Summer Fall 2 Spring 2  Search Engines Algorithms for NLP Intro to ML (MLD) MIIS Directed Study   Question Answering Intro to Deep Learning\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.14s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: higher\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 10–11:30 a.m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.70s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Chute Flagger\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: there is no formal mechanism\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.73s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: better understanding our past\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.29s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.15s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Balance Due\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 11.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 22\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.55s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: linking mention references in text corpora to events from a knowledge base\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: July 4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Jeremy Olisar\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.06s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Carnegie Mellon University\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.84s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 412- 268-6591\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.39s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 11-727\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.58s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Shijian Lu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: My heart is in the work\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Block- Wise\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: novel features\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.35s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 78.24%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: default letter grade\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 412- 268-6298\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.38s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The Little\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.66s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Two drop deadline\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Summer Semester 2024 (M24)**   2023-2024\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1999\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.38s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Carnegie Mellon Univers ity {jiefuo, vpratapa, rishubhg, teruko }@andrew.cmu.edu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.47s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: CMU, the University of Pittsburgh and Westinghouse Electric Corp\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.31s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:24<00:00, 24.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 90701\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.62s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: GHC 5404 and GHC 66 04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.14s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 42\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Brown vs BoE\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.92s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 8.1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: StarCoderBase and StarCoder\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Forward-Looking Active REtrieval augmented generation\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: performing style transformation\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Simmons Auditorium, Tepper Building\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: oh no...Figure 1: Hedging in peer tutoring\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.05s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.45s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Pushbar\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: $80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.29s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 462\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.43s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.60s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 966\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.36s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: longer user-item interactions\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.25s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Yahoo!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.93s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a simulation component\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.19s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.50s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March 15 F Mini-4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.93s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Lab Chip\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: October 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.42s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Portugal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.43s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 23\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: July 29\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.76s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: LLMs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Susan Bass\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:18<00:00, 18.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 48 GB A6000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.91s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 94%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 10, 2019\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: June 28\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.38s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: May 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.26s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: No Classes & University Closed\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.32s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 6 units\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.30s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2am\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:23<00:00, 23.19s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 480\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.68s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2019 ›              April ›              Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.26s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Yonatan Bisk\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.79s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.17s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.77s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 46th International\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.46s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: AudioSet\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.08s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 412-268-5704\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: research advisor\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.46s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: June 28\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Stacey Young\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Zhenghao Liu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.16s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 Top-20 Top-100 BM25 62\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: environmental impact, equity, and impact on peer reviewing\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Sergis et al., 2019; Venant et al.,\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Running Scared\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.19s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.04s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 11 -April 13 Th-Sa Spring Carnival\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.30s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: doi.org/10.3390/e25071039 Academic Editors: Kevin H. Knuth and Deniz Gença˘ ga Received: 17 April 2023 Revised: 3 July 2023 Accepted: 7 July 2023 Published: 10 July 2023 Copyright: © 2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). entropy Article Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation Wayne Zhao\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.03s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Mao Yisheng\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.29s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: lower word error rates\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Xuhui Zhou♡Hao Zhu♡Akhila Yerukola♡Thomas Davidson\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.33s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: social acceptability and hate speech detection\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: you can’t, i don’t know what that means. you don’t have any units bordering you don’t have units capable of doing that you physically can’t what do you mean? none of my units can do that\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: retrieval-based approach\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.74s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fusion-in-T5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Hao Peng\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: drop deadline\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 4.3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: $80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.62s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: time-domain and time-frequency domain\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.66s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.74s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Pittsburgh\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Shi Yu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.93s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: December 13, 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fall 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.31s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Yu Gu Northeastern University Shenyang, China guyu@mail.neu.edu.cnGe Yu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 10, 2019\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.49s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Michael I. Shamos\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Carnegie Mellon's School of Computer Science\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00, 10.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: WikiData\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.30s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Stacey Young\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.16s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: multimodal prediction\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Mar 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.22s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: David R. Mortensen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 144\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.61s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Advantage-Leftover Lunch RL\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.14s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 966\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: cprose@cs.cmu.edu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.09s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: GPT-3 text-davinci-003\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: $80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.68s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: June 28\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.17s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Jiefu Ou1Benno Krojer2Daniel Fried1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.05s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: GPT-4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.63s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: October 27\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: online engineering\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 15 -April 19\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.53s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Loughran\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.75s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The design of the phone\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.43s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1993\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.20s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Short -Term Accommodation for Gestational Parents  – A short term absence from  academic responsibilities up to a maximum of six (6) weeks.   Short- Term Accommodation   LTI Ph.D.  Graduate Student Handbook  Page 31   may be extended by two (2) weeks, for a tot al of eight (8) weeks, where a longer absence  is medically necessary. Prior to the absence students must work with relevant university  faculty and staff to adjust their course work, research, teaching and other academic  responsibilities during the period of absence. This may include extensions of time to  complete assignments, incomplete grades, and/or dropping courses, shifting research  responsibilities and adjusting TA assignments. Students who take a Short -Term  Accommodation will remain enrolled.   • Formal Leave of Absence\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Zhiqing Sun\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.45s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: natural language input\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00, 10.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: November 2006\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: https://arxiv.org\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Must have valid CMU ID to secure tickets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Pittsburgh\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.93s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 50th year\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.20s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 268-4525  Fax: (412) 268-7287  Robert Frederking   Graduate Program Chair  Language Technologies Institute  School of Computer Science   Carnegie Mellon University  Gates-Hillman Center 6515  5000 Forbes Avenue, Pgh, PA 15213  Phone: (412) 268 -6656\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.49s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1965\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.77s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.08s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: lack clear spatial delineations\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Sergey Brin and Lawrence Page\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Marriage Committee\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.69s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: SIGMORPHON\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.50s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Reddy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.01s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: concentration courses\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: https://webarena.dev/\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.54s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.76s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: August 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.93s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: unseen environments\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:24<00:00, 24.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 06600\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: September 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.02s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: University of Washington‡Carnegie Mellon University\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.87s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: QA\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 144\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.01s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Tepper Building Atrium\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.50s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 110\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.47s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Systems, Analytics, and Human-Centered Data Science\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.91s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Meet Assoc Comput Linguistics\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Office Manager\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.20s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: log in with your CMU ID card\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.43s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: gsa@cmu.edu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.11s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Prompt2Model\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.06s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: anchors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: SequenTial rEcommenda-tion model\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.60s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.42s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3.0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: honorary degree\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.14s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: July 8 M Mini-6 drop deadline\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.15s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: encodes the text as a layer of interpretable categories\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.33s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1Language Technologies Institute, Carnegie Mellon University2Meta AI\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 15-213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: October 31\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.05s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.79s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Aman Madaan\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.88s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 11-711\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2.75x\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.85s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Master of Architecture\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.05s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 9:15 a.m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.90s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1967\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.90s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: e-commerce, social forum discussions\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.18s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: SPAE PaLM and SPAE GPT\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.87s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: when used in isolation\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.17s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.09s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: increase in accuracy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.07s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: did!!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.29s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: questions when they lack knowledge\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.04s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: text\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.80s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Limassol, Cyprus Cao\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.81s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: through constructing and matching Contextualized Surface Forms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.07s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: condition image\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.09s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.46s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1,128\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.20s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Robert Lo†Abishek Sridhar†Xianyi Cheng Tianyue Ou Yonatan Bisk Daniel Fried Uri Alon Graham Neubig Carnegie Mellon University {shuyanzh, fangzhex, gneubig}@cs.cmu.edu ABSTRACT With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic andreproducible\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.15s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: https://github.com/JaredFern/Framework-Tax.\",\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.58s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: lexicographic pre- cision or lexiprecision\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.22s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: CoRR\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.03s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.30s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 5 top- ranked features. This confirms the findings men- tioned in related work for the characterization of the different hedge classes ( justwith Propositional Hedges ,sorry with Apologizer ,Iwith Subjectiviz- ers). The presence of Ohalso has high importance for the characterization of Apologizer (n=2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.66s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Maria Antoniak♣Anjalie Field\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.10s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: language-agnostic and language-aware\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Joan Axelson\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.77s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Balance Due\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.66s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Synthesizing speech with accents\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.09s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ride\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.77s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: publicationVenue\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.77s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: October 23\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: StyleRF\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.24s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Harnoor Dhingra Preetiha Jayashanker Sayali Moghe Emma Strubell\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Duolingo\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Pursuing a PhD could allow you to become highly specialized in your field and develop cutting-edge research skills, which could be very valuable in industry. On the other hand, gaining hands-on experience through internships or entry-level positions can also be incredibly beneficial, especially if you’re looking to start your career in a particular area. Ultimately, the decision is up to you - consider all of your options carefully before making a choice. 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.49s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: cognate detection\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.39s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Mona Diab\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.54s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: No Classes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.87s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Master of Language Technologies\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 15-213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.07s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: inpainting and outpainting\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: What Is Your Course\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Julie Nys\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.53s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 11 -April 13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.66s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Amir Hussein\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.53s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The Little Mermaid\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.69s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: compression, transmission, and platform-specific processing\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:23<00:00, 23.90s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: M\tMW\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.02s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Python\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ZZ\tTBA\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.16s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: lower word error rates\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.59s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 12:00 PM-2:00 PM\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 11 -928 – Masters ’ Thesis I (with the chosen thesis advisor, typically for 12 units,  typically in the Fall of their second year) and 11 -929 – Masters ’ Thesis II in the following  Spring\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.07s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Paul Pu Liang\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Xiang Li1, Yandong Wen2, Muqiao Yang1, Jinglu Wang3, Rita Singh1, Bhiksha Raj1,4 1Carnegie Mellon University\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.66s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: B\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.81s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Patrick Fernandes∗,2,3,4Daniel Deutsch1Mara Finkelstein1Parker Riley1 André F. T. Martins3,4,5Graham Neubig2,6 Ankush Garg1Jonathan H. Clark1Markus Freitag1Orhan Firat1 1Google2Carnegie Mellon University3Instituto Superior Técnico\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Balance Due\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.08s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 14.41%,\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.69s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: November 7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Oct 16-20 M-F Fall Break; No\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.51s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 412-268-7281\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.15s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Association for Computational Linguistics\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.85s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Shaochun Hao\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester (S24\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.83s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: transphobic\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.99s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Robust and Multilingual\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.26s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Habermann\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.58s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 23-04-1945\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:24<00:00, 24.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: StuCo): Movies You Should Have Watched By Now\t3.0\tA\tW\t07:00PM\t09:50PM\tHH B103\tPittsburgh, Pennsylvania\tShah, Hayes 98230\tStudent Taught Courses (StuCo): Avatar: The Last Airbender & The Legend of Korra\t3.0\tA\tW\t07:00PM\t07:50PM\tPH A18A\tPittsburgh, Pennsylvania\tAceti, Crawford, Rodas Leal 98242\tStudent Taught Courses (StuCo): Intro to Esoteric Programming Languages\t3.0\tA\tM\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 15 -April 19\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.71s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: coarse masks, boxes, scribbles, and points\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.26s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fried\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.77s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Samiran Gode,1Supreeth Bare,1Bhiksha Raj,1Hyungon Yoo,1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.16s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Completely Automated Public Turing Test to tell Computers and Humans Apart\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.08s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.11s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.80s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.82s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Audio-Visual Representation Models\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.07s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 10-605\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.03s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Computers\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.18s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Task -Orientation Focus Course\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ORCA\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:28<00:00, 28.36s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: E\tTR\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.90s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conf Empir Method Nat Lang Process\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.50s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.17s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: GREs are now optional\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.18s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: October 13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Lingjing Kong*\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.16s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ADLES-VFT algorithm\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: multilingual\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.69s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: impact indexes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.45s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: A. Nico Habermann and then-CMU provost Angel Jordan\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 11.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 5402\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Scottish terrier\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.22s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 20%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.90s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: September 18\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: frequency-related parameters, energy or amplitude-related pa- rameters, spectral balance parameters, and temporal features\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The world’s first university  robotics department ,   founded in 1979\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.92s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: improves top precision\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Anubha Kabra1∗\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: October 13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.26s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: International Conference on Acoustics, Speech, and Signal Processing\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.97s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 412) 268-7287\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: without additional training\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Methodologicak justication psychology as areas of thought\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.17s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Dean, School of Computer Science\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.88s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: few-shot in- context learning\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: https://github.com/OpenMatch/OpenMatch.\",\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.85s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: bridges the two models\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Driver\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: MS MARCO and TREC DL\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.55s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: any member of the campus community with music experience\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: dean\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.29s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: improves the retrieval of answer passages for the current question\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.81s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Junhong Shen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: May 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: speech quality\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Portugal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.99s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 31\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: quality metrics\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.82s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fringe\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 10, 2019\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.83s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: University Closed & No Classes\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: expanded the document-grounded dialogue task to encompass multiple languages\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.08s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Agarwal\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: word error rate\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.79s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Stacey Young\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Transition\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: CUC Studio Theater\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.84s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: fully retain their language capabilities\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.41s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 16 task families\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.04s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 2007\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a small model with a k-sparse projector\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.29s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: David Garlan\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.25s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Dec 8 F Semester & Mini-2 Last Day of Classes  --------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.62s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: do you agree?) (b) What do you think the research community prioritizes now? (c) What is the scope of the work that your research group does? (d) How do computing resources affect your group’s work now? (e) How does the software workflow look like for you or your students now? (f) What tools, frameworks, libraries do you or your students use? (g)Have these tools, frameworks, and libraries made an impact on your (or your students’) research? (h)What impacts do you think these tools, frameworks, and libraries have made on your commu- nity’s research? (i)Where does the funding for your work come from? (Prompt: what are the major costs involved in your research?) (j)When you or your students start a new project, how long on average do you expect it to take, from the start of work to a paper submission? (k) Has this changed over your career\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:27<00:00, 27.36s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Deng et al., 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: May 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.85s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ACL 60/60\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.89s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Ting-Han Fan\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.02s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: at least one author\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.22s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: whenever things moved one another\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.54s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 3.3.5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.75s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: proficiencies in Writing, Presentation, Programming, and Teaching\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.92s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a revamp of the open-source ESPnet-ST toolkit\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.13s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: five\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:22<00:00, 22.29s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: B\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: outperforms existing interpretable language representations\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Punjabi, Por- tuguese, and Wu Chinese\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.12s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 17th Conference of the European Chapter of the Association for Computational Linguistics System Demonstrations\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.55s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 5:30-6:30 p.m. Location is the CUC Studio Theater. Q: When are performances? A: The Kiltie Band performs at all home football games, a holiday concert, and two spring concerts. Times and dates for all performances are announced at the first practice\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.24s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: teenagers, and not trained tutors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.57s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: research advisor\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 15 -April 19\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.25s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: associate dean\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Environment interfaces, controllers, detection, and segmentation modules\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.24s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Watson\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.02s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: agentivity\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.28s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 15-213\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.04s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 10-601 Introduction to Machine  Learning; 05-839 Interactive Data Science; 15-6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 5 top- ranked features. This confirms the findings men- tioned in related work for the characterization of the different hedge classes ( justwith Propositional Hedges ,sorry with Apologizer ,Iwith Subjectiviz- ers). The presence of Ohalso has high importance for the characterization of Apologizer (n=2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.53s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: $100 per program and $80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.51s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Fall   Spring   Summer\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.08s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: WavLabLM\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.74s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1138–1143\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.86s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: senior\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.78s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.03s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.03s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: This event is open to the entire CMU community and their guests\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 10, 2019\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.18s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Xuhui Zhou\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.69s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: retrieval effectiveness and efficiency\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.26s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Paris, France\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.09s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: GPT-4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.90s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Cambridge University Press\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.81s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 20%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.36s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Shell\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.75s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 100x\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: significant improvements\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.70s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1 trillion tokens sourced from The Stack\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Omar Khan\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.92s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: March 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 38\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.53s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: November 18 -November 22\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 1Language Technologies Institute, Carnegie Mellon University2Meta AI\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.99s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: human feed- back\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.52s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Mona Diab\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 10, 2019\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.33s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 21\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.79s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 28 July 2023\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.37s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: GlobalBench\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.06s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The Graduate School of Industrial Administration\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.47s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.49s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Pusher\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.31s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.27s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: ASR, TTS, MT\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.72s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: temporal acoustic parameters\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.34s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Extreme bad/good actions\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.17s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.81s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: April 3 -April 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.94s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.19s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: cross-encoder teacher\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.98s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.75s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: privacy\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.23s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.99s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Semester\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:15<00:00, 15.08s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 48234\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.78s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Carnegie Tech-Mellon Institute\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: D1\tTR\t02:00PM\t03:20PM\tCUC AR 255\tPittsburgh, Pennsylvania\tStragar, Gauntner  \tIntroduction to Yoga:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:35<00:00, 35.40s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Embedded System Development\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.67s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 143\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:17<00:00, 17.91s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Patel\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: latency, throughput, memory overhead, and energy consumption\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.10s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: green_basket\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.30s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Thai-Binh Nguyen\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00, 10.00s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: pliang279/MultiViz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.44s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Pursuing a PhD\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.68s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Paul Pu Liang Yun Cheng\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.50s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 4 pm\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.92s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: block-wise modeling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.18s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: abil- ity to infer the mental states of other agents in social environments\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.21s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: six\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.11s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 63.99 49\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.49s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: pound sterling\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.17s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: SPAE PaLM and SPAE GPT\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: there is no formal mechanism\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.85s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.81s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: 9\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.95s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Daphne Ippolito\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.06s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: a language\\u2019s resource level\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.96s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: voice-anthropometric measurement (AM)-face paradigm\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.04s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: MSMARCOv1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.48s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.55s/ Batches]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Mar 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:14<00:00, 14.75s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: UIST ’23, October 29–November 01, 2023, San Francisco, CA, USA  © 2023 Copyright held by the owner/author(s).  ACM ISBN 979-8-4007-0132-0/23/10.  https://doi .org/10 .1145/3586183 .3606724 in collegiate classrooms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.11s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: expectation- maximization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 26 F Semester & Mini-4 voucher deadline (4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.19s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Office of the Dean of Student Affairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.81s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: November 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The BigCode project\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.65s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: DialDoc 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.42s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: real-world speech from YouTube and podcasts\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.84s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: January 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:19<00:00, 19.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: StuCo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.24s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Shell\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.23s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 4 pm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: mismatch between lexical surface form and implicit term semantics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 3.3.27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.97s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: image, text, and imbalanced classification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.07s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.85s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: m.McConomy Auditorium, Cohon University Center\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Summarization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.91s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Fringe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.44s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The LTI does not allow direct transfers from its master’s programs into its Ph.D.  program\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.13s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: minus\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.59s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: named entity recognition, de- pendency parsing, and event argument extrac- tion\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.94s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: eye gaze\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.14s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Kiltie Band website\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.88s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Liao Qu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.22s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: fall and spring\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.09s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.32s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: brittle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.72s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: multimodal cluster prediction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.42s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Fall   Spring   Summer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.15s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.80s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Spring 2024 Semester (S24)Fall 2023 Semester (F23)   2023-2024 OFFICIAL Academic Calendar Carnegie Mellon University Revised: 28 July 2023 Page 2 of 3March 1 F Mini-3 Last Day of Classes  March 1 F Mini-3 voucher deadline (4) March 2 Sa Mini-3 Exams March 2 Sa Mini-3 Faculty Course Evaluations close March 4 -March 8 M-F Spring Break; No Classes March 11 M First day of Mini-4 Classes March 11 M Mid-Semester & Mini-3 grades due by 4 pm March 12 T Summer 2024 Registration Opens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.06s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: DALL-E 2, Imagen, and Stable Diffusion\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.82s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:13<00:00, 13.00s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: September 6, 2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: K-spArse Projector for Lexical Expansion\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.96s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: December\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: it is  possible to change advisors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.48s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.12s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 200 150 7,892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.87s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: audio-visual speech recognition (A VSR), code-switched speech recognition (CS-ASR), and speech translation (ST)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.15s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.64s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: the effectiveness for users interested in exactly one relevant item\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.64s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 6.7B\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Two drop deadline\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:06<00:00,  6.51s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: N24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:33<00:00, 33.17s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 12271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.95s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: November 18 -November 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:12<00:00, 12.21s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: five\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.18s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Chute\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: School of Urban and Public Affairs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.62s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Language Technologies Institute\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.95s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: April 10, 2019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.05s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March 4 -March 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.96s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LSNV [19] and Hyper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.49s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: framing all audio tasks as text-generation tasks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.77s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: pathology, mood prediction, and robotic perception\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.88s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 0.019%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.56s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Conversational Interfaces\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.03s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: March 4 -March 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.47s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Fall   Spring   Summer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a research career in academia or industry\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.01s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: two\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.95s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.90s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:10<00:00, 10.46s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: cross-attention\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a twilled woolen fabric with a plaid design\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:11<00:00, 11.04s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Token-and- Duration Transducer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.26s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: performance gains\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  8.00s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Monday\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.99s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: How2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.43s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: generalization, (2) time and space complexity, and (3) modality robustness\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:08<00:00,  8.63s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Joan Axelson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:07<00:00,  7.03s/ Batches]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Philadelphia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:09<00:00,  9.76s/ Batches]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "85ea2c107d7945555de8e73270cf8a4d668bafec7aac344fa62e3415dc7bf5ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48c2f0fa5e304757993b0c9f3cc89dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b18952c2ebc74e279d75016a54e32094",
              "IPY_MODEL_b31be110f01a4184b01ba9291a34ad83",
              "IPY_MODEL_8378aa12de4348969f6cc56554d13132"
            ],
            "layout": "IPY_MODEL_f58042af6c0b4d189c458f9859c1a42e"
          }
        },
        "b18952c2ebc74e279d75016a54e32094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c164764eef3643448b21bf9fdb44a464",
            "placeholder": "​",
            "style": "IPY_MODEL_9f562f7e12b5487fb30c5b4a1acaf9b1",
            "value": "config.json: 100%"
          }
        },
        "b31be110f01a4184b01ba9291a34ad83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58cd7d979ab04c49b9a48159d188050d",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_490c285fb079445bb37a8df2a3e669ac",
            "value": 571
          }
        },
        "8378aa12de4348969f6cc56554d13132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6106158fe93042e9af78e525154b4d65",
            "placeholder": "​",
            "style": "IPY_MODEL_37869b7cd7b841699080126abb9b7db1",
            "value": " 571/571 [00:00&lt;00:00, 21.1kB/s]"
          }
        },
        "f58042af6c0b4d189c458f9859c1a42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c164764eef3643448b21bf9fdb44a464": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f562f7e12b5487fb30c5b4a1acaf9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58cd7d979ab04c49b9a48159d188050d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490c285fb079445bb37a8df2a3e669ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6106158fe93042e9af78e525154b4d65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37869b7cd7b841699080126abb9b7db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed1d92c77840443a9ffcbede0c3ff97b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cb8ca59fbd649febe6effabbbfb5bd9",
              "IPY_MODEL_3d57d822da444ea584cabc0a2981aeb7",
              "IPY_MODEL_4ad0ff5a3e554a4c8064954b7f62af2f"
            ],
            "layout": "IPY_MODEL_c3f7e9ce639040628428d60acb2bf500"
          }
        },
        "8cb8ca59fbd649febe6effabbbfb5bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_373493e93fbe48b59a265586d645d4eb",
            "placeholder": "​",
            "style": "IPY_MODEL_ad4bd2550fe84fea83692180db054376",
            "value": "model.safetensors: 100%"
          }
        },
        "3d57d822da444ea584cabc0a2981aeb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1f3a80eb73b44e1a4ef4746878f865d",
            "max": 496254442,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cce4bf58fac4288b8170131b0204623",
            "value": 496254442
          }
        },
        "4ad0ff5a3e554a4c8064954b7f62af2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11349692d0614f52bb2115d41a96dc8a",
            "placeholder": "​",
            "style": "IPY_MODEL_b2641f14af294dd28fbec018755ee1e2",
            "value": " 496M/496M [00:08&lt;00:00, 49.8MB/s]"
          }
        },
        "c3f7e9ce639040628428d60acb2bf500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "373493e93fbe48b59a265586d645d4eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad4bd2550fe84fea83692180db054376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1f3a80eb73b44e1a4ef4746878f865d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cce4bf58fac4288b8170131b0204623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11349692d0614f52bb2115d41a96dc8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2641f14af294dd28fbec018755ee1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e14a925eb9747c7b161b402b55a5ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_344a5d26bbcf469ab1d1b30ca9804dde",
              "IPY_MODEL_6548bc06655c476a8ce091a3ef17f28a",
              "IPY_MODEL_c88bac54e3ba4ca5bf8e03a6cb3ac76e"
            ],
            "layout": "IPY_MODEL_95a2cd05397343609079b83bb672c605"
          }
        },
        "344a5d26bbcf469ab1d1b30ca9804dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f702037f0084d78bd39be03c1560e55",
            "placeholder": "​",
            "style": "IPY_MODEL_4b891f679e7d4d53bc3a553da0a74ec7",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6548bc06655c476a8ce091a3ef17f28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41871c4df2d94efd9d048a907ad22774",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4026e2d73fa4807b40ff040c375c139",
            "value": 79
          }
        },
        "c88bac54e3ba4ca5bf8e03a6cb3ac76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ce97c1186924ed5bd765630e059abe4",
            "placeholder": "​",
            "style": "IPY_MODEL_d23dd3a8b1d54e0d8540512f67fa9c13",
            "value": " 79.0/79.0 [00:00&lt;00:00, 4.14kB/s]"
          }
        },
        "95a2cd05397343609079b83bb672c605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f702037f0084d78bd39be03c1560e55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b891f679e7d4d53bc3a553da0a74ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41871c4df2d94efd9d048a907ad22774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4026e2d73fa4807b40ff040c375c139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ce97c1186924ed5bd765630e059abe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d23dd3a8b1d54e0d8540512f67fa9c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34cc58a8af944f30aab104aafe9b6b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_481a33dab98c4879ae894f0c657e445c",
              "IPY_MODEL_9947bc83666b4cdda12f60f09c113f6c",
              "IPY_MODEL_abb7228db67a4ac893126dedb3bd19a4"
            ],
            "layout": "IPY_MODEL_de6f2dd2dd7845a9976f80a8bc944228"
          }
        },
        "481a33dab98c4879ae894f0c657e445c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3482cfa627d44301910c2d9a261bb584",
            "placeholder": "​",
            "style": "IPY_MODEL_63a9b97133dd4dc188fef005c1ffd60a",
            "value": "vocab.json: 100%"
          }
        },
        "9947bc83666b4cdda12f60f09c113f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d00d2d1da9c4ba99f6399bb3feee61a",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_698a95f05e82476aa10c9fe59522f641",
            "value": 898822
          }
        },
        "abb7228db67a4ac893126dedb3bd19a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfc594c2bbb649008ee151332b7c5fc4",
            "placeholder": "​",
            "style": "IPY_MODEL_59a9bd975f004fb8acac1eee81e4769c",
            "value": " 899k/899k [00:00&lt;00:00, 1.42MB/s]"
          }
        },
        "de6f2dd2dd7845a9976f80a8bc944228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3482cfa627d44301910c2d9a261bb584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a9b97133dd4dc188fef005c1ffd60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d00d2d1da9c4ba99f6399bb3feee61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698a95f05e82476aa10c9fe59522f641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfc594c2bbb649008ee151332b7c5fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59a9bd975f004fb8acac1eee81e4769c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccfb510a094f440d9ae01ce68f3f4cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c60b81c14aee487084cb85e9f2bc8b2b",
              "IPY_MODEL_328f9ddc822e48318acf77ecef396c0e",
              "IPY_MODEL_9d61a2235a88432cacb6d4e6d4382d1e"
            ],
            "layout": "IPY_MODEL_da772006ad37494293fcdc5c46987a4e"
          }
        },
        "c60b81c14aee487084cb85e9f2bc8b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd0b198324c74c23b89ab04772024412",
            "placeholder": "​",
            "style": "IPY_MODEL_0710cee2c3ce4a7b99af6650005a1235",
            "value": "merges.txt: 100%"
          }
        },
        "328f9ddc822e48318acf77ecef396c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a964770def024335a1647f90175d2f9a",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5e514d605a24dbb956c5c934f186ed9",
            "value": 456318
          }
        },
        "9d61a2235a88432cacb6d4e6d4382d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9be58183ccb84482a5a6ab5975b4d474",
            "placeholder": "​",
            "style": "IPY_MODEL_9fe3429c59a144e7b31a6cdb95f8c3a6",
            "value": " 456k/456k [00:00&lt;00:00, 5.14MB/s]"
          }
        },
        "da772006ad37494293fcdc5c46987a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd0b198324c74c23b89ab04772024412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0710cee2c3ce4a7b99af6650005a1235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a964770def024335a1647f90175d2f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5e514d605a24dbb956c5c934f186ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9be58183ccb84482a5a6ab5975b4d474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fe3429c59a144e7b31a6cdb95f8c3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e51b99518184a8bb7fcc86aa6209c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86adba64214f4d5f943882396c66d859",
              "IPY_MODEL_aa20383988574695869a29119c484504",
              "IPY_MODEL_dc20378c34cb400aac44249a65ca8288"
            ],
            "layout": "IPY_MODEL_98fd54f25b7341149eb78d55c76bb584"
          }
        },
        "86adba64214f4d5f943882396c66d859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca22d257d57c4c6696c785459e4cab6f",
            "placeholder": "​",
            "style": "IPY_MODEL_96b16a3802dd4bc4a268f8130aad8de4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "aa20383988574695869a29119c484504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7436da62e7cd424288556836a0b6b348",
            "max": 772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab58efc1a0234b76b6158674d70d10c4",
            "value": 772
          }
        },
        "dc20378c34cb400aac44249a65ca8288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a638372424498ba04b7c9558c8a138",
            "placeholder": "​",
            "style": "IPY_MODEL_71eb4e1c2d254ab49accf9037d015332",
            "value": " 772/772 [00:00&lt;00:00, 40.1kB/s]"
          }
        },
        "98fd54f25b7341149eb78d55c76bb584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca22d257d57c4c6696c785459e4cab6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96b16a3802dd4bc4a268f8130aad8de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7436da62e7cd424288556836a0b6b348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab58efc1a0234b76b6158674d70d10c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09a638372424498ba04b7c9558c8a138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71eb4e1c2d254ab49accf9037d015332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}