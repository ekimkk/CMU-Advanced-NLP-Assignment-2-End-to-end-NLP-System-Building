{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://rajpurkar.github.io/SQuAD-explorer/\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s: return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = Counter(gold_toks) & Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        return int(gold_toks == pred_toks), int(gold_toks == pred_toks), int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0, 0, 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1, precision, recall\n",
        "\n",
        "def read_answers(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "def evaluate(gold_file, pred_file):\n",
        "    gold_answers = read_answers(gold_file)\n",
        "    predictions = read_answers(pred_file)\n",
        "    f1, precision, recall, exact_match = 0.0, 0.0, 0.0, 0\n",
        "    for gold, pred in zip(gold_answers, predictions):\n",
        "        _f1, _precision, _recall = compute_f1(gold, pred)\n",
        "        f1 += _f1\n",
        "        precision += _precision\n",
        "        recall += _recall\n",
        "        exact_match += compute_exact(gold, pred)\n",
        "    total = len(gold_answers)\n",
        "    return {\n",
        "        \"F1\": 100.0 * f1 / total,\n",
        "        \"Recall\": 100.0 * recall / total,\n",
        "        \"Exact Match\": 100.0 * exact_match / total\n",
        "    }\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "predictions = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/Roberta_answer/roberta_model_answer.txt'\n",
        "gold = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/t5_answer/t5_gold_answer.txt'\n",
        "results1 = evaluate(gold, predictions)\n",
        "print(results1)"
      ],
      "metadata": {
        "id": "HC_8CtFEOVbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cd1125-bf1f-40ad-c468-caf6dd8643fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "{'F1': 43.04146909184546, 'Recall': 42.227779229104456, 'Exact Match': 32.87671232876713}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "predictions = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/t5_answer/model_answer_t5.txt'\n",
        "gold = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/t5_answer/t5_gold_answer.txt'\n",
        "results2 = evaluate(gold, predictions)\n",
        "print(results2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv2Z4ps0UivT",
        "outputId": "243fe5d7-4b29-4b2c-ad72-a3ccb287a2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "{'F1': 3.159836282311732, 'Recall': 13.824470304012987, 'Exact Match': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "predictions = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/mistral_answer/mistral_answer.txt'\n",
        "gold = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/t5_answer/t5_gold_answer.txt'\n",
        "results3 = evaluate(gold, predictions)\n",
        "print(results3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdJe2MFnC8cA",
        "outputId": "4bd731c6-f16a-4e93-fae3-4ab0a9830606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "{'F1': 3.3183087214459523, 'Recall': 5.071700254925944, 'Exact Match': 0.684931506849315}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "roberta_ans = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/Roberta_answer/roberta_model_answer.txt'\n",
        "t5_ans = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/t5_answer/model_answer_t5.txt'\n",
        "mis_ans = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/mistral_answer/mistral_answer.txt'\n",
        "gold = '/content/drive/My Drive/Colab Notebooks/NLP2/model_answer/t5_answer/t5_gold_answer.txt'\n",
        "\n",
        "def evaluate_sig(gold_file, pred_file):\n",
        "    gold_answers = read_answers(gold_file)\n",
        "    predictions = read_answers(pred_file)\n",
        "    scores_per_question = []\n",
        "    for gold, pred in zip(gold_answers, predictions):\n",
        "        f1, precision, recall = compute_f1(gold, pred)\n",
        "        exact_match = compute_exact(gold, pred)\n",
        "        scores_per_question.append({\n",
        "            \"F1\": f1\n",
        "        })\n",
        "    return scores_per_question\n",
        "\n",
        "roberta_scores = evaluate_sig(gold, roberta_ans)\n",
        "t5_scores = evaluate_sig(gold, t5_ans)\n",
        "mis_scores = evaluate_sig(gold, mis_ans)\n",
        "\n",
        "f1_scores_model1 = [score['F1'] for score in roberta_scores]\n",
        "f1_scores_model2 = [score['F1'] for score in t5_scores]\n",
        "f1_scores_model3 = [score['F1'] for score in mis_scores]\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Function to conduct a paired t-test and print the results\n",
        "def conduct_t_test(scores_model_a, scores_model_b, model_a_name, model_b_name):\n",
        "    t_statistic, p_value = stats.ttest_rel(scores_model_a, scores_model_b)\n",
        "    print(f\"Paired T-Test between {model_a_name} and {model_b_name}:\")\n",
        "    print(f\"  T-statistic: {t_statistic}, P-value: {p_value}\")\n",
        "    if p_value < 0.05:\n",
        "        print(\"  The difference is statistically significant.\\n\")\n",
        "    else:\n",
        "        print(\"  No statistically significant difference.\\n\")\n",
        "\n",
        "# Conduct paired t-tests between each pair of models\n",
        "conduct_t_test(f1_scores_model1, f1_scores_model2, 'Model 1', 'Model 2')\n",
        "conduct_t_test(f1_scores_model1, f1_scores_model3, 'Model 1', 'Model 3')\n",
        "conduct_t_test(f1_scores_model2, f1_scores_model3, 'Model 2', 'Model 3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQhHvHIXTYGM",
        "outputId": "fd1008d3-87da-4d78-e7fd-cd2988d37616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Paired T-Test between Model 1 and Model 2:\n",
            "  T-statistic: 21.378974249623514, P-value: 2.6080521451742205e-75\n",
            "  The difference is statistically significant.\n",
            "\n",
            "Paired T-Test between Model 1 and Model 3:\n",
            "  T-statistic: 20.872705989695085, P-value: 1.1297994852892257e-72\n",
            "  The difference is statistically significant.\n",
            "\n",
            "Paired T-Test between Model 2 and Model 3:\n",
            "  T-statistic: -0.32033981060363065, P-value: 0.748825526567398\n",
            "  No statistically significant difference.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "85ea2c107d7945555de8e73270cf8a4d668bafec7aac344fa62e3415dc7bf5ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}